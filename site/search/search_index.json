{"config":{"lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Welcome to Cape Privacy's documentation # Cape Privacy offers data scientists and data engineers a policy-based interface for applying privacy-enhancing techniques. This documentation site includes: A guide to help you understand the different elements of Cape Privacy , including architecture and best practices. How to install and use Cape Core services and tools. Information on how to use Cape Privacy's code libraries . Best practice guidance on integrating Cape Privacy into your data workflow. News about new features in our release notes .","title":"Home"},{"location":"#welcome-to-cape-privacys-documentation","text":"Cape Privacy offers data scientists and data engineers a policy-based interface for applying privacy-enhancing techniques. This documentation site includes: A guide to help you understand the different elements of Cape Privacy , including architecture and best practices. How to install and use Cape Core services and tools. Information on how to use Cape Privacy's code libraries . Best practice guidance on integrating Cape Privacy into your data workflow. News about new features in our release notes .","title":"Welcome to Cape Privacy's documentation"},{"location":"cape-core/","text":"Cape Core # Cape Core is a collection of services and tools that form the foundation of Cape functionality. Cape Core currently comprises: Cape Coordinator : a service for managing data privacy policies. Cape CLI : a command line interface to allow users to interact with Cape services. We will add more services and tools in future releases. Refer to our roadmap for more information.","title":"Cape Core"},{"location":"cape-core/#cape-core","text":"Cape Core is a collection of services and tools that form the foundation of Cape functionality. Cape Core currently comprises: Cape Coordinator : a service for managing data privacy policies. Cape CLI : a command line interface to allow users to interact with Cape services. We will add more services and tools in future releases. Refer to our roadmap for more information.","title":"Cape Core"},{"location":"cape-core/cli/","text":"Cape CLI # This section includes an installation guide , usage examples , and a reference for Cape CLI. About Cape CLI # The CLI (command line interface) is a way of interacting with Cape Coordinator. It provides commands to manage policies, projects and users.","title":"Cape CLI"},{"location":"cape-core/cli/#cape-cli","text":"This section includes an installation guide , usage examples , and a reference for Cape CLI.","title":"Cape CLI"},{"location":"cape-core/cli/#about-cape-cli","text":"The CLI (command line interface) is a way of interacting with Cape Coordinator. It provides commands to manage policies, projects and users.","title":"About Cape CLI"},{"location":"cape-core/cli/installation/","text":"Cape CLI installation # Binaries are available for Windows 10, Linux, and macOS. Download here . Once downloaded, you can run the binary from anywhere. You don't need to install it into a global location. For ease of use, you should ideally place it in a sensible location and add it to your PATH . The location depends on on your operating system. For example, on Linux this might be /usr/local/bin , while on Windows it might be your Programs directory, or C:\\Users\\<USERNAME> . If you are unsure how to modify your PATH , check out the following guides: Windows 10 Linux macOS","title":"Cape CLI installation"},{"location":"cape-core/cli/installation/#cape-cli-installation","text":"Binaries are available for Windows 10, Linux, and macOS. Download here . Once downloaded, you can run the binary from anywhere. You don't need to install it into a global location. For ease of use, you should ideally place it in a sensible location and add it to your PATH . The location depends on on your operating system. For example, on Linux this might be /usr/local/bin , while on Windows it might be your Programs directory, or C:\\Users\\<USERNAME> . If you are unsure how to modify your PATH , check out the following guides: Windows 10 Linux macOS","title":"Cape CLI installation"},{"location":"cape-core/cli/usage/","text":"CLI usage # To view available CLI commands, enter cape --help . This section provides some examples of how to use the CLI for common tasks: Create a user Create a project Add a policy to a project Suggest a policy change","title":"CLI usage"},{"location":"cape-core/cli/usage/#cli-usage","text":"To view available CLI commands, enter cape --help . This section provides some examples of how to use the CLI for common tasks: Create a user Create a project Add a policy to a project Suggest a policy change","title":"CLI usage"},{"location":"cape-core/cli/usage/add-policy/","text":"Add a policy to a project # The data policy defines the data you want to change, and the transformations or redactions you want to apply. Cape Python requires data policies in YAML format. Refer to policies for guidance on writing a policy file. You must be a project-owner to add a policy file. Refer to user roles for more information. To add a policy to a project, use the update command. Replace <LABEL> with the project label you assigned when creating the project. cape projects update --from-spec <PATH_TO_YOUR_POLICY_FILE> <LABEL>","title":"Add a policy to a project"},{"location":"cape-core/cli/usage/add-policy/#add-a-policy-to-a-project","text":"The data policy defines the data you want to change, and the transformations or redactions you want to apply. Cape Python requires data policies in YAML format. Refer to policies for guidance on writing a policy file. You must be a project-owner to add a policy file. Refer to user roles for more information. To add a policy to a project, use the update command. Replace <LABEL> with the project label you assigned when creating the project. cape projects update --from-spec <PATH_TO_YOUR_POLICY_FILE> <LABEL>","title":"Add a policy to a project"},{"location":"cape-core/cli/usage/create-project/","text":"Create a project # A project in Cape should represent one of your data science projects. Within the Cape project, you can add and manage data policies, as well as users associated with the project. To create a new project, you need to be an admin user. Run projects create , passing in a lable and description: cape projects create \"<LABEL>\" \"<DESCRIPTION>\" When you create a project, you are the only contributor associated with the project. You have the project-owner user role . Add a project-contributor . This user must already exist. Refer to create a user for guidance on creating users. cape projects contributors add <CONTRIBUTOR_EMAIL> <LABEL> project-contributor Now, list the project contributors: cape project contributors list <LABEL> # Output Name Email <YOUR_NAME> <YOUR_EMAIL> <CONTRIBUTOR_NAME> <CONTRIBUTOR_EMAIL> To change a contributor's role, use the role command. Downgrade the contributor you created in step 3 to project-reader . cape roles set --project <LABEL> <CONTRIBUTOR_EMAIL> project-reader","title":"Create a project"},{"location":"cape-core/cli/usage/create-project/#create-a-project","text":"A project in Cape should represent one of your data science projects. Within the Cape project, you can add and manage data policies, as well as users associated with the project. To create a new project, you need to be an admin user. Run projects create , passing in a lable and description: cape projects create \"<LABEL>\" \"<DESCRIPTION>\" When you create a project, you are the only contributor associated with the project. You have the project-owner user role . Add a project-contributor . This user must already exist. Refer to create a user for guidance on creating users. cape projects contributors add <CONTRIBUTOR_EMAIL> <LABEL> project-contributor Now, list the project contributors: cape project contributors list <LABEL> # Output Name Email <YOUR_NAME> <YOUR_EMAIL> <CONTRIBUTOR_NAME> <CONTRIBUTOR_EMAIL> To change a contributor's role, use the role command. Downgrade the contributor you created in step 3 to project-reader . cape roles set --project <LABEL> <CONTRIBUTOR_EMAIL> project-reader","title":"Create a project"},{"location":"cape-core/cli/usage/create-user/","text":"Create a user # You must add users to Cape, so that they can interact with projects and policy files through the Cape CLI. To add a user, you must have the global admin role. Refer to user roles for more information. Create a new user, with user level access. When prompted, enter their name. Cape automatically generates a password. cape users create <USER_EMAIL>","title":"Create a user"},{"location":"cape-core/cli/usage/create-user/#create-a-user","text":"You must add users to Cape, so that they can interact with projects and policy files through the Cape CLI. To add a user, you must have the global admin role. Refer to user roles for more information. Create a new user, with user level access. When prompted, enter their name. Cape automatically generates a password. cape users create <USER_EMAIL>","title":"Create a user"},{"location":"cape-core/cli/usage/suggest-policy/","text":"Suggest a policy change # Users with the project-contributor or project-owner role types can suggest changes to privacy policy through the CLI. This workflow changes the policy attached to a project. Create a policy YAML file. This can be a new file, or a copy of an existing policy. Write the new policy. Submit the policy for approval: cape projects policy create --from-spec <PATH_TO_YOUR_POLICY_FILE> \"<TITLE>\" \"<DESCRIPTION>\" Project owners can review the suggestion: # List all suggestions cape policy list-suggestions <PROJECT_NAME> # View details of a suggestion using the ID displayed by list-suggestions cape policy get-suggestion <ID> A project owner can choose to accept or reject the suggestion. If they approve it, the new policy replaces the old one attached to the project. # Accept cape policy approve <ID> # Reject cape policy reject <ID>","title":"Suggest a policy change"},{"location":"cape-core/cli/usage/suggest-policy/#suggest-a-policy-change","text":"Users with the project-contributor or project-owner role types can suggest changes to privacy policy through the CLI. This workflow changes the policy attached to a project. Create a policy YAML file. This can be a new file, or a copy of an existing policy. Write the new policy. Submit the policy for approval: cape projects policy create --from-spec <PATH_TO_YOUR_POLICY_FILE> \"<TITLE>\" \"<DESCRIPTION>\" Project owners can review the suggestion: # List all suggestions cape policy list-suggestions <PROJECT_NAME> # View details of a suggestion using the ID displayed by list-suggestions cape policy get-suggestion <ID> A project owner can choose to accept or reject the suggestion. If they approve it, the new policy replaces the old one attached to the project. # Accept cape policy approve <ID> # Reject cape policy reject <ID>","title":"Suggest a policy change"},{"location":"cape-core/coordinator/","text":"Cape Coordinator # About Cape Coordinator # Cape Coordinator is a server application that manages access to policies and projects based on user authentication and user roles. Features include: Storing data in a database for retrieval. Exposing GraphQL and HTTP endpoints for managing Cape Coordinator. Exposing GraphQL and HTTP endpoints to retrieve projects, policies, and user information. Encrypting secret data (such as transformation keys and user credentials) before storing them in the database. Installation # There are two ways to install Cape Coordinator. Refer to the installation instructions for your preferred method: Deploy to Kubernetes for production use, or if you are familiar with Kubernetes. Install locally using the CLI to try out Cape Coordinator on your local machine.","title":"Cape Coordinator"},{"location":"cape-core/coordinator/#cape-coordinator","text":"","title":"Cape Coordinator"},{"location":"cape-core/coordinator/#about-cape-coordinator","text":"Cape Coordinator is a server application that manages access to policies and projects based on user authentication and user roles. Features include: Storing data in a database for retrieval. Exposing GraphQL and HTTP endpoints for managing Cape Coordinator. Exposing GraphQL and HTTP endpoints to retrieve projects, policies, and user information. Encrypting secret data (such as transformation keys and user credentials) before storing them in the database.","title":"About Cape Coordinator"},{"location":"cape-core/coordinator/#installation","text":"There are two ways to install Cape Coordinator. Refer to the installation instructions for your preferred method: Deploy to Kubernetes for production use, or if you are familiar with Kubernetes. Install locally using the CLI to try out Cape Coordinator on your local machine.","title":"Installation"},{"location":"cape-core/coordinator/kubernetes/","text":"Install Cape Coordinator with Helm # This document describes how to deploy Cape Coordinator to Kubernetes. It assumes you are familiar with Kubernetes and Helm, and can configure your deployment environment. You need to install Cape CLI separately to interact with Coordinator. Prerequisites # To deploy Cape Coordinator, you need: A Kubernetes cluster (1.16+) A Helm installation (3.0+) A PostgreSQL database (11.0+) If you don't already have an environment to deploy Cape Coordinator into, refer to these guides to set up a local testing environment: Kubernetes-in-Docker Quick Start Helm: Quickstart Guide PostgreSQL: Bitnami Helm Chart Add the Cape Helm charts repository # helm repo remove cape helm repo add cape https://capeprivacy-charts.storage.googleapis.com/ Install the Helm chart # To install Cape Coordinator using the default values, use the following helm command: helm install cape-coordinator cape/coordinator The Helm chart supports many options that allow you to customize the Cape Coordinator deployment to suit your environment. For more information about these options, refer to Configuration . Configuration # The following table lists common configurable parameters of the chart and their default values. See values.yaml for all available options. Parameter Description Default image.pullPolicy Container pull policy IfNotPresent image.repository Container image to use capeprivacy/cape image.tag Container image tag to deploy latest podAnnotations List of custom annotations to attach to pods {} podLabels List of custom labels to attach to pods {} annotations List of custom annotations to attach to all resources {} labels List of custom labels to attach to all resources {} replicas Number of instances of Cape coordinator `1 deploymentStrategy.type Update strategy for deployment RollingUpdate serviceAccount.create Enable or disable creation of a service account true serviceAccount.name Name of the service account to use for resources \"\" service.type Cape Service type \"ClusterIP\" service.port Port on which Cape coordinator listens 8080 service.targetPort Port on which Cape coordinator pods listen 3000 service.uiPort Port on which to serve Cape web UI 80 service.uiTargetPort Port on which Cape pods serve web UI 80 service.annotations List of custome annotations to attach to the service {} service.labels List of custome labels to attach to the service {} service.portName Name of the port for on which Cape coordinator listens service admin.existingSecret Name of existing secret to use for certs and other configuration. Disables creation of helm-managed secret when set. \"\" includeUI Enable or disable serving of the Cape web UI false uiImage.repository Container image to use for Cape web UI capeprivacy/cape-ui uiImage.tag Container image tag to use for Cape web UI latest uiImage.pullPolicy Container pull policy for Cape web UI IfNotPresent priorityClassName Scheduler priority class for pods \"\" schedulerName Alternative scheduler to use for Cape pods \"\" namespaceOverride Namespace to use for Cape resources \"\" nameOverride Override for generated deployment name \"\" fullnameOverride Override for generated full name \"\" config.db.addr Address of the backing database for Cape coordinator \"postgres://postgres:dev@postgres-cape-postgresql:5432/cape\" config.port Port on which Cape coordinator containers listen 8080 config.root_key Root key for decrypting stored data keys ul0b9qQONZDn4kLNgULl3WJOZEhBqDjT1YK-kgceFUU config.version Version of the configuration file supplied. Currently must be 1 1 config.cors Enable or disable servicing CORS headers on Cape coordinator responses true config.allow_origin Origin hosts from which to allow API requests http://localhost:8080 Specify each parameter using the --set[-file] key=value[,key=value] argument to helm install . Uninstall # helm delete cape-coordinator To delete the deployment and its history: helm delete --purge cape-coordinator","title":"Install Cape Coordinator with Helm"},{"location":"cape-core/coordinator/kubernetes/#install-cape-coordinator-with-helm","text":"This document describes how to deploy Cape Coordinator to Kubernetes. It assumes you are familiar with Kubernetes and Helm, and can configure your deployment environment. You need to install Cape CLI separately to interact with Coordinator.","title":"Install Cape Coordinator with Helm"},{"location":"cape-core/coordinator/kubernetes/#prerequisites","text":"To deploy Cape Coordinator, you need: A Kubernetes cluster (1.16+) A Helm installation (3.0+) A PostgreSQL database (11.0+) If you don't already have an environment to deploy Cape Coordinator into, refer to these guides to set up a local testing environment: Kubernetes-in-Docker Quick Start Helm: Quickstart Guide PostgreSQL: Bitnami Helm Chart","title":"Prerequisites"},{"location":"cape-core/coordinator/kubernetes/#add-the-cape-helm-charts-repository","text":"helm repo remove cape helm repo add cape https://capeprivacy-charts.storage.googleapis.com/","title":"Add the Cape Helm charts repository"},{"location":"cape-core/coordinator/kubernetes/#install-the-helm-chart","text":"To install Cape Coordinator using the default values, use the following helm command: helm install cape-coordinator cape/coordinator The Helm chart supports many options that allow you to customize the Cape Coordinator deployment to suit your environment. For more information about these options, refer to Configuration .","title":"Install the Helm chart"},{"location":"cape-core/coordinator/kubernetes/#configuration","text":"The following table lists common configurable parameters of the chart and their default values. See values.yaml for all available options. Parameter Description Default image.pullPolicy Container pull policy IfNotPresent image.repository Container image to use capeprivacy/cape image.tag Container image tag to deploy latest podAnnotations List of custom annotations to attach to pods {} podLabels List of custom labels to attach to pods {} annotations List of custom annotations to attach to all resources {} labels List of custom labels to attach to all resources {} replicas Number of instances of Cape coordinator `1 deploymentStrategy.type Update strategy for deployment RollingUpdate serviceAccount.create Enable or disable creation of a service account true serviceAccount.name Name of the service account to use for resources \"\" service.type Cape Service type \"ClusterIP\" service.port Port on which Cape coordinator listens 8080 service.targetPort Port on which Cape coordinator pods listen 3000 service.uiPort Port on which to serve Cape web UI 80 service.uiTargetPort Port on which Cape pods serve web UI 80 service.annotations List of custome annotations to attach to the service {} service.labels List of custome labels to attach to the service {} service.portName Name of the port for on which Cape coordinator listens service admin.existingSecret Name of existing secret to use for certs and other configuration. Disables creation of helm-managed secret when set. \"\" includeUI Enable or disable serving of the Cape web UI false uiImage.repository Container image to use for Cape web UI capeprivacy/cape-ui uiImage.tag Container image tag to use for Cape web UI latest uiImage.pullPolicy Container pull policy for Cape web UI IfNotPresent priorityClassName Scheduler priority class for pods \"\" schedulerName Alternative scheduler to use for Cape pods \"\" namespaceOverride Namespace to use for Cape resources \"\" nameOverride Override for generated deployment name \"\" fullnameOverride Override for generated full name \"\" config.db.addr Address of the backing database for Cape coordinator \"postgres://postgres:dev@postgres-cape-postgresql:5432/cape\" config.port Port on which Cape coordinator containers listen 8080 config.root_key Root key for decrypting stored data keys ul0b9qQONZDn4kLNgULl3WJOZEhBqDjT1YK-kgceFUU config.version Version of the configuration file supplied. Currently must be 1 1 config.cors Enable or disable servicing CORS headers on Cape coordinator responses true config.allow_origin Origin hosts from which to allow API requests http://localhost:8080 Specify each parameter using the --set[-file] key=value[,key=value] argument to helm install .","title":"Configuration"},{"location":"cape-core/coordinator/kubernetes/#uninstall","text":"helm delete cape-coordinator To delete the deployment and its history: helm delete --purge cape-coordinator","title":"Uninstall"},{"location":"cape-core/coordinator/local-cli/","text":"Install and run Cape Coordinator using the Cape CLI # This document describes how to install and run Cape Coordinator on your local machine, using the Cape CLI. This setup allows you to try out Cape Coordinator without going through the full Kubernetes deployment process . Set up PostgreSQL for Cape # The following instructions include how to install PostgreSQL with a package manager for Linux and macOS, and using a GUI installer for Windows. For alternative PostgreSQL installation methods, refer to the PostgreSQL downloads page . Postgresql version 11.0+ is required. All Windows instructions assume you are using PowerShell. Linux Install PostgreSQL. Refer to the guidance for your distribution: PostgreSQL downloads . Create a user: # Create a user named cape createuser -U postgres --createdb --pwprompt cape Create a Cape database: createdb -U cape cape MacOS Install Postgres. This requires Homebrew: https://brew.sh/ brew install postgres Create a user: # Create a user named cape. createuser -U postgres --createdb --pwprompt cape Create a Cape database: createdb -U cape cape Windows (PowerShell) We recommend using the GUI installer for PostgreSQL on Windows. There is also a Chocolatey package available. Download the installer from PostgreSQL Windows installers . Run the installer. Accept the default settings at each stage. It will prompt you to set a password for the postgres user. Make a note of this password. Do not launch StackBuilder when the installation finishes. Create a user. When prompted, enter the postgres user password that you created during installation. # Create a user named cape. createuser -U postgres --createdb --pwprompt cape Create a database. When prompted, enter the user password you created in the previous step. # Create a database. createdb -U cape cape Download and set up Cape Coordinator # Linux Download the Cape binary curl -LO https://github.com/capeprivacy/cape/releases/download/v0.0.1/cape_0.0.1_Linux_x86_64.tar.gz tar xf cape_0.0.1_Linux_x86_64.tar.gz chmod +x ./cape Add cape to your PATH. Check the method for your Linux distro. If you skip this step, replace cape with ./cape in subsequent commands. Download and extract the database schema files: curl -LO https://github.com/capeprivacy/cape/releases/download/v0.0.1/capeprivacy-cape-postgres-v0.0.1.zip unzip capeprivacy-cape-postgres-v0.0.1.zip Create the database schema. Replace <PASSWORD> with your postgres user password. CAPE_DB_URL = postgres://postgres:<PASSWORD>@localhost:5432/cape cape update coordinator \\m igrations \\ Configure the Cape Coordinator server component: cape coordinator configure Cape asks you for information about your installation. For this example, use port 8080 . When this process completes, it creates a file called config.yaml . You can edit this file to change the inital configuration. Set user details, then start the coordinator. Make a note of the values you set for each field. You will use these to log in to Cape Coordinator. export CAPE_USER_NAME = <USERNAME> export CAPE_USER_EMAIL = <EMAIL> # The password must be eight characters minimum export CAPE_USER_PASSWORD = <PASSWORD> # Start the coordinator cape coordinator start --file config.yaml Note Cape Coordinator is now running in this window. If you close the terminal, or attempt further commands, you will stop Cape Coordinator. MacOS Download the Cape binary curl -LO https://github.com/capeprivacy/cape/releases/download/v0.0.1/cape_0.0.1_Linux_x86_64.tar.gz tar xf cape_0.0.1_Linux_x86_64.tar.gz chmod +x ./cape Add cape to your PATH. Check the method for your system. If you skip this step, replace cape with ./cape in subsequent commands. Download and extract the database schema files: curl -LO https://github.com/capeprivacy/cape/releases/download/v0.0.1/capeprivacy-cape-postgres-v0.0.1.zip unzip capeprivacy-cape-postgres-v0.0.1.zip Create the database schema. Replace <PASSWORD> with your postgres user password. CAPE_DB_URL = postgres://postgres:<PASSWORD>@localhost:5432/cape cape update coordinator \\m igrations \\ Configure the Cape Coordinator server component: cape coordinator configure Cape asks you for information about your installation. For this example, use port 8080 . When this process completes, it creates a file called config.yaml . You can edit this file to change the inital configuration. Set user details, then start the coordinator. Make a note of the values you set for each field. You will use these to log in to Cape Coordinator. export CAPE_USER_NAME = <USERNAME> export CAPE_USER_EMAIL = <EMAIL> # The password must be eight characters minimum export CAPE_USER_PASSWORD = <PASSWORD> # Start the coordinator cape coordinator start --file config.yaml Note Cape Coordinator is now running in this window. If you close the terminal, or attempt further commands, you will stop Cape Coordinator. Windows (PowerShell) Download the Windows installer .zip from Cape releases . Unzip the installer. Copy the unzipped directory to a location of your choice. Add the directory to your PATH . Refer to Add to Windows PATH environment variable for help. If you are still using the same PowerShell window as in the previous section, run refreshenv to load the updated PATH . Download the database schema files: capeprivacy-cape-postgres-v0.0.1.zip Unzip the files. Run the following command to set up the database. Replace <PASSWORD> with the password you created when installing PostgreSQL. $env :CAPE_DB_URL = 'postgres://postgres:<PASSSWORD>@localhost:5432/cape' cape update coordinator/migrations # Configure the Cape Coordinator server component cape coordinator configure Cape asks you for information about your installation. For this example, use port 8080 . When this process completes, it creates a file called config.yaml . You can edit this file to change the inital configuration. Set user details, then start the coordinator. Make a note of the values you set for each field. You will use these to log in to Cape Coordinator. $env :CAPE_USER_NAME = '<USERNAME>' $env :CAPE_USER_EMAIL = '<EMAIL>' # The password must be eight characters minimum $env :CAPE_USER_PASSWORD = '<PASSWORD>' # Start the coordinator cape coordinator start --file config.yaml Note Cape Coordinator is now running in this window. If you close the terminal, or attempt further commands, you will stop Cape Coordinator. Configure the CLI and log in to Cape # Open a new terminal or PowerShell window, and run the following commands. If you used a port other than 8080 when setting up Cape Coordinator, make sure to use your port number. You need the email and password you set in the previous section. # Configure the Cape CLI cape config clusters add local http://localhost:8080 cape config clusters use local # Log in to Cape cape login At this point you can begin exploring Cape. Refer to the usage tutorials for your next steps. For more information about using your Cape installation check out the CLI usage tutorials and a tutorial on using the Coordinator with Cape Python .","title":"Install and run Cape Coordinator using the Cape CLI"},{"location":"cape-core/coordinator/local-cli/#install-and-run-cape-coordinator-using-the-cape-cli","text":"This document describes how to install and run Cape Coordinator on your local machine, using the Cape CLI. This setup allows you to try out Cape Coordinator without going through the full Kubernetes deployment process .","title":"Install and run Cape Coordinator using the Cape CLI"},{"location":"cape-core/coordinator/local-cli/#set-up-postgresql-for-cape","text":"The following instructions include how to install PostgreSQL with a package manager for Linux and macOS, and using a GUI installer for Windows. For alternative PostgreSQL installation methods, refer to the PostgreSQL downloads page . Postgresql version 11.0+ is required. All Windows instructions assume you are using PowerShell. Linux Install PostgreSQL. Refer to the guidance for your distribution: PostgreSQL downloads . Create a user: # Create a user named cape createuser -U postgres --createdb --pwprompt cape Create a Cape database: createdb -U cape cape MacOS Install Postgres. This requires Homebrew: https://brew.sh/ brew install postgres Create a user: # Create a user named cape. createuser -U postgres --createdb --pwprompt cape Create a Cape database: createdb -U cape cape Windows (PowerShell) We recommend using the GUI installer for PostgreSQL on Windows. There is also a Chocolatey package available. Download the installer from PostgreSQL Windows installers . Run the installer. Accept the default settings at each stage. It will prompt you to set a password for the postgres user. Make a note of this password. Do not launch StackBuilder when the installation finishes. Create a user. When prompted, enter the postgres user password that you created during installation. # Create a user named cape. createuser -U postgres --createdb --pwprompt cape Create a database. When prompted, enter the user password you created in the previous step. # Create a database. createdb -U cape cape","title":"Set up PostgreSQL for Cape"},{"location":"cape-core/coordinator/local-cli/#download-and-set-up-cape-coordinator","text":"Linux Download the Cape binary curl -LO https://github.com/capeprivacy/cape/releases/download/v0.0.1/cape_0.0.1_Linux_x86_64.tar.gz tar xf cape_0.0.1_Linux_x86_64.tar.gz chmod +x ./cape Add cape to your PATH. Check the method for your Linux distro. If you skip this step, replace cape with ./cape in subsequent commands. Download and extract the database schema files: curl -LO https://github.com/capeprivacy/cape/releases/download/v0.0.1/capeprivacy-cape-postgres-v0.0.1.zip unzip capeprivacy-cape-postgres-v0.0.1.zip Create the database schema. Replace <PASSWORD> with your postgres user password. CAPE_DB_URL = postgres://postgres:<PASSWORD>@localhost:5432/cape cape update coordinator \\m igrations \\ Configure the Cape Coordinator server component: cape coordinator configure Cape asks you for information about your installation. For this example, use port 8080 . When this process completes, it creates a file called config.yaml . You can edit this file to change the inital configuration. Set user details, then start the coordinator. Make a note of the values you set for each field. You will use these to log in to Cape Coordinator. export CAPE_USER_NAME = <USERNAME> export CAPE_USER_EMAIL = <EMAIL> # The password must be eight characters minimum export CAPE_USER_PASSWORD = <PASSWORD> # Start the coordinator cape coordinator start --file config.yaml Note Cape Coordinator is now running in this window. If you close the terminal, or attempt further commands, you will stop Cape Coordinator. MacOS Download the Cape binary curl -LO https://github.com/capeprivacy/cape/releases/download/v0.0.1/cape_0.0.1_Linux_x86_64.tar.gz tar xf cape_0.0.1_Linux_x86_64.tar.gz chmod +x ./cape Add cape to your PATH. Check the method for your system. If you skip this step, replace cape with ./cape in subsequent commands. Download and extract the database schema files: curl -LO https://github.com/capeprivacy/cape/releases/download/v0.0.1/capeprivacy-cape-postgres-v0.0.1.zip unzip capeprivacy-cape-postgres-v0.0.1.zip Create the database schema. Replace <PASSWORD> with your postgres user password. CAPE_DB_URL = postgres://postgres:<PASSWORD>@localhost:5432/cape cape update coordinator \\m igrations \\ Configure the Cape Coordinator server component: cape coordinator configure Cape asks you for information about your installation. For this example, use port 8080 . When this process completes, it creates a file called config.yaml . You can edit this file to change the inital configuration. Set user details, then start the coordinator. Make a note of the values you set for each field. You will use these to log in to Cape Coordinator. export CAPE_USER_NAME = <USERNAME> export CAPE_USER_EMAIL = <EMAIL> # The password must be eight characters minimum export CAPE_USER_PASSWORD = <PASSWORD> # Start the coordinator cape coordinator start --file config.yaml Note Cape Coordinator is now running in this window. If you close the terminal, or attempt further commands, you will stop Cape Coordinator. Windows (PowerShell) Download the Windows installer .zip from Cape releases . Unzip the installer. Copy the unzipped directory to a location of your choice. Add the directory to your PATH . Refer to Add to Windows PATH environment variable for help. If you are still using the same PowerShell window as in the previous section, run refreshenv to load the updated PATH . Download the database schema files: capeprivacy-cape-postgres-v0.0.1.zip Unzip the files. Run the following command to set up the database. Replace <PASSWORD> with the password you created when installing PostgreSQL. $env :CAPE_DB_URL = 'postgres://postgres:<PASSSWORD>@localhost:5432/cape' cape update coordinator/migrations # Configure the Cape Coordinator server component cape coordinator configure Cape asks you for information about your installation. For this example, use port 8080 . When this process completes, it creates a file called config.yaml . You can edit this file to change the inital configuration. Set user details, then start the coordinator. Make a note of the values you set for each field. You will use these to log in to Cape Coordinator. $env :CAPE_USER_NAME = '<USERNAME>' $env :CAPE_USER_EMAIL = '<EMAIL>' # The password must be eight characters minimum $env :CAPE_USER_PASSWORD = '<PASSWORD>' # Start the coordinator cape coordinator start --file config.yaml Note Cape Coordinator is now running in this window. If you close the terminal, or attempt further commands, you will stop Cape Coordinator.","title":"Download and set up Cape Coordinator"},{"location":"cape-core/coordinator/local-cli/#configure-the-cli-and-log-in-to-cape","text":"Open a new terminal or PowerShell window, and run the following commands. If you used a port other than 8080 when setting up Cape Coordinator, make sure to use your port number. You need the email and password you set in the previous section. # Configure the Cape CLI cape config clusters add local http://localhost:8080 cape config clusters use local # Log in to Cape cape login At this point you can begin exploring Cape. Refer to the usage tutorials for your next steps. For more information about using your Cape installation check out the CLI usage tutorials and a tutorial on using the Coordinator with Cape Python .","title":"Configure the CLI and log in to Cape"},{"location":"libraries/","text":"Libraries # Cape Privacy provides libraries that allow you to: Write data security policies in YAML format. Create scripts that apply your policies to your data. Use Cape Privacy's standard data transformations.","title":"Introduction"},{"location":"libraries/#libraries","text":"Cape Privacy provides libraries that allow you to: Write data security policies in YAML format. Create scripts that apply your policies to your data. Use Cape Privacy's standard data transformations.","title":"Libraries"},{"location":"libraries/cape-python/","text":"Cape Python overview # Cape Python allows you to write data privacy policies and data transformations to integrate with Pandas and Spark . You can view the source code in the Cape Python GitHub Repository . Use cases # Review the transformations and decide which are a good fit for your data science needs. The 0.1.0 release includes five transformations that provide some common privacy protections. Use case Text data Numeric data Inconsistent data EDA Tokenization Rounding or pertubation Tokenization Analytics Tokenization Rounding or pertubation - ML development - Rounding or pertubation Tokenization ML training/serving No transformation No transformation No transformation Cape Privacy will support more use cases through additional transformations in future releases.","title":"Overview"},{"location":"libraries/cape-python/#cape-python-overview","text":"Cape Python allows you to write data privacy policies and data transformations to integrate with Pandas and Spark . You can view the source code in the Cape Python GitHub Repository .","title":"Cape Python overview"},{"location":"libraries/cape-python/#use-cases","text":"Review the transformations and decide which are a good fit for your data science needs. The 0.1.0 release includes five transformations that provide some common privacy protections. Use case Text data Numeric data Inconsistent data EDA Tokenization Rounding or pertubation Tokenization Analytics Tokenization Rounding or pertubation - ML development - Rounding or pertubation Tokenization ML training/serving No transformation No transformation No transformation Cape Privacy will support more use cases through additional transformations in future releases.","title":"Use cases"},{"location":"libraries/cape-python/coordinator-quickstart/","text":"Cape Python API with Coordinator # This document describes how to use Cape Python with Cape Coordinator. It builds on the Cape Python quickstart guide and the Cape Coordinator installation guides. Quickstart # Prerequisites # Cape Coordinator installed. You need the password and email that you set during Cape Coordinator installation. Cape Python installed. We recommend working through the Cape Python quickstart guide and reading the Cape Coordinator documentation before following the steps below. Create your first project # Authenticate with Cape Coordinator by running: $ CAPE_PASSWORD = <PASSWORD> cape login --email <EMAIL> Replace <PASSWORD> and <EMAIL> with the values set during Cape Coordinator installation. Cape Coordinator uses projects to manage which version of a policy is applied to a dataset. To create your first project, run: $ cape projects create first-project \"Hello Project World\" You can now add a policy to the project from a policy specification file. Copy the following YAML to a file called first-policy.yaml : version : 1 rules : # Set the column name - match : name : weight actions : - transform : # This example shows an unnamed transformation. # It tells the policy runner to: # (1) Apply the transformation numeric-rounding # (2) Round to one decimal place type : numeric-rounding dtype : Double precision : 1 Update the project with the first version of policy: $ cape projects update --from-spec first-policy.yaml first-project Get an API token # To connect to Cape Coordinator from Cape Python you need an API token. Obtain the token by running: $ cape tokens create You'll see output like: A token for cape_user@mycape.com has been created! Token: <REDACTED TOKEN> \u203c Remember: Please keep the token safe and share it only over secure channels. Copy the token from the CLI. You will need it in the next section. Write the policy application script # The following script is very similar to the Cape Python quickstart guide , except in this case we're connecting to Cape Coordinator to retrieve the policy. Create a coordinator-policy.py file in your project, with the following content: import cape_privacy as cape import pandas as pd # In the Coordinator installation instructions you should have run the # Coordinator on port 8080. If not, edit the line below with the proper # port. client = cape . Client ( \"http://localhost:8080\" ) client . login ( \"<PASTE TOKEN HERE>\" ) p_dict = client . get_policy ( \"first-project\" ) policy = cape . parse_policy ( p_dict ) # Create a simple Pandas DataFrame df = pd . DataFrame ([ 114.432 , 134.622 , 142.984 ], columns = [ \"weight\" ]) df = cape . apply_policy ( policy , df , inplace = False ) print ( df . head ()) Make sure to replace <PASTE TOKEN HERE> with your API token. Run your transformations # In coordinator-policy.py we create a dataset programatically, so there are no further steps to load a dataset. Run the policy application script and view the output: $ python coordinator-policy.py","title":"Coordinator quickstart"},{"location":"libraries/cape-python/coordinator-quickstart/#cape-python-api-with-coordinator","text":"This document describes how to use Cape Python with Cape Coordinator. It builds on the Cape Python quickstart guide and the Cape Coordinator installation guides.","title":"Cape Python API with Coordinator"},{"location":"libraries/cape-python/coordinator-quickstart/#quickstart","text":"","title":"Quickstart"},{"location":"libraries/cape-python/coordinator-quickstart/#prerequisites","text":"Cape Coordinator installed. You need the password and email that you set during Cape Coordinator installation. Cape Python installed. We recommend working through the Cape Python quickstart guide and reading the Cape Coordinator documentation before following the steps below.","title":"Prerequisites"},{"location":"libraries/cape-python/coordinator-quickstart/#create-your-first-project","text":"Authenticate with Cape Coordinator by running: $ CAPE_PASSWORD = <PASSWORD> cape login --email <EMAIL> Replace <PASSWORD> and <EMAIL> with the values set during Cape Coordinator installation. Cape Coordinator uses projects to manage which version of a policy is applied to a dataset. To create your first project, run: $ cape projects create first-project \"Hello Project World\" You can now add a policy to the project from a policy specification file. Copy the following YAML to a file called first-policy.yaml : version : 1 rules : # Set the column name - match : name : weight actions : - transform : # This example shows an unnamed transformation. # It tells the policy runner to: # (1) Apply the transformation numeric-rounding # (2) Round to one decimal place type : numeric-rounding dtype : Double precision : 1 Update the project with the first version of policy: $ cape projects update --from-spec first-policy.yaml first-project","title":"Create your first project"},{"location":"libraries/cape-python/coordinator-quickstart/#get-an-api-token","text":"To connect to Cape Coordinator from Cape Python you need an API token. Obtain the token by running: $ cape tokens create You'll see output like: A token for cape_user@mycape.com has been created! Token: <REDACTED TOKEN> \u203c Remember: Please keep the token safe and share it only over secure channels. Copy the token from the CLI. You will need it in the next section.","title":"Get an API token"},{"location":"libraries/cape-python/coordinator-quickstart/#write-the-policy-application-script","text":"The following script is very similar to the Cape Python quickstart guide , except in this case we're connecting to Cape Coordinator to retrieve the policy. Create a coordinator-policy.py file in your project, with the following content: import cape_privacy as cape import pandas as pd # In the Coordinator installation instructions you should have run the # Coordinator on port 8080. If not, edit the line below with the proper # port. client = cape . Client ( \"http://localhost:8080\" ) client . login ( \"<PASTE TOKEN HERE>\" ) p_dict = client . get_policy ( \"first-project\" ) policy = cape . parse_policy ( p_dict ) # Create a simple Pandas DataFrame df = pd . DataFrame ([ 114.432 , 134.622 , 142.984 ], columns = [ \"weight\" ]) df = cape . apply_policy ( policy , df , inplace = False ) print ( df . head ()) Make sure to replace <PASTE TOKEN HERE> with your API token.","title":"Write the policy application script"},{"location":"libraries/cape-python/coordinator-quickstart/#run-your-transformations","text":"In coordinator-policy.py we create a dataset programatically, so there are no further steps to load a dataset. Run the policy application script and view the output: $ python coordinator-policy.py","title":"Run your transformations"},{"location":"libraries/cape-python/policies/","text":"Policies # The data policy defines the data you want to change, and the transformations or redactions you want to apply. Cape Python requires data policies in YAML format. This example describes all the available YAML objects: # Required. The policy name. label : test_policy # Required. The Cape Privacy specification version. Must be 1. version : 1 # Configure your named transformations. # Named transformations allow you to reuse a transformation # with a set value throughout your policy. transformations : # This named transformation uses the built-in tokenizer transformation - name : my_tokenizer type : tokenizer max_token_len : 10 key : \"my secret\" rules : # Required. The column name. - match : name : fruit actions : # This example shows a named transformation. # It tells the policy runner to apply the my_tokenizer transformation # to all fields in the \"fruit\" column. - transform : name : my_tokenizer - match : name : weight actions : - transform : # This example shows an unnamed transformation. # It tells the policy runner to: # (1) Apply the transformation numeric-rounding # (2) Round to one decimal place type : numeric-rounding dtype : Double precision : 1","title":"Policies"},{"location":"libraries/cape-python/policies/#policies","text":"The data policy defines the data you want to change, and the transformations or redactions you want to apply. Cape Python requires data policies in YAML format. This example describes all the available YAML objects: # Required. The policy name. label : test_policy # Required. The Cape Privacy specification version. Must be 1. version : 1 # Configure your named transformations. # Named transformations allow you to reuse a transformation # with a set value throughout your policy. transformations : # This named transformation uses the built-in tokenizer transformation - name : my_tokenizer type : tokenizer max_token_len : 10 key : \"my secret\" rules : # Required. The column name. - match : name : fruit actions : # This example shows a named transformation. # It tells the policy runner to apply the my_tokenizer transformation # to all fields in the \"fruit\" column. - transform : name : my_tokenizer - match : name : weight actions : - transform : # This example shows an unnamed transformation. # It tells the policy runner to: # (1) Apply the transformation numeric-rounding # (2) Round to one decimal place type : numeric-rounding dtype : Double precision : 1","title":"Policies"},{"location":"libraries/cape-python/quickstart/","text":"Cape Python API # This guide provides an example of using Cape Python with either Pandas or Spark. Prerequisites # Python 3.6 or above. Cape Privacy recommends using a virtual environment such as venv . Installation # You can install Cape Python with pip: pip install cape-privacy Quickstart # Write the policy # The data policy file defines the target data and permissions. It is written in YAML. Cape Python reads the .yaml policy file and applies the policies based on your policy application script . Create a test-policy.yaml file in your project, with the following content: label : test-policy version : 1 rules : # Set the column name - match : name : weight actions : - transform : # This example shows an unnamed transformation. # It tells the policy runner to: # (1) Apply the transformation numeric-rounding # (2) Round to one decimal place type : numeric-rounding dtype : Double precision : 1 Write the policy application script # To apply the policy .yaml to your data, you must run a script that defines which policy you apply to which data target. Create a test-transformation.py file in your project, with the following content: Pandas import cape_privacy as cape import pandas as pd # Create a simple Pandas DataFrame df = pd . DataFrame ([ 114.432 , 134.622 , 142.984 ], columns = [ \"weight\" ]) # Load the privacy policy policy = cape . parse_policy ( \"test-policy.yaml\" ) # Apply the policy to the DataFrame df = cape . apply_policy ( policy , df , inplace = False ) # Output the altered data print ( df . head ()) Spark import cape_privacy as cape from pyspark import sql sess_builder = sql . SparkSession . builder sess_builder = sess_builder . appName ( 'cape.examples.rounding' ) sess_builder = sess_builder . config ( 'spark.sql.execution.arrow.enabled' , 'true' ) sess = sess_builder . getOrCreate () # Create a simple Spark DataFrame df = sess . createDataFrame ([ 114.432 , 134.622 , 142.984 ], \"double\" ) . toDF ( \"weight\" ) # Load the privacy policy policy = cape . parse_policy ( \"test-policy.yaml\" ) # Apply the policy to the DataFrame df = cape . apply_policy ( policy , df , inplace = False ) # Output the altered data print ( df . show ()) Run your transformations # The quickstart example creates a dataset programatically, so you can run the policy application script and view the output: python test-transformation.py Note You can choose where in your workflow to run your transformation scripts. Refer to Best practices - Running transformations for guidance.","title":"Quickstart"},{"location":"libraries/cape-python/quickstart/#cape-python-api","text":"This guide provides an example of using Cape Python with either Pandas or Spark.","title":"Cape Python API"},{"location":"libraries/cape-python/quickstart/#prerequisites","text":"Python 3.6 or above. Cape Privacy recommends using a virtual environment such as venv .","title":"Prerequisites"},{"location":"libraries/cape-python/quickstart/#installation","text":"You can install Cape Python with pip: pip install cape-privacy","title":"Installation"},{"location":"libraries/cape-python/quickstart/#quickstart","text":"","title":"Quickstart"},{"location":"libraries/cape-python/quickstart/#write-the-policy","text":"The data policy file defines the target data and permissions. It is written in YAML. Cape Python reads the .yaml policy file and applies the policies based on your policy application script . Create a test-policy.yaml file in your project, with the following content: label : test-policy version : 1 rules : # Set the column name - match : name : weight actions : - transform : # This example shows an unnamed transformation. # It tells the policy runner to: # (1) Apply the transformation numeric-rounding # (2) Round to one decimal place type : numeric-rounding dtype : Double precision : 1","title":"Write the policy"},{"location":"libraries/cape-python/quickstart/#write-the-policy-application-script","text":"To apply the policy .yaml to your data, you must run a script that defines which policy you apply to which data target. Create a test-transformation.py file in your project, with the following content: Pandas import cape_privacy as cape import pandas as pd # Create a simple Pandas DataFrame df = pd . DataFrame ([ 114.432 , 134.622 , 142.984 ], columns = [ \"weight\" ]) # Load the privacy policy policy = cape . parse_policy ( \"test-policy.yaml\" ) # Apply the policy to the DataFrame df = cape . apply_policy ( policy , df , inplace = False ) # Output the altered data print ( df . head ()) Spark import cape_privacy as cape from pyspark import sql sess_builder = sql . SparkSession . builder sess_builder = sess_builder . appName ( 'cape.examples.rounding' ) sess_builder = sess_builder . config ( 'spark.sql.execution.arrow.enabled' , 'true' ) sess = sess_builder . getOrCreate () # Create a simple Spark DataFrame df = sess . createDataFrame ([ 114.432 , 134.622 , 142.984 ], \"double\" ) . toDF ( \"weight\" ) # Load the privacy policy policy = cape . parse_policy ( \"test-policy.yaml\" ) # Apply the policy to the DataFrame df = cape . apply_policy ( policy , df , inplace = False ) # Output the altered data print ( df . show ())","title":"Write the policy application script"},{"location":"libraries/cape-python/quickstart/#run-your-transformations","text":"The quickstart example creates a dataset programatically, so you can run the policy application script and view the output: python test-transformation.py Note You can choose where in your workflow to run your transformation scripts. Refer to Best practices - Running transformations for guidance.","title":"Run your transformations"},{"location":"libraries/cape-python/redactions/","text":"Redactions # Redactions involve dropping the matched data. Unlike transformations , which modify but preserve data, redactions will change the shape of your dataframes. Cape Python has one built-in redaction function. This document describes what it does, and provides an example of how to use it in your policy. Warning Redactions change the shape of your data. Column redaction # The column-redact redaction deletes matching columns. - transform : type : \"column-redact\" # Replace <COLUMN_NAME> with the column name you want to redact. columns : [ \"<COLUMN_NAME>\" ]","title":"Redactions"},{"location":"libraries/cape-python/redactions/#redactions","text":"Redactions involve dropping the matched data. Unlike transformations , which modify but preserve data, redactions will change the shape of your dataframes. Cape Python has one built-in redaction function. This document describes what it does, and provides an example of how to use it in your policy. Warning Redactions change the shape of your data.","title":"Redactions"},{"location":"libraries/cape-python/redactions/#column-redaction","text":"The column-redact redaction deletes matching columns. - transform : type : \"column-redact\" # Replace <COLUMN_NAME> with the column name you want to redact. columns : [ \"<COLUMN_NAME>\" ]","title":"Column redaction"},{"location":"libraries/cape-python/transformations/","text":"Transformations # Transformations are functions that alter your data, ensuring it is free of sensitive information. Cape Python has five built-in transformation functions. This document describes what they do, and provides an example of how to use each transformation in your policy. Date perturbation # The date-perturbation transformation adds random noise to dates. The amount of noise depends on the min and max values that you set in the policy. - transform : type : date-pertubation frequency : <one of : 'year' , 'month' , 'day' , 'hour' , 'minute' , 'second' > min : <int or float> max : <int or float> # Optional. The base number to initialize the random number generator. # Pandas only (Spark does not currently support seeding) seed : <int> Date truncation # The date-truncation transformation shortens dates to a unit (year or month). Set the unit in frequency . - transform : type : date-truncation frequency : <one of : 'year' , 'month' , 'day' , 'hour' , 'minute' , 'second' > Numeric pertubation # The numeric-pertubation transformation adds random noise to numeric data sets. The amount of noise depends on the min and max values that you set in the policy. - transform : type : numeric-pertubation dtype : <Pandas Series type or Spark Series type> min : <int or float> max : <int or float> # Optional. The base number to initialize the random number generator. seed : <int> Numeric rounding # The numeric-rounding transformation rounds numeric values to a given number of decimal places. Use precision to set the number of decimal places. - transform : type : numeric-rounding dtype : <Pandas Series type or Spark Series type> precision : <int> Tokenizer # The tokenizer transformation maps a string to a token to obfuscate it. Warning Linkable tokenization for sensitive data is vulnerable to privacy attacks. Cape Privacy does not recommend sharing tokenized data with preserved linkability with untrusted or outside parties. Cape Python does not support anonymized transformations. - transform : type : tokenizer # Default is 64 max_token_len : <int or bytes> # If unspecified, Cape Python uses a random byte string key : <string or byte string> ReversibleTokenizer # The ReversibleTokenizer transformation maps a sting to a token to obfuscate it. However, when using the ReversibleTokenizer , the tokens can be reverted back to their plaintext form by using the TokenReverser . - transform : type : reversible-tokenizer # If unspecified, Cape Python uses a random byte string key : <string or byte string> TokenReverser # The TokenReverser is designed to be used with the ReversibleTokenizer . The TokenReverser reverts tokens produced by the ReversibleTokenizer back to their plaintext form. - transform : type : token-reverser # If unspecified, Cape Python uses a random byte string key : <string or byte string>","title":"Transformations"},{"location":"libraries/cape-python/transformations/#transformations","text":"Transformations are functions that alter your data, ensuring it is free of sensitive information. Cape Python has five built-in transformation functions. This document describes what they do, and provides an example of how to use each transformation in your policy.","title":"Transformations"},{"location":"libraries/cape-python/transformations/#date-perturbation","text":"The date-perturbation transformation adds random noise to dates. The amount of noise depends on the min and max values that you set in the policy. - transform : type : date-pertubation frequency : <one of : 'year' , 'month' , 'day' , 'hour' , 'minute' , 'second' > min : <int or float> max : <int or float> # Optional. The base number to initialize the random number generator. # Pandas only (Spark does not currently support seeding) seed : <int>","title":"Date perturbation"},{"location":"libraries/cape-python/transformations/#date-truncation","text":"The date-truncation transformation shortens dates to a unit (year or month). Set the unit in frequency . - transform : type : date-truncation frequency : <one of : 'year' , 'month' , 'day' , 'hour' , 'minute' , 'second' >","title":"Date truncation"},{"location":"libraries/cape-python/transformations/#numeric-pertubation","text":"The numeric-pertubation transformation adds random noise to numeric data sets. The amount of noise depends on the min and max values that you set in the policy. - transform : type : numeric-pertubation dtype : <Pandas Series type or Spark Series type> min : <int or float> max : <int or float> # Optional. The base number to initialize the random number generator. seed : <int>","title":"Numeric pertubation"},{"location":"libraries/cape-python/transformations/#numeric-rounding","text":"The numeric-rounding transformation rounds numeric values to a given number of decimal places. Use precision to set the number of decimal places. - transform : type : numeric-rounding dtype : <Pandas Series type or Spark Series type> precision : <int>","title":"Numeric rounding"},{"location":"libraries/cape-python/transformations/#tokenizer","text":"The tokenizer transformation maps a string to a token to obfuscate it. Warning Linkable tokenization for sensitive data is vulnerable to privacy attacks. Cape Privacy does not recommend sharing tokenized data with preserved linkability with untrusted or outside parties. Cape Python does not support anonymized transformations. - transform : type : tokenizer # Default is 64 max_token_len : <int or bytes> # If unspecified, Cape Python uses a random byte string key : <string or byte string>","title":"Tokenizer"},{"location":"libraries/cape-python/transformations/#reversibletokenizer","text":"The ReversibleTokenizer transformation maps a sting to a token to obfuscate it. However, when using the ReversibleTokenizer , the tokens can be reverted back to their plaintext form by using the TokenReverser . - transform : type : reversible-tokenizer # If unspecified, Cape Python uses a random byte string key : <string or byte string>","title":"ReversibleTokenizer"},{"location":"libraries/cape-python/transformations/#tokenreverser","text":"The TokenReverser is designed to be used with the ReversibleTokenizer . The TokenReverser reverts tokens produced by the ReversibleTokenizer back to their plaintext form. - transform : type : token-reverser # If unspecified, Cape Python uses a random byte string key : <string or byte string>","title":"TokenReverser"},{"location":"libraries/cape-python/tutorials/reversible-tokenization/","text":"Reversible Tokenizer # Here we show an example of how you can use the ReversibleTokenizer to tokenize data within a pandas dataframe. The ReversibleTokenizer will tokenize the input data so it can be used in a privacy preserving manner. The ReversibleTokenizer can be used in conjunction with the TokenReverser to recover the original data. Tokenizing Data # The ReversibleTokenizer and TokenReverser classes can be found in the pandas.transformations package. from cape_privacy.pandas.transformations import ReversibleTokenizer from cape_privacy.pandas.transformations import TokenReverser In this example, we will simply hide the names within our dataset. import pandas as pd plaintext_data = pd . DataFrame ({ 'name' : [ \"Alice\" , \"Bob\" , \"Carol\" ], \"# friends\" : [ 100 , 200 , 300 ]}) You instantiate a ReversibleTokenizer by passing it a key. For the TokenReverser to be able to reverse the tokens produced by the ReversibleTokenizer , you must use the same key. key = b \"5\" * 32 tokenizer = ReversibleTokenizer ( key = key ) tokenized = pd . DataFrame ( plaintext_data ) tokenized [ \"name\" ] = tokenizer ( plaintext_data [ \"name\" ]) Recovering Tokens # If we ever need to reveal the tokenized data, we can use the TokenReverser class. reverser = TokenReverser ( key = key ) recovered = pd . DataFrame ( tokenized ) recovered [ \"name\" ] = reverser ( tokenized [ \"name\" ]) You can see full code for this example on Github","title":"Reversible Tokenization"},{"location":"libraries/cape-python/tutorials/reversible-tokenization/#reversible-tokenizer","text":"Here we show an example of how you can use the ReversibleTokenizer to tokenize data within a pandas dataframe. The ReversibleTokenizer will tokenize the input data so it can be used in a privacy preserving manner. The ReversibleTokenizer can be used in conjunction with the TokenReverser to recover the original data.","title":"Reversible Tokenizer"},{"location":"libraries/cape-python/tutorials/reversible-tokenization/#tokenizing-data","text":"The ReversibleTokenizer and TokenReverser classes can be found in the pandas.transformations package. from cape_privacy.pandas.transformations import ReversibleTokenizer from cape_privacy.pandas.transformations import TokenReverser In this example, we will simply hide the names within our dataset. import pandas as pd plaintext_data = pd . DataFrame ({ 'name' : [ \"Alice\" , \"Bob\" , \"Carol\" ], \"# friends\" : [ 100 , 200 , 300 ]}) You instantiate a ReversibleTokenizer by passing it a key. For the TokenReverser to be able to reverse the tokens produced by the ReversibleTokenizer , you must use the same key. key = b \"5\" * 32 tokenizer = ReversibleTokenizer ( key = key ) tokenized = pd . DataFrame ( plaintext_data ) tokenized [ \"name\" ] = tokenizer ( plaintext_data [ \"name\" ])","title":"Tokenizing Data"},{"location":"libraries/cape-python/tutorials/reversible-tokenization/#recovering-tokens","text":"If we ever need to reveal the tokenized data, we can use the TokenReverser class. reverser = TokenReverser ( key = key ) recovered = pd . DataFrame ( tokenized ) recovered [ \"name\" ] = reverser ( tokenized [ \"name\" ]) You can see full code for this example on Github","title":"Recovering Tokens"},{"location":"libraries/pycape/","text":"pycape # pycape is a set of Python modules for interacting with your Cape Privacy data. Using pycape , you can: Create and query dataviews , or pointers to the data that you want to use to train a model using Cape's encrypted learning protocol. Submit and track jobs , which are computational sessions which contain instructions for how to train your model. Short Tutorial # Access your Cape projects by creating an instance of the main Cape class: from pycape import Cape c = Cape() c.login() my_projects = c.list_projects() Add dataviews to your project, review dataviews added by other organizations collaborating with you in the project, and submit your job. from pycape import VerticallyPartitionedLinearRegression my_project = c . get_project ( \"project_123\" ) my_project . create_dataview ( name = \"my-data\" , uri = \"s3://my-data.csv\" owner_label = \"my-org\" ) dvs = my_project . list_dataviews () vlr_job = VerticallyPartitionedLinearRegression ( train_dataview_x = dvs [ 0 ], train_dataview_y = dvs [ 1 ] ) my_project . submit_job ( vlr_job ) See our example usage or a more in-depth tutorial . Installation # Prerequisites # Python 3.6 or above, and pip Make (if installing from source) Install via pip # pip install pycape License # Licensed under Apache License, Version 2.0. See LICENSE or http://www.apache.org/licenses/LICENSE-2.0 .","title":"Overview"},{"location":"libraries/pycape/#pycape","text":"pycape is a set of Python modules for interacting with your Cape Privacy data. Using pycape , you can: Create and query dataviews , or pointers to the data that you want to use to train a model using Cape's encrypted learning protocol. Submit and track jobs , which are computational sessions which contain instructions for how to train your model.","title":"pycape"},{"location":"libraries/pycape/#short-tutorial","text":"Access your Cape projects by creating an instance of the main Cape class: from pycape import Cape c = Cape() c.login() my_projects = c.list_projects() Add dataviews to your project, review dataviews added by other organizations collaborating with you in the project, and submit your job. from pycape import VerticallyPartitionedLinearRegression my_project = c . get_project ( \"project_123\" ) my_project . create_dataview ( name = \"my-data\" , uri = \"s3://my-data.csv\" owner_label = \"my-org\" ) dvs = my_project . list_dataviews () vlr_job = VerticallyPartitionedLinearRegression ( train_dataview_x = dvs [ 0 ], train_dataview_y = dvs [ 1 ] ) my_project . submit_job ( vlr_job ) See our example usage or a more in-depth tutorial .","title":"Short Tutorial"},{"location":"libraries/pycape/#installation","text":"","title":"Installation"},{"location":"libraries/pycape/#prerequisites","text":"Python 3.6 or above, and pip Make (if installing from source)","title":"Prerequisites"},{"location":"libraries/pycape/#install-via-pip","text":"pip install pycape","title":"Install via pip"},{"location":"libraries/pycape/#license","text":"Licensed under Apache License, Version 2.0. See LICENSE or http://www.apache.org/licenses/LICENSE-2.0 .","title":"License"},{"location":"libraries/pycape/reference/","text":"API Reference # pycape.Cape # This is the main class you instantiate to access the PyCape API. Use to authenticate with the Cape Cloud and manage top-level resources such as Project . delete_project ( self , id ) # Delete a Job by ID. Parameters: Name Type Description Default id str ID of Project . required Returns: Type Description str A success messsage write out. get_project ( self , id = None , label = None ) # Query a Project by either ID or label. Returns: Type Description Project A list of Project instances. Parameters: Name Type Description Default id Optional[str] ID of Project . None label Optional[str] Unique Project label. None Returns: Type Description Project A Project instance. list_projects ( self ) # Returns all list of projects that requesting user is a contributor of. Returns: Type Description str A list of Project instances. login ( self , token = None ) # Calls POST /v1/login . Authenticate with Cape Cloud in order to make subsequent requests. Parameters: Name Type Description Default token Optional[str] User authentication token. None Returns: Type Description None A success messsage write out. pycape.Project # Projects are the business contexts in which you collaborate with other organizations or Cape users to train models. Parameters: Name Type Description Default id str ID of Project . required name str name of Project . required label str label of Project . required description str description of Project . required owner dict Returned dictionary of fields related to the Project owner. required organizations list Returned list of fields related to the organizations associated with the Project . required dataviews list Returned list of DataViews added to the Project . required create_dataview ( self , name , uri , owner_id = None , owner_label = None , schema = None ) # Creates a DataView in Cape Cloud. Returns created Dataview Parameters: Name Type Description Default name str a name for the DataView . required uri str URI location of the dataset. required owner_id Optional[str] The ID of the organization that owns this dataset. None owner_label Optional[str] The label of the organization that owns this dataset. None schema Union[pandas.core.series.Series, List] The schema of the data that DataView points to. A string value for each column's datatype. Possible datatypes: string integer number datetime None Returns: Type Description DataView A DataView instance. delete_dataview ( self , id ) # Remove a DataView by ID. Parameters: Name Type Description Default id str ID of DataView . required get_dataview ( self , id = None , uri = None ) # Query a DataView for the scoped Project by DataView ID or URI. Parameters: Name Type Description Default id Optional[str] ID of DataView . None uri Optional[str] Unique DataView URI. None Returns: Type Description DataView A DataView instance. get_job ( self , id ) # Returns a Job given an ID. Parameters: Name Type Description Default id str ID of Job . required Returns: Type Description Job A Job instance. list_dataviews ( self ) # Returns a list of dataviews for the scoped Project . Returns: Type Description List[pycape.pycape.api.dataview.dataview.DataView] A list of DataView instances. submit_job ( self , task , timeout = 600 ) # Submits a Job to be run by your Cape worker in collaboration with other organizations in your Project . Parameters: Name Type Description Default task Task Instance of class that inherits from Task . required timeout float How long (in ms) a Cape Worker should run before canceling the Job . 600 Returns: Type Description Job A Job instance. pycape.Organization # Organization represents an organization in Cape. Parameters: Name Type Description Default id str ID of Organization required name str name of Organization . required label str label of Organization . required pycape.DataView # Dataviews store metadata around datasets, including namely a pointer to the dataset's location. Parameters: Name Type Description Default id str ID of DataView required name str name of DataView . required schema list schema of the data that DataView points to. required location str URI of DataView . required owner dict Dictionary of fields related to the DataView owner. required user_id str User ID of requester. required pycape.Job # Jobs track the status and eventually report the results of computation sessions run on Cape workers. Parameters: Name Type Description Default id str ID of Job required status str name of Job . required project_id str ID of Project . required approve ( self , org_id ) # Approve the Job on behalf of your organizations. Once all organizations approve a job, the computation will run. Parameters: Name Type Description Default org_id str ID of Organization . required Returns: Type Description Job A Job instance. get_results ( self ) # Given the requester's project role and authorization level, returns the trained model's weights and metrics. Returns: Type Description Tuple[numpy.ndarray, dict] weights: A numpy array. metrics: A dictionary of different metric values. get_status ( self ) # Query the current status of the Cape Job . Returns: Type Description str A Job status string. Status Types: Status Desciption Initialized Job has been intialized. NeedsApproval Job is awaiting approval by at least one party. Approved Job has been approved, the computation will commence. Rejected Job has been rejected, the computation will not run. Started Job has started. Completed Job has completed. Stopped Job has been stopped. Error Error in running Job. pycape.Task # Tasks contain the instructions for how a Cape worker should run a job. pycape.VerticallyPartitionedLinearRegressionJob # Inherits from: Task . Contains instructions for training linear regression models using verically-partioned datasets. Verically-partioned datasets refer to the joining of columns (i.e. features) from serveral parties. Parameters: Name Type Description Default x_train_dataview Union[`DataView`, `DataView`List[str]] DataView that points to a dataset that contains training set values. required y_train_dataview Union[`DataView`, `DataView`List[str]] DataView that points to a dataset that contains target values. required","title":"Reference"},{"location":"libraries/pycape/reference/#api-reference","text":"","title":"API Reference"},{"location":"libraries/pycape/reference/#pycapecape","text":"This is the main class you instantiate to access the PyCape API. Use to authenticate with the Cape Cloud and manage top-level resources such as Project .","title":"pycape.Cape"},{"location":"libraries/pycape/reference/#pycape.pycape.api.cape.cape.Cape.delete_project","text":"Delete a Job by ID. Parameters: Name Type Description Default id str ID of Project . required Returns: Type Description str A success messsage write out.","title":"delete_project()"},{"location":"libraries/pycape/reference/#pycape.pycape.api.cape.cape.Cape.get_project","text":"Query a Project by either ID or label. Returns: Type Description Project A list of Project instances. Parameters: Name Type Description Default id Optional[str] ID of Project . None label Optional[str] Unique Project label. None Returns: Type Description Project A Project instance.","title":"get_project()"},{"location":"libraries/pycape/reference/#pycape.pycape.api.cape.cape.Cape.list_projects","text":"Returns all list of projects that requesting user is a contributor of. Returns: Type Description str A list of Project instances.","title":"list_projects()"},{"location":"libraries/pycape/reference/#pycape.pycape.api.cape.cape.Cape.login","text":"Calls POST /v1/login . Authenticate with Cape Cloud in order to make subsequent requests. Parameters: Name Type Description Default token Optional[str] User authentication token. None Returns: Type Description None A success messsage write out.","title":"login()"},{"location":"libraries/pycape/reference/#pycapeproject","text":"Projects are the business contexts in which you collaborate with other organizations or Cape users to train models. Parameters: Name Type Description Default id str ID of Project . required name str name of Project . required label str label of Project . required description str description of Project . required owner dict Returned dictionary of fields related to the Project owner. required organizations list Returned list of fields related to the organizations associated with the Project . required dataviews list Returned list of DataViews added to the Project . required","title":"pycape.Project"},{"location":"libraries/pycape/reference/#pycape.pycape.api.project.project.Project.create_dataview","text":"Creates a DataView in Cape Cloud. Returns created Dataview Parameters: Name Type Description Default name str a name for the DataView . required uri str URI location of the dataset. required owner_id Optional[str] The ID of the organization that owns this dataset. None owner_label Optional[str] The label of the organization that owns this dataset. None schema Union[pandas.core.series.Series, List] The schema of the data that DataView points to. A string value for each column's datatype. Possible datatypes: string integer number datetime None Returns: Type Description DataView A DataView instance.","title":"create_dataview()"},{"location":"libraries/pycape/reference/#pycape.pycape.api.project.project.Project.delete_dataview","text":"Remove a DataView by ID. Parameters: Name Type Description Default id str ID of DataView . required","title":"delete_dataview()"},{"location":"libraries/pycape/reference/#pycape.pycape.api.project.project.Project.get_dataview","text":"Query a DataView for the scoped Project by DataView ID or URI. Parameters: Name Type Description Default id Optional[str] ID of DataView . None uri Optional[str] Unique DataView URI. None Returns: Type Description DataView A DataView instance.","title":"get_dataview()"},{"location":"libraries/pycape/reference/#pycape.pycape.api.project.project.Project.get_job","text":"Returns a Job given an ID. Parameters: Name Type Description Default id str ID of Job . required Returns: Type Description Job A Job instance.","title":"get_job()"},{"location":"libraries/pycape/reference/#pycape.pycape.api.project.project.Project.list_dataviews","text":"Returns a list of dataviews for the scoped Project . Returns: Type Description List[pycape.pycape.api.dataview.dataview.DataView] A list of DataView instances.","title":"list_dataviews()"},{"location":"libraries/pycape/reference/#pycape.pycape.api.project.project.Project.submit_job","text":"Submits a Job to be run by your Cape worker in collaboration with other organizations in your Project . Parameters: Name Type Description Default task Task Instance of class that inherits from Task . required timeout float How long (in ms) a Cape Worker should run before canceling the Job . 600 Returns: Type Description Job A Job instance.","title":"submit_job()"},{"location":"libraries/pycape/reference/#pycapeorganization","text":"Organization represents an organization in Cape. Parameters: Name Type Description Default id str ID of Organization required name str name of Organization . required label str label of Organization . required","title":"pycape.Organization"},{"location":"libraries/pycape/reference/#pycapedataview","text":"Dataviews store metadata around datasets, including namely a pointer to the dataset's location. Parameters: Name Type Description Default id str ID of DataView required name str name of DataView . required schema list schema of the data that DataView points to. required location str URI of DataView . required owner dict Dictionary of fields related to the DataView owner. required user_id str User ID of requester. required","title":"pycape.DataView"},{"location":"libraries/pycape/reference/#pycapejob","text":"Jobs track the status and eventually report the results of computation sessions run on Cape workers. Parameters: Name Type Description Default id str ID of Job required status str name of Job . required project_id str ID of Project . required","title":"pycape.Job"},{"location":"libraries/pycape/reference/#pycape.pycape.api.job.job.Job.approve","text":"Approve the Job on behalf of your organizations. Once all organizations approve a job, the computation will run. Parameters: Name Type Description Default org_id str ID of Organization . required Returns: Type Description Job A Job instance.","title":"approve()"},{"location":"libraries/pycape/reference/#pycape.pycape.api.job.job.Job.get_results","text":"Given the requester's project role and authorization level, returns the trained model's weights and metrics. Returns: Type Description Tuple[numpy.ndarray, dict] weights: A numpy array. metrics: A dictionary of different metric values.","title":"get_results()"},{"location":"libraries/pycape/reference/#pycape.pycape.api.job.job.Job.get_status","text":"Query the current status of the Cape Job . Returns: Type Description str A Job status string. Status Types: Status Desciption Initialized Job has been intialized. NeedsApproval Job is awaiting approval by at least one party. Approved Job has been approved, the computation will commence. Rejected Job has been rejected, the computation will not run. Started Job has started. Completed Job has completed. Stopped Job has been stopped. Error Error in running Job.","title":"get_status()"},{"location":"libraries/pycape/reference/#pycapetask","text":"Tasks contain the instructions for how a Cape worker should run a job.","title":"pycape.Task"},{"location":"libraries/pycape/reference/#pycapeverticallypartitionedlinearregressionjob","text":"Inherits from: Task . Contains instructions for training linear regression models using verically-partioned datasets. Verically-partioned datasets refer to the joining of columns (i.e. features) from serveral parties. Parameters: Name Type Description Default x_train_dataview Union[`DataView`, `DataView`List[str]] DataView that points to a dataset that contains training set values. required y_train_dataview Union[`DataView`, `DataView`List[str]] DataView that points to a dataset that contains target values. required","title":"pycape.VerticallyPartitionedLinearRegressionJob"},{"location":"libraries/pycape/tutorials/submit_linear_regression_job/","text":"Train a Linear Regression Model using Cape DataViews & Jobs # This tutorial will walk you through the process of training an encrypted linear regression model in collaboration with another organization using Cape Privacy. You'll learn how to: Send datasets securely to Cape Cloud. Review the dataset schemas of other organizations in your project. Approve and reject model computation jobs. View the metrics or weights of the trained model, depending on your role in the project. We'll use the Cape UI to set up and review activity in the project. We'll also use the pytest Python library to create and review pointers to datasets or DataViews , create Tasks , which are Cape Python objects that contain instructions on how to train a model using the data provided, and review Jobs in order to track the status of the training, and view the results of the trained model. Project Setup # Create an Organization # First you'll need to create an organization at demo.capeprivacy.com . Once you've created your organization, you can navigate to Organization Settings and generate a token for your organization. You'll need this token to configure your worker . Take note of this value as you cannot recover it after you reload the page. Create a Project # Next, create a Project within one of the organizations you just created. Projects serve as the context in which you can define and review Jobs with other organizations. Add organizations to your project in order to begin collaborating with them on training a model. Get a User Token # Finally, we will need a user token to authenticate against pycape . Ensure you are working within your user context and navigate to Account Settings to create a token. Take note of this value as, like the user token, you cannot recover it after you reload the page. That is it for the UI for now! We'll return later to review DataViews and approve Jobs . Next we will set up these DataViews and Jobs in pycape . Working with the PyCape Python Library # Login to PyCape # Before you can make requests to Cape Cloud, you'll need to authenticate with the API. Follow these instructions to authenticate with our API using pycape . Once you've logged in successfully, you should see a success message. >>> c = Cape () >>> c . login () Login successful Add a DataView to your project # Use the list_projects method defined on the main Cape class to query a list of projects that belong to your organization. >>> my_projects = c . list_projects () PROJECT ID NAME LABEL ----------- ----------------------- ----------------------- project_123 Default Risk Assessment default - risk - assessment >>> my_projects [ Project ( id = project_123 , name = Default Risk Assessment , label = default - risk - assessment )] To create a DataView and add it to your project, simply call the create_dataview method defined on the Project class. >>> my_project = c . get_project ( id = \"project_123\" ) >>> my_project . create_dataview ( name = \"my-data\" , uri = \"s3://my-data.csv\" , owner_label = \"my-org\" ) All DataViews must be associated with an organization. This association can be made by passing either an owner_label or an owner_id to the create_dataview method. Note Use the organization attribute on your Project class instance to verify the metadata of organizations that are contributing to the project. Note Unless your dataset is accessible via HTTP, you'll need to specify your schema . Review Your Collaborator's DataView # Before we can submit a job to train our linear regression model, we'll need to review the DataViews added to the project by our collaborators. Use the list_dataviews method defined on the Project class to inspect the name, owner (organization) and location of DataViews added to the project: >>> my_project = c . get_project ( id = \"project_123\" ) >>> dataviews = my_project . list_dataviews () DATAVIEW ID NAME LOCATION OWNER ----------- ------------ --------------- ------------- 01 EY48 orgacle - data s3 : // mydata . csv orgacle ( You ) 01 EY49 atlas - data atlas Note You'll only be able to see the locations or URIs of datasets that belong to your organization. You can also inspect the schema of each Dataview in your project in order to see the data types of the columns, and to assess which data columns should be used to train the linear regression model. >>> dataviews [ 1 ] . schema { 'debt equity ratio' : 'number' , 'operating margin' : 'number' , 'working capital' : 'integer' } You can also review the dataviews added to your project in the UI. Submitting a Linear Regression Job # Now that we've added our own DataView to the project, and vetted the DataView of our collaborator, we are ready to submit our Cape linear regression job. Pass the DataView that contains training data to x_train_dataview , and the DataView that contains the target values to y_train_dataview . >>> dataview_1 = my_project . get_dataview ( id = \"01EY48\" ) >>> dataview_2 = my_project . get_dataview ( id = \"01EY49\" ) >>> vlr = VerticallyPartitionedLinearRegression ( >>> x_train_dataview = dataview_1 , >>> y_train_dataview = dataview_2 , >>> ) >>> my_project . submit_job ( job = vlr ) You can also specify which data columns the model should be trained on or evaluated against by passing the dataview to the VerticallyPartitionedLinearRegression class like so: >>> VerticallyPartitionedLinearRegression ( >>> x_train_dataview = dataview_1 [ \"debt equity ratio\" ], >>> y_train_dataview = dataview_2 [ \"debt equity ratio\" ], >>> ) VerticallyPartitionedLinearRegression ( x_train_dataview = Orgacle Dataview [ 'debt equity ratio' ], y_train_dataview = Atlas Dataview [ 'debt equity ratio' ]) Note In order for your linear regression job to train a model using Cape's encrypted learning protocol, you'll need to run your own Cape workers. Read our documentation to get set up with Cape workers . Tracking Job Status # After submitting your job, you should be able to see the status and details of your Job in the UI. To check the status of your submitted linear regression job using pycape , use the get_status method: >>> lr_job = my_project . get_job ( id = \"abc_123\" ) >>> lr_job . get_status () Success Approving Jobs # Before Cape can begin to train a linear regression model using the datasets submitted via submit_job method, both parties need to review and approve the Job. To approve, you'll need to head over to the UI and navigate to your Job's details page. Once you've reviewed the details of your Job are correct, you can click \"Approve Job\" to let Cape know the job looks good on your end. Note Before your job can run, both parties need to approve it. Getting Weights and Metrics from Trained Model # Once your job has successfully completed, you can view the results of the trained model. Whether you can view the weights or metrics of the trained model (or both!) depends on the role you and your organization play in the project. To view the weights and metrics of a job, use the get_results method: >>> lr_job = my_project . get_job ( id = \"abc_123\" ) >>> weights , metrics = lr_job . get_results () >>> weights array ([ 12.14955139 , 1.96560669 ]) >>> metrics { 'r_squared_result' : [ 0.8804865768463074 ], 'mse_result' : [ 37.94773864746094 ]} If you are the model owner, the first value in the returned tuple will be populated with a numpy array of weights from your trained model. Note To access model weights you'll need to inform pycape about your AWS IAM authentication credentials .","title":"Train Linear Regression"},{"location":"libraries/pycape/tutorials/submit_linear_regression_job/#train-a-linear-regression-model-using-cape-dataviews-jobs","text":"This tutorial will walk you through the process of training an encrypted linear regression model in collaboration with another organization using Cape Privacy. You'll learn how to: Send datasets securely to Cape Cloud. Review the dataset schemas of other organizations in your project. Approve and reject model computation jobs. View the metrics or weights of the trained model, depending on your role in the project. We'll use the Cape UI to set up and review activity in the project. We'll also use the pytest Python library to create and review pointers to datasets or DataViews , create Tasks , which are Cape Python objects that contain instructions on how to train a model using the data provided, and review Jobs in order to track the status of the training, and view the results of the trained model.","title":"Train a Linear Regression Model using Cape DataViews &amp; Jobs"},{"location":"libraries/pycape/tutorials/submit_linear_regression_job/#project-setup","text":"","title":"Project Setup"},{"location":"libraries/pycape/tutorials/submit_linear_regression_job/#create-an-organization","text":"First you'll need to create an organization at demo.capeprivacy.com . Once you've created your organization, you can navigate to Organization Settings and generate a token for your organization. You'll need this token to configure your worker . Take note of this value as you cannot recover it after you reload the page.","title":"Create an Organization"},{"location":"libraries/pycape/tutorials/submit_linear_regression_job/#create-a-project","text":"Next, create a Project within one of the organizations you just created. Projects serve as the context in which you can define and review Jobs with other organizations. Add organizations to your project in order to begin collaborating with them on training a model.","title":"Create a Project"},{"location":"libraries/pycape/tutorials/submit_linear_regression_job/#get-a-user-token","text":"Finally, we will need a user token to authenticate against pycape . Ensure you are working within your user context and navigate to Account Settings to create a token. Take note of this value as, like the user token, you cannot recover it after you reload the page. That is it for the UI for now! We'll return later to review DataViews and approve Jobs . Next we will set up these DataViews and Jobs in pycape .","title":"Get a User Token"},{"location":"libraries/pycape/tutorials/submit_linear_regression_job/#working-with-the-pycape-python-library","text":"","title":"Working with the PyCape Python Library"},{"location":"libraries/pycape/tutorials/submit_linear_regression_job/#login-to-pycape","text":"Before you can make requests to Cape Cloud, you'll need to authenticate with the API. Follow these instructions to authenticate with our API using pycape . Once you've logged in successfully, you should see a success message. >>> c = Cape () >>> c . login () Login successful","title":"Login to PyCape"},{"location":"libraries/pycape/tutorials/submit_linear_regression_job/#add-a-dataview-to-your-project","text":"Use the list_projects method defined on the main Cape class to query a list of projects that belong to your organization. >>> my_projects = c . list_projects () PROJECT ID NAME LABEL ----------- ----------------------- ----------------------- project_123 Default Risk Assessment default - risk - assessment >>> my_projects [ Project ( id = project_123 , name = Default Risk Assessment , label = default - risk - assessment )] To create a DataView and add it to your project, simply call the create_dataview method defined on the Project class. >>> my_project = c . get_project ( id = \"project_123\" ) >>> my_project . create_dataview ( name = \"my-data\" , uri = \"s3://my-data.csv\" , owner_label = \"my-org\" ) All DataViews must be associated with an organization. This association can be made by passing either an owner_label or an owner_id to the create_dataview method. Note Use the organization attribute on your Project class instance to verify the metadata of organizations that are contributing to the project. Note Unless your dataset is accessible via HTTP, you'll need to specify your schema .","title":"Add a DataView to your project"},{"location":"libraries/pycape/tutorials/submit_linear_regression_job/#review-your-collaborators-dataview","text":"Before we can submit a job to train our linear regression model, we'll need to review the DataViews added to the project by our collaborators. Use the list_dataviews method defined on the Project class to inspect the name, owner (organization) and location of DataViews added to the project: >>> my_project = c . get_project ( id = \"project_123\" ) >>> dataviews = my_project . list_dataviews () DATAVIEW ID NAME LOCATION OWNER ----------- ------------ --------------- ------------- 01 EY48 orgacle - data s3 : // mydata . csv orgacle ( You ) 01 EY49 atlas - data atlas Note You'll only be able to see the locations or URIs of datasets that belong to your organization. You can also inspect the schema of each Dataview in your project in order to see the data types of the columns, and to assess which data columns should be used to train the linear regression model. >>> dataviews [ 1 ] . schema { 'debt equity ratio' : 'number' , 'operating margin' : 'number' , 'working capital' : 'integer' } You can also review the dataviews added to your project in the UI.","title":"Review Your Collaborator's DataView"},{"location":"libraries/pycape/tutorials/submit_linear_regression_job/#submitting-a-linear-regression-job","text":"Now that we've added our own DataView to the project, and vetted the DataView of our collaborator, we are ready to submit our Cape linear regression job. Pass the DataView that contains training data to x_train_dataview , and the DataView that contains the target values to y_train_dataview . >>> dataview_1 = my_project . get_dataview ( id = \"01EY48\" ) >>> dataview_2 = my_project . get_dataview ( id = \"01EY49\" ) >>> vlr = VerticallyPartitionedLinearRegression ( >>> x_train_dataview = dataview_1 , >>> y_train_dataview = dataview_2 , >>> ) >>> my_project . submit_job ( job = vlr ) You can also specify which data columns the model should be trained on or evaluated against by passing the dataview to the VerticallyPartitionedLinearRegression class like so: >>> VerticallyPartitionedLinearRegression ( >>> x_train_dataview = dataview_1 [ \"debt equity ratio\" ], >>> y_train_dataview = dataview_2 [ \"debt equity ratio\" ], >>> ) VerticallyPartitionedLinearRegression ( x_train_dataview = Orgacle Dataview [ 'debt equity ratio' ], y_train_dataview = Atlas Dataview [ 'debt equity ratio' ]) Note In order for your linear regression job to train a model using Cape's encrypted learning protocol, you'll need to run your own Cape workers. Read our documentation to get set up with Cape workers .","title":"Submitting a Linear Regression Job"},{"location":"libraries/pycape/tutorials/submit_linear_regression_job/#tracking-job-status","text":"After submitting your job, you should be able to see the status and details of your Job in the UI. To check the status of your submitted linear regression job using pycape , use the get_status method: >>> lr_job = my_project . get_job ( id = \"abc_123\" ) >>> lr_job . get_status () Success","title":"Tracking Job Status"},{"location":"libraries/pycape/tutorials/submit_linear_regression_job/#approving-jobs","text":"Before Cape can begin to train a linear regression model using the datasets submitted via submit_job method, both parties need to review and approve the Job. To approve, you'll need to head over to the UI and navigate to your Job's details page. Once you've reviewed the details of your Job are correct, you can click \"Approve Job\" to let Cape know the job looks good on your end. Note Before your job can run, both parties need to approve it.","title":"Approving Jobs"},{"location":"libraries/pycape/tutorials/submit_linear_regression_job/#getting-weights-and-metrics-from-trained-model","text":"Once your job has successfully completed, you can view the results of the trained model. Whether you can view the weights or metrics of the trained model (or both!) depends on the role you and your organization play in the project. To view the weights and metrics of a job, use the get_results method: >>> lr_job = my_project . get_job ( id = \"abc_123\" ) >>> weights , metrics = lr_job . get_results () >>> weights array ([ 12.14955139 , 1.96560669 ]) >>> metrics { 'r_squared_result' : [ 0.8804865768463074 ], 'mse_result' : [ 37.94773864746094 ]} If you are the model owner, the first value in the returned tuple will be populated with a numpy array of weights from your trained model. Note To access model weights you'll need to inform pycape about your AWS IAM authentication credentials .","title":"Getting Weights and Metrics from Trained Model"},{"location":"libraries/pycape/usage/dataview/","text":"Managing DataViews # Get list of data views for a project # my_project = c . get_project ( id = \"project_123\" ) my_project . list_dataviews () Default response: DATAVIEW ID NAME LOCATION OWNER -------------------------- ------------- --------------- ----------- 01EY48EFT4H7PWAN45SG2AEZ81 armazorn-data s3://mydata.csv armazorn ( You ) 01EY49J86722ENT9JSMKTE65EX gorgle-data gorgle Get a data view # my_project = c . get_project ( id = \"project_123\" ) # get by id my_project . get_dataview ( id = \"dataview_123\" ) # get by uri my_project . get_dataview ( uri = \"s3://my-data.csv\" ) Default response: DataView ( id = dataview_123, name = my-data, location = s3://my-data.csv ) Add a data view to a project # Initialize a DataView class and pass the instance to the create_dataview method. my_project = c . get_project ( id = \"project_123\" ) my_project . create_dataview ( name = \"my-data\" , uri = \"s3://my-data.csv\" , owner_label = \"my-org\" ) Default response: DataView ( id = dataview_123, name = my-data, location = s3://my-data.csv ) Specifying a Schema for your DataView # DataView schemas allow you to clarify the data types of your dataset. They will be visible for other project contributors - even ones from other organizations - to your project to query and inspect. By inspecting the schema property, other project contributors are able to identify which data columns should be used to train the model. If you provide a dataset to Cape that is publicly accessible via HTTP, Cape will attempt to preview your data and create a schema. However, if your dataset is not publicly accessible you'll have to specify your data's schema yourself. You can do so using the schema parameter. DataViews can be instantiated with a pandas Series schema of type dataframe.dftypes : >>> import pandas as pd >>> df = pd . DataFrame ( data = { \"col_1\" : [ 1 , 2 ], \"col_2\" : [ 3 , 4 ]}) >>> dataview = my_project . create_dataview ( name = \"my-data\" , uri = \"s3://my-data.csv\" , owner_label = \"my-org\" , schema = df . dtypes ) >>> dataview . schema { 'col_1' : 'integer' , 'col_2' : 'integer' } DataViews can also be instantiated as a list of data types. Accepted schema data types include: string , integer , number , datetime , and any . >>> schema = [{ \"name\" : \"col_1\" , \"schema_type\" : \"integer\" }, { \"name\" : \"col_2\" , \"schema_type\" : \"integer\" }] >>> data_view = DataView ( name = \"my-data\" , uri = \"s3://my-data.csv\" , schema = schema ) >>> dataview . schema { 'col_1' : 'integer' , 'col_2' : 'integer' } Delete a DataView # my_project = c . get_project ( id = \"project_123\" ) my_project . delete_dataview ( id = \"dataview_123\" ) Default response: DataView ( dataview_123 ) deleted","title":"DataViews"},{"location":"libraries/pycape/usage/dataview/#managing-dataviews","text":"","title":"Managing DataViews"},{"location":"libraries/pycape/usage/dataview/#get-list-of-data-views-for-a-project","text":"my_project = c . get_project ( id = \"project_123\" ) my_project . list_dataviews () Default response: DATAVIEW ID NAME LOCATION OWNER -------------------------- ------------- --------------- ----------- 01EY48EFT4H7PWAN45SG2AEZ81 armazorn-data s3://mydata.csv armazorn ( You ) 01EY49J86722ENT9JSMKTE65EX gorgle-data gorgle","title":"Get list of data views for a project"},{"location":"libraries/pycape/usage/dataview/#get-a-data-view","text":"my_project = c . get_project ( id = \"project_123\" ) # get by id my_project . get_dataview ( id = \"dataview_123\" ) # get by uri my_project . get_dataview ( uri = \"s3://my-data.csv\" ) Default response: DataView ( id = dataview_123, name = my-data, location = s3://my-data.csv )","title":"Get a data view"},{"location":"libraries/pycape/usage/dataview/#add-a-data-view-to-a-project","text":"Initialize a DataView class and pass the instance to the create_dataview method. my_project = c . get_project ( id = \"project_123\" ) my_project . create_dataview ( name = \"my-data\" , uri = \"s3://my-data.csv\" , owner_label = \"my-org\" ) Default response: DataView ( id = dataview_123, name = my-data, location = s3://my-data.csv )","title":"Add a data view to a project"},{"location":"libraries/pycape/usage/dataview/#specifying-a-schema-for-your-dataview","text":"DataView schemas allow you to clarify the data types of your dataset. They will be visible for other project contributors - even ones from other organizations - to your project to query and inspect. By inspecting the schema property, other project contributors are able to identify which data columns should be used to train the model. If you provide a dataset to Cape that is publicly accessible via HTTP, Cape will attempt to preview your data and create a schema. However, if your dataset is not publicly accessible you'll have to specify your data's schema yourself. You can do so using the schema parameter. DataViews can be instantiated with a pandas Series schema of type dataframe.dftypes : >>> import pandas as pd >>> df = pd . DataFrame ( data = { \"col_1\" : [ 1 , 2 ], \"col_2\" : [ 3 , 4 ]}) >>> dataview = my_project . create_dataview ( name = \"my-data\" , uri = \"s3://my-data.csv\" , owner_label = \"my-org\" , schema = df . dtypes ) >>> dataview . schema { 'col_1' : 'integer' , 'col_2' : 'integer' } DataViews can also be instantiated as a list of data types. Accepted schema data types include: string , integer , number , datetime , and any . >>> schema = [{ \"name\" : \"col_1\" , \"schema_type\" : \"integer\" }, { \"name\" : \"col_2\" , \"schema_type\" : \"integer\" }] >>> data_view = DataView ( name = \"my-data\" , uri = \"s3://my-data.csv\" , schema = schema ) >>> dataview . schema { 'col_1' : 'integer' , 'col_2' : 'integer' }","title":"Specifying a Schema for your DataView"},{"location":"libraries/pycape/usage/dataview/#delete-a-dataview","text":"my_project = c . get_project ( id = \"project_123\" ) my_project . delete_dataview ( id = \"dataview_123\" ) Default response: DataView ( dataview_123 ) deleted","title":"Delete a DataView"},{"location":"libraries/pycape/usage/job/","text":"Managing Jobs # Submit a job # my_project = c . get_project ( id = \"project_123\" ) dataview_1 = my_project . get_dataview ( uri = \"s3://my-data.csv\" ) dataview_2 = my_project . get_dataview ( uri = \"s3://my-data-2.csv\" ) vlr = VerticallyPartitionedLinearRegression ( x_train_dataview = dataview_1 , y_train_dataview = dataview_2 , ) my_project . submit_job ( job = vlr ) Default response: Job ( id = abc_123, job_type = LINEAR_REGRESSION, status = Created ) Get a Job's Status # lr_job = my_project . get_job ( id = \"abc_123\" ) lr_job . get_status () Default response: Created Get a Job's Results # lr_job = my_project . get_job ( id = \"abc_123\" ) weights , metrics = lr_job . get_results () Default response: ( array ([ 12 .14955139, 1 .96560669 ]) , { 'r_squared_result' : [ 0 .8804865768463074 ] , 'mse_result' : [ 37 .94773864746094 ]}) Accessing Weights as a Model Owner in Cape # pycape uses boto to access the model weights in your S3 bucket. You'll need to inform pycape about your IAM authentication credentials. Cape expects values for the following AWS configuration keys: AWS_ACCESS_KEY_ID , AWS_SECRET_ACCESS_KEY , and AWS_REGION . You can set these keys as environment variables in the interpreter running pycape : export AWS_ACCESS_KEY_ID = <Access-Key> export AWS_SECRET_ACCESS_KEY = <Secret-Key> export AWS_REGION = <Region> Alternatively you can simply add these keys to your AWS Configuration file .","title":"Jobs"},{"location":"libraries/pycape/usage/job/#managing-jobs","text":"","title":"Managing Jobs"},{"location":"libraries/pycape/usage/job/#submit-a-job","text":"my_project = c . get_project ( id = \"project_123\" ) dataview_1 = my_project . get_dataview ( uri = \"s3://my-data.csv\" ) dataview_2 = my_project . get_dataview ( uri = \"s3://my-data-2.csv\" ) vlr = VerticallyPartitionedLinearRegression ( x_train_dataview = dataview_1 , y_train_dataview = dataview_2 , ) my_project . submit_job ( job = vlr ) Default response: Job ( id = abc_123, job_type = LINEAR_REGRESSION, status = Created )","title":"Submit a job"},{"location":"libraries/pycape/usage/job/#get-a-jobs-status","text":"lr_job = my_project . get_job ( id = \"abc_123\" ) lr_job . get_status () Default response: Created","title":"Get a Job's Status"},{"location":"libraries/pycape/usage/job/#get-a-jobs-results","text":"lr_job = my_project . get_job ( id = \"abc_123\" ) weights , metrics = lr_job . get_results () Default response: ( array ([ 12 .14955139, 1 .96560669 ]) , { 'r_squared_result' : [ 0 .8804865768463074 ] , 'mse_result' : [ 37 .94773864746094 ]})","title":"Get a Job's Results"},{"location":"libraries/pycape/usage/job/#accessing-weights-as-a-model-owner-in-cape","text":"pycape uses boto to access the model weights in your S3 bucket. You'll need to inform pycape about your IAM authentication credentials. Cape expects values for the following AWS configuration keys: AWS_ACCESS_KEY_ID , AWS_SECRET_ACCESS_KEY , and AWS_REGION . You can set these keys as environment variables in the interpreter running pycape : export AWS_ACCESS_KEY_ID = <Access-Key> export AWS_SECRET_ACCESS_KEY = <Secret-Key> export AWS_REGION = <Region> Alternatively you can simply add these keys to your AWS Configuration file .","title":"Accessing Weights as a Model Owner in Cape"},{"location":"libraries/pycape/usage/login/","text":"Login to PyCape # To use pycape , you'll need to generate a user token . You'll use this token to authenticate requests to Cape Cloud. After setting up an account in Cape , ensure you are working within your user context and navigate to Account Settings to generate a user token. Take note of this value as you cannot recover it after you reload the page. c = Cape () c . login ( token = \"abc,123\" , endpoint = \"http://cape.com\" ) It is also possible to set your Auth Token and Coordinator endpoint via the environment variables CAPE_TOKEN and CAPE_COORDINATOR . # Call the login method after exporting CAPE_TOKEN and CAPE_COORDINATOR. c = Cape () c . login () Default response: Login successful","title":"Login"},{"location":"libraries/pycape/usage/login/#login-to-pycape","text":"To use pycape , you'll need to generate a user token . You'll use this token to authenticate requests to Cape Cloud. After setting up an account in Cape , ensure you are working within your user context and navigate to Account Settings to generate a user token. Take note of this value as you cannot recover it after you reload the page. c = Cape () c . login ( token = \"abc,123\" , endpoint = \"http://cape.com\" ) It is also possible to set your Auth Token and Coordinator endpoint via the environment variables CAPE_TOKEN and CAPE_COORDINATOR . # Call the login method after exporting CAPE_TOKEN and CAPE_COORDINATOR. c = Cape () c . login () Default response: Login successful","title":"Login to PyCape"},{"location":"libraries/pycape/usage/project/","text":"Manage Projects # Using pycape you can create, delete, or query for your Cape projects. List Projects # c . list_projects () Default response: PROJECT ID NAME LABEL ----------- ------------------ ------------------ project_123 Sales Transactions sales-transactions Get Project # # Get project by ID c . get_project ( id = \"project_123\" ) # Get project by label c . get_project ( label = \"my-project\" ) Default response: Project ( id = project_123, name = My Project, label = my-project ) Create a Project # c . create_project ( name = \"My Project\" owner = \"org_123\" description = \"Linear Regression model with amazorn.\" ) Default response: Project ( id = project_123, name = My Project, label = my-project ) Delete a Project # c . delete_project ( id = \"project_123\" ) Default response: Project ( project_123 ) deleted","title":"Projects"},{"location":"libraries/pycape/usage/project/#manage-projects","text":"Using pycape you can create, delete, or query for your Cape projects.","title":"Manage Projects"},{"location":"libraries/pycape/usage/project/#list-projects","text":"c . list_projects () Default response: PROJECT ID NAME LABEL ----------- ------------------ ------------------ project_123 Sales Transactions sales-transactions","title":"List Projects"},{"location":"libraries/pycape/usage/project/#get-project","text":"# Get project by ID c . get_project ( id = \"project_123\" ) # Get project by label c . get_project ( label = \"my-project\" ) Default response: Project ( id = project_123, name = My Project, label = my-project )","title":"Get Project"},{"location":"libraries/pycape/usage/project/#create-a-project","text":"c . create_project ( name = \"My Project\" owner = \"org_123\" description = \"Linear Regression model with amazorn.\" ) Default response: Project ( id = project_123, name = My Project, label = my-project )","title":"Create a Project"},{"location":"libraries/pycape/usage/project/#delete-a-project","text":"c . delete_project ( id = \"project_123\" ) Default response: Project ( project_123 ) deleted","title":"Delete a Project"},{"location":"pythonv1/readme/","text":"Cape Python # A Python library supporting data transformations and collaborative privacy policies, for data science projects in Pandas and Apache Spark See below for instructions on how to get started or visit the documentation . Getting started # Prerequisites # Python 3.6 or above, and pip Pandas 1.0+ PySpark 3.0+ (if using Spark) Make (if installing from source) Install with pip # Cape Python is available through PyPi. pip install cape-privacy Support for Apache Spark is optional. If you plan on using the library together with Apache Spark, we suggest the following instead: pip install cape-privacy [ spark ] We recommend running it in a virtual environment, such as venv . Install from source # It is possible to install the library from source. This installs all dependencies, including Apache Spark: git clone https://github.com/capeprivacy/cape-python.git cd cape-python make bootstrap Usage example # This example is an abridged version of the tutorial found here df = pd . DataFrame ({ \"name\" : [ \"alice\" , \"bob\" ], \"age\" : [ 34 , 55 ], \"birthdate\" : [ pd . Timestamp ( 1985 , 2 , 23 ), pd . Timestamp ( 1963 , 5 , 10 )], }) tokenize = Tokenizer ( max_token_len = 10 , key = b \"my secret\" ) perturb_numeric = NumericPerturbation ( dtype = dtypes . Integer , min =- 10 , max = 10 ) df [ \"name\" ] = tokenize ( df [ \"name\" ]) df [ \"age\" ] = perturb_numeric ( df [ \"age\" ]) print ( df . head ()) # >> # name age birthdate # 0 f42c2f1964 34 1985-02-23 # 1 2e586494b2 63 1963-05-10 These steps can be saved in policy files so you can share them and collaborate with your team: # my-policy.yaml label : my-policy version : 1 rules : - match : name : age actions : - transform : type : numeric-perturbation dtype : Integer min : -10 max : 10 seed : 4984 - match : name : name actions : - transform : type : tokenizer max_token_len : 10 key : my secret You can then load this policy and apply it to your data frame: # df can be a Pandas or Spark data frame policy = cape . parse_policy ( \"my-policy.yaml\" ) df = cape . apply_policy ( policy , df ) print ( df . head ()) # >> # name age birthdate # 0 f42c2f1964 34 1985-02-23 # 1 2e586494b2 63 1963-05-10 You can see more examples and usage here or in our documentation . About Cape Privacy and Cape Python # Cape Privacy helps teams share data and make decisions for safer and more powerful data science. Learn more at capeprivacy.com . Cape Python brings Cape's policy language to Pandas and Apache Spark. The supported techniques include tokenization with linkability as well as perturbation and rounding. You can experiment with these techniques programmatically, in Python or in human-readable policy files. Cape architecture # Cape is comprised of multiples services and libraries. You can use Cape Python as a standalone library, or you can integrate it with the Coordinator in Cape Core , which supports user and policy management. Project status and roadmap # Cape Python 0.1.1 was released 24th June 2020. It is actively maintained and developed, alongside other elements of the Cape ecosystem. Upcoming features: Reversible tokenisation: allow reversing of tokenization to reveal the raw value. Policy audit logging: create logging hooks to allow audit logs for policy downloads and usage in Cape Python. Expand pipeline integrations: add Apache Beam, Apache Flink, Apache Arrow Flight or Dask integration as another pipeline we can support, either as part of Cape Python or in its own separate project. The goal is a complete data management ecosystem. Cape Privacy provides Cape Coordinator , to manage policy and users. This will interact with the Cape Privacy libraries (such as Cape Python ) through a workers interface, and with your own data services through an API. Help and resources # If you need help using Cape Python, you can: View the documentation . Submit an issue. Talk to us on our community Slack . Please file feature requests and bug reports as GitHub issues. Community # Contributing # View our contributing guide for more information. Code of conduct # Our code of conduct is included on the Cape Privacy website. All community members are expected to follow it. Please refer to that page for information on how to report problems. License # Licensed under Apache License, Version 2.0 (see LICENSE or http://www.apache.org/licenses/LICENSE-2.0). Copyright as specified in NOTICE .","title":"README"},{"location":"pythonv1/readme/#cape-python","text":"A Python library supporting data transformations and collaborative privacy policies, for data science projects in Pandas and Apache Spark See below for instructions on how to get started or visit the documentation .","title":"Cape Python"},{"location":"pythonv1/readme/#getting-started","text":"","title":"Getting started"},{"location":"pythonv1/readme/#prerequisites","text":"Python 3.6 or above, and pip Pandas 1.0+ PySpark 3.0+ (if using Spark) Make (if installing from source)","title":"Prerequisites"},{"location":"pythonv1/readme/#install-with-pip","text":"Cape Python is available through PyPi. pip install cape-privacy Support for Apache Spark is optional. If you plan on using the library together with Apache Spark, we suggest the following instead: pip install cape-privacy [ spark ] We recommend running it in a virtual environment, such as venv .","title":"Install with pip"},{"location":"pythonv1/readme/#install-from-source","text":"It is possible to install the library from source. This installs all dependencies, including Apache Spark: git clone https://github.com/capeprivacy/cape-python.git cd cape-python make bootstrap","title":"Install from source"},{"location":"pythonv1/readme/#usage-example","text":"This example is an abridged version of the tutorial found here df = pd . DataFrame ({ \"name\" : [ \"alice\" , \"bob\" ], \"age\" : [ 34 , 55 ], \"birthdate\" : [ pd . Timestamp ( 1985 , 2 , 23 ), pd . Timestamp ( 1963 , 5 , 10 )], }) tokenize = Tokenizer ( max_token_len = 10 , key = b \"my secret\" ) perturb_numeric = NumericPerturbation ( dtype = dtypes . Integer , min =- 10 , max = 10 ) df [ \"name\" ] = tokenize ( df [ \"name\" ]) df [ \"age\" ] = perturb_numeric ( df [ \"age\" ]) print ( df . head ()) # >> # name age birthdate # 0 f42c2f1964 34 1985-02-23 # 1 2e586494b2 63 1963-05-10 These steps can be saved in policy files so you can share them and collaborate with your team: # my-policy.yaml label : my-policy version : 1 rules : - match : name : age actions : - transform : type : numeric-perturbation dtype : Integer min : -10 max : 10 seed : 4984 - match : name : name actions : - transform : type : tokenizer max_token_len : 10 key : my secret You can then load this policy and apply it to your data frame: # df can be a Pandas or Spark data frame policy = cape . parse_policy ( \"my-policy.yaml\" ) df = cape . apply_policy ( policy , df ) print ( df . head ()) # >> # name age birthdate # 0 f42c2f1964 34 1985-02-23 # 1 2e586494b2 63 1963-05-10 You can see more examples and usage here or in our documentation .","title":"Usage example"},{"location":"pythonv1/readme/#about-cape-privacy-and-cape-python","text":"Cape Privacy helps teams share data and make decisions for safer and more powerful data science. Learn more at capeprivacy.com . Cape Python brings Cape's policy language to Pandas and Apache Spark. The supported techniques include tokenization with linkability as well as perturbation and rounding. You can experiment with these techniques programmatically, in Python or in human-readable policy files.","title":"About Cape Privacy and Cape Python"},{"location":"pythonv1/readme/#cape-architecture","text":"Cape is comprised of multiples services and libraries. You can use Cape Python as a standalone library, or you can integrate it with the Coordinator in Cape Core , which supports user and policy management.","title":"Cape architecture"},{"location":"pythonv1/readme/#project-status-and-roadmap","text":"Cape Python 0.1.1 was released 24th June 2020. It is actively maintained and developed, alongside other elements of the Cape ecosystem. Upcoming features: Reversible tokenisation: allow reversing of tokenization to reveal the raw value. Policy audit logging: create logging hooks to allow audit logs for policy downloads and usage in Cape Python. Expand pipeline integrations: add Apache Beam, Apache Flink, Apache Arrow Flight or Dask integration as another pipeline we can support, either as part of Cape Python or in its own separate project. The goal is a complete data management ecosystem. Cape Privacy provides Cape Coordinator , to manage policy and users. This will interact with the Cape Privacy libraries (such as Cape Python ) through a workers interface, and with your own data services through an API.","title":"Project status and roadmap"},{"location":"pythonv1/readme/#help-and-resources","text":"If you need help using Cape Python, you can: View the documentation . Submit an issue. Talk to us on our community Slack . Please file feature requests and bug reports as GitHub issues.","title":"Help and resources"},{"location":"pythonv1/readme/#community","text":"","title":"Community"},{"location":"pythonv1/readme/#contributing","text":"View our contributing guide for more information.","title":"Contributing"},{"location":"pythonv1/readme/#code-of-conduct","text":"Our code of conduct is included on the Cape Privacy website. All community members are expected to follow it. Please refer to that page for information on how to report problems.","title":"Code of conduct"},{"location":"pythonv1/readme/#license","text":"Licensed under Apache License, Version 2.0 (see LICENSE or http://www.apache.org/licenses/LICENSE-2.0). Copyright as specified in NOTICE .","title":"License"},{"location":"pythonv1/cape_privacy.pandas/dtypes/","text":"cape_privacy.pandas.dtypes #","title":"dtypes"},{"location":"pythonv1/cape_privacy.pandas/dtypes/#cape_privacypandasdtypes","text":"","title":"cape_privacy.pandas.dtypes"},{"location":"pythonv1/cape_privacy.pandas/registry/","text":"cape_privacy.pandas.registry # get # get ( transformation : str ) -> TransformationCtor Returns the constructor for the given key. Arguments : transformation - The key of transformation to retrieve. register # register ( label : str , ctor : TransformationCtor ) Registers a new transformation constructor under the label provided. Arguments : label - The label that will be used as the key in the registry ctor - The transformation constructor","title":"registry"},{"location":"pythonv1/cape_privacy.pandas/registry/#cape_privacypandasregistry","text":"","title":"cape_privacy.pandas.registry"},{"location":"pythonv1/cape_privacy.pandas/registry/#get","text":"get ( transformation : str ) -> TransformationCtor Returns the constructor for the given key. Arguments : transformation - The key of transformation to retrieve.","title":"get"},{"location":"pythonv1/cape_privacy.pandas/registry/#register","text":"register ( label : str , ctor : TransformationCtor ) Registers a new transformation constructor under the label provided. Arguments : label - The label that will be used as the key in the registry ctor - The transformation constructor","title":"register"},{"location":"pythonv1/cape_privacy.pandas/transformations/column_redact/","text":"cape_privacy.pandas.transformations.column_redact # ColumnRedact Objects # class ColumnRedact () Redacts columns. Attributes : columns - The columns to redact.","title":"column_redact"},{"location":"pythonv1/cape_privacy.pandas/transformations/column_redact/#cape_privacypandastransformationscolumn_redact","text":"","title":"cape_privacy.pandas.transformations.column_redact"},{"location":"pythonv1/cape_privacy.pandas/transformations/column_redact/#columnredact-objects","text":"class ColumnRedact () Redacts columns. Attributes : columns - The columns to redact.","title":"ColumnRedact Objects"},{"location":"pythonv1/cape_privacy.pandas/transformations/perturbation/","text":"cape_privacy.pandas.transformations.perturbation # NumericPerturbation Objects # class NumericPerturbation ( base . Transformation ) Add uniform random noise to a numeric Pandas series Mask a numeric Pandas series by adding uniform random noise to each value. The amount of noise is drawn from the interval [min, max). Example : s = pd.Series([0, 1, 2, 3, 4]) perturb = NumericPerturbation(dtype=Integer, min=-10, max=10, seed=123) perturb(s) # pd.Series([3, -7, -3, -3]) Attributes : dtype dtypes.Numerics - Pandas Series type min int, float - the values generated will be greater then or equal to min max int, float - the values generated will be less than max seed (int), optional: a seed to initialize the random generator DatePerturbation Objects # class DatePerturbation ( base . Transformation ) Add uniform random noise to a Pandas series of timestamps Mask a Pandas series by adding uniform random noise to the specified frequencies of timestamps. The amount of noise for each frequency is drawn from the internal [min_freq, max_freq). Example : s = pd.Series([datetime.date(year=2020, month=2, day=15)]) perturb = DatePerturbation(frequency=\"MONTH\", min=-10, max=10, seed=1234) perturb(s) # pd.Series([datetime.date(year=2020, month=11, day=11)]) Attributes : frequency str, str list - one or more frequencies to perturbate min int, int list - the frequency value will be greater or equal to min max int, int list - the frequency value will be less than max seed (int), optional: a seed to initialize the random generator","title":"perturbation"},{"location":"pythonv1/cape_privacy.pandas/transformations/perturbation/#cape_privacypandastransformationsperturbation","text":"","title":"cape_privacy.pandas.transformations.perturbation"},{"location":"pythonv1/cape_privacy.pandas/transformations/perturbation/#numericperturbation-objects","text":"class NumericPerturbation ( base . Transformation ) Add uniform random noise to a numeric Pandas series Mask a numeric Pandas series by adding uniform random noise to each value. The amount of noise is drawn from the interval [min, max). Example : s = pd.Series([0, 1, 2, 3, 4]) perturb = NumericPerturbation(dtype=Integer, min=-10, max=10, seed=123) perturb(s) # pd.Series([3, -7, -3, -3]) Attributes : dtype dtypes.Numerics - Pandas Series type min int, float - the values generated will be greater then or equal to min max int, float - the values generated will be less than max seed (int), optional: a seed to initialize the random generator","title":"NumericPerturbation Objects"},{"location":"pythonv1/cape_privacy.pandas/transformations/perturbation/#dateperturbation-objects","text":"class DatePerturbation ( base . Transformation ) Add uniform random noise to a Pandas series of timestamps Mask a Pandas series by adding uniform random noise to the specified frequencies of timestamps. The amount of noise for each frequency is drawn from the internal [min_freq, max_freq). Example : s = pd.Series([datetime.date(year=2020, month=2, day=15)]) perturb = DatePerturbation(frequency=\"MONTH\", min=-10, max=10, seed=1234) perturb(s) # pd.Series([datetime.date(year=2020, month=11, day=11)]) Attributes : frequency str, str list - one or more frequencies to perturbate min int, int list - the frequency value will be greater or equal to min max int, int list - the frequency value will be less than max seed (int), optional: a seed to initialize the random generator","title":"DatePerturbation Objects"},{"location":"pythonv1/cape_privacy.pandas/transformations/rounding/","text":"cape_privacy.pandas.transformations.rounding # NumericRounding Objects # class NumericRounding ( base . Transformation ) Reduce the precision of a numeric Pandas Series Round each value in the Pandas Series to the given number of digits. Example : s = pd.Series([1.384]) round = NumericRounding(precision=1) round(s) # pd.Series([1.4]) Attributes : dtypes dtypes.Numerics - Pandas Series type. precision int - set the number of digits. __call__ # | __call__ ( x : pd . Series ) -> pd . Series Round each value in the Pandas Series Arguments : x A Pandas Series - need to be a list of numeric values. Returns : A Pandas Series with each value rounded DateTruncation Objects # class DateTruncation ( base . Transformation ) Reduce the precision of a date Pandas Series Truncate each date in a Pandas Series to the unit (year or month) specified by frequency. Example : s = pd.Series([pd.Timestamp(\"2018-10-15\")]) trunc = DateTruncation(frequency=\"year\") trunc(s) # pd.Serie([pd.Timestamp(\"2018-01-01\")]) Attributes : frequency string - expect to be 'year' or 'month'","title":"rounding"},{"location":"pythonv1/cape_privacy.pandas/transformations/rounding/#cape_privacypandastransformationsrounding","text":"","title":"cape_privacy.pandas.transformations.rounding"},{"location":"pythonv1/cape_privacy.pandas/transformations/rounding/#numericrounding-objects","text":"class NumericRounding ( base . Transformation ) Reduce the precision of a numeric Pandas Series Round each value in the Pandas Series to the given number of digits. Example : s = pd.Series([1.384]) round = NumericRounding(precision=1) round(s) # pd.Series([1.4]) Attributes : dtypes dtypes.Numerics - Pandas Series type. precision int - set the number of digits.","title":"NumericRounding Objects"},{"location":"pythonv1/cape_privacy.pandas/transformations/rounding/#__call__","text":"| __call__ ( x : pd . Series ) -> pd . Series Round each value in the Pandas Series Arguments : x A Pandas Series - need to be a list of numeric values. Returns : A Pandas Series with each value rounded","title":"__call__"},{"location":"pythonv1/cape_privacy.pandas/transformations/rounding/#datetruncation-objects","text":"class DateTruncation ( base . Transformation ) Reduce the precision of a date Pandas Series Truncate each date in a Pandas Series to the unit (year or month) specified by frequency. Example : s = pd.Series([pd.Timestamp(\"2018-10-15\")]) trunc = DateTruncation(frequency=\"year\") trunc(s) # pd.Serie([pd.Timestamp(\"2018-01-01\")]) Attributes : frequency string - expect to be 'year' or 'month'","title":"DateTruncation Objects"},{"location":"pythonv1/cape_privacy.pandas/transformations/row_redact/","text":"cape_privacy.pandas.transformations.row_redact # RowRedact Objects # class RowRedact () Redacts rows based on the condition. Attributes : condition - The condition to be passed into the query function. __call__ # | __call__ ( df : pd . DataFrame ) -> pd . DataFrame Redacts rows using Dataframe.query. DataFrame.query returns all the fields that it matches so we negate it here to get the opposite.","title":"row_redact"},{"location":"pythonv1/cape_privacy.pandas/transformations/row_redact/#cape_privacypandastransformationsrow_redact","text":"","title":"cape_privacy.pandas.transformations.row_redact"},{"location":"pythonv1/cape_privacy.pandas/transformations/row_redact/#rowredact-objects","text":"class RowRedact () Redacts rows based on the condition. Attributes : condition - The condition to be passed into the query function.","title":"RowRedact Objects"},{"location":"pythonv1/cape_privacy.pandas/transformations/row_redact/#__call__","text":"| __call__ ( df : pd . DataFrame ) -> pd . DataFrame Redacts rows using Dataframe.query. DataFrame.query returns all the fields that it matches so we negate it here to get the opposite.","title":"__call__"},{"location":"pythonv1/cape_privacy.pandas/transformations/tokenizer/","text":"cape_privacy.pandas.transformations.tokenizer # Tokenizer Objects # class Tokenizer ( base . Transformation ) Tokenizer: map a string to a token to obfuscate it. When applying the Tokenizer to a Pandas Series of type string, each value gets mapped to a token (hexadecimal string). If a value is repeated several times across the series, it always get mapped to the same token in order to maintain the count. A value can be mapped to different tokens by setting the key to a different value. Example : s = pd.Series(['A']) tokenize = Tokenizer(max_token_len=5, key='secret') tokenize(s) # pd.Series(['40a1e']) Attributes : max_token_len int or bytes - control the token length (default length is 64) key - expect a string or byte string. If not specified, key will be set to a random byte string. __call__ # | __call__ ( series : pd . Series ) -> pd . Series Map a Pandas Series to tokens. Arguments : series A Pandas Series - need to be a list of strings. Returns : A Pandas Series with a list of tokens represented as hexadecimal strings. ReversibleTokenizer Objects # class ReversibleTokenizer ( base . Transformation ) ReversibleTokenizer: map a string to a token to obfuscate it. When applying the Tokenizer to a Pandas Series of type string, each value gets mapped to a token (hexadecimal string). If a value is repeated several times across the series, it always get mapped to the same token in order to maintain the count. A value can be mapped to different tokens by setting the key to a different value. This tokenizer allows tokens to be reversed to their original data when the secret key is known. Example : s = pd.Series(['A']) tokenize = ReversibleTokenizer(key='secret') tokenize(s) # pd.Series(['40a1e']) Attributes : key - expect a string or byte string of length exactly 32 bytes. encoding - string identifying the Python encoding used for inputs. __call__ # | __call__ ( series : pd . Series ) -> pd . Series Map a Pandas Series to tokens. Arguments : series A Pandas Series - need to be a list of strings. Returns : A Pandas Series with a list of tokens represented as hexadecimal strings. TokenReverser Objects # class TokenReverser ( base . Transformation ) TokenReverser: recover string from token. When applying the TokenReverser to a Pandas Series of tokens, each token is mapped back to the string that was originally used by ReversibleTokenizer to construct the token. The same key must be used. Example : s = pd.Series(['40a1e']) reverser = TokenReverser(key='secret') reverser(s) # pd.Series(['A']) Attributes : key - expect a string or byte string of length exactly 32 bytes. encoding - string identifying the Python encoding used for outputs. __call__ # | __call__ ( series : pd . Series ) -> pd . Series Reverse a Pandas Series of tokens. Arguments : series A Pandas Series - need to be a list of strings. Returns : A Pandas Series with a list of recovered strings.","title":"tokenizer"},{"location":"pythonv1/cape_privacy.pandas/transformations/tokenizer/#cape_privacypandastransformationstokenizer","text":"","title":"cape_privacy.pandas.transformations.tokenizer"},{"location":"pythonv1/cape_privacy.pandas/transformations/tokenizer/#tokenizer-objects","text":"class Tokenizer ( base . Transformation ) Tokenizer: map a string to a token to obfuscate it. When applying the Tokenizer to a Pandas Series of type string, each value gets mapped to a token (hexadecimal string). If a value is repeated several times across the series, it always get mapped to the same token in order to maintain the count. A value can be mapped to different tokens by setting the key to a different value. Example : s = pd.Series(['A']) tokenize = Tokenizer(max_token_len=5, key='secret') tokenize(s) # pd.Series(['40a1e']) Attributes : max_token_len int or bytes - control the token length (default length is 64) key - expect a string or byte string. If not specified, key will be set to a random byte string.","title":"Tokenizer Objects"},{"location":"pythonv1/cape_privacy.pandas/transformations/tokenizer/#__call__","text":"| __call__ ( series : pd . Series ) -> pd . Series Map a Pandas Series to tokens. Arguments : series A Pandas Series - need to be a list of strings. Returns : A Pandas Series with a list of tokens represented as hexadecimal strings.","title":"__call__"},{"location":"pythonv1/cape_privacy.pandas/transformations/tokenizer/#reversibletokenizer-objects","text":"class ReversibleTokenizer ( base . Transformation ) ReversibleTokenizer: map a string to a token to obfuscate it. When applying the Tokenizer to a Pandas Series of type string, each value gets mapped to a token (hexadecimal string). If a value is repeated several times across the series, it always get mapped to the same token in order to maintain the count. A value can be mapped to different tokens by setting the key to a different value. This tokenizer allows tokens to be reversed to their original data when the secret key is known. Example : s = pd.Series(['A']) tokenize = ReversibleTokenizer(key='secret') tokenize(s) # pd.Series(['40a1e']) Attributes : key - expect a string or byte string of length exactly 32 bytes. encoding - string identifying the Python encoding used for inputs.","title":"ReversibleTokenizer Objects"},{"location":"pythonv1/cape_privacy.pandas/transformations/tokenizer/#__call___1","text":"| __call__ ( series : pd . Series ) -> pd . Series Map a Pandas Series to tokens. Arguments : series A Pandas Series - need to be a list of strings. Returns : A Pandas Series with a list of tokens represented as hexadecimal strings.","title":"__call__"},{"location":"pythonv1/cape_privacy.pandas/transformations/tokenizer/#tokenreverser-objects","text":"class TokenReverser ( base . Transformation ) TokenReverser: recover string from token. When applying the TokenReverser to a Pandas Series of tokens, each token is mapped back to the string that was originally used by ReversibleTokenizer to construct the token. The same key must be used. Example : s = pd.Series(['40a1e']) reverser = TokenReverser(key='secret') reverser(s) # pd.Series(['A']) Attributes : key - expect a string or byte string of length exactly 32 bytes. encoding - string identifying the Python encoding used for outputs.","title":"TokenReverser Objects"},{"location":"pythonv1/cape_privacy.pandas/transformations/tokenizer/#__call___2","text":"| __call__ ( series : pd . Series ) -> pd . Series Reverse a Pandas Series of tokens. Arguments : series A Pandas Series - need to be a list of strings. Returns : A Pandas Series with a list of recovered strings.","title":"__call__"},{"location":"pythonv1/cape_privacy.policy/data/","text":"cape_privacy.policy.data # Contains the policy classes that are initialized from a yaml policy file. There are five main classes with Policy being the top level class. Policy contains the PolicySpec and NamedTransformations. PolicySpec contains Rules and Rules contain Transformations. Typical usage example: yaml_str = \"....\" d = yaml.load(yaml_str, Loader=yaml.FullLoad) # **d unpacks the dictionary produced by yaml and # passes them in has keyword arguments. policy = Policy(**d) Transform Objects # class Transform () A actual transform that will be applied. Either named or function must be passed in here. The process to apply this transform will look at both function and named and apply the relevant one. Attributes : field - The field this transform will be applied to. name - The name of the named transform, referenced from the top level policy object. type - The builtin transform that will be initialized. kwargs - The rest of the arguments that will be passed to the transformation. Rule Objects # class Rule () A rule contains actionable information of a policy. Attributes : match - The match used to select a field to be transformed. actions - The actions to take on a matched field. NamedTransform Objects # class NamedTransform () A named transformation that captures the args. Attributes : name - The name of the named transformation. type - The builtin type (i.e. transform) that the named transform initializes to. kwargs - The args that are captured by the named transform. Policy Objects # class Policy () Top level policy object. The top level policy object holds the all of the relevant information for applying policy to data. Attributes : label - The label of the policy. version - The version of the policy. rules - List of rules that will be applied to a data frame. transformations - The named transformations for this policy.","title":"data"},{"location":"pythonv1/cape_privacy.policy/data/#cape_privacypolicydata","text":"Contains the policy classes that are initialized from a yaml policy file. There are five main classes with Policy being the top level class. Policy contains the PolicySpec and NamedTransformations. PolicySpec contains Rules and Rules contain Transformations. Typical usage example: yaml_str = \"....\" d = yaml.load(yaml_str, Loader=yaml.FullLoad) # **d unpacks the dictionary produced by yaml and # passes them in has keyword arguments. policy = Policy(**d)","title":"cape_privacy.policy.data"},{"location":"pythonv1/cape_privacy.policy/data/#transform-objects","text":"class Transform () A actual transform that will be applied. Either named or function must be passed in here. The process to apply this transform will look at both function and named and apply the relevant one. Attributes : field - The field this transform will be applied to. name - The name of the named transform, referenced from the top level policy object. type - The builtin transform that will be initialized. kwargs - The rest of the arguments that will be passed to the transformation.","title":"Transform Objects"},{"location":"pythonv1/cape_privacy.policy/data/#rule-objects","text":"class Rule () A rule contains actionable information of a policy. Attributes : match - The match used to select a field to be transformed. actions - The actions to take on a matched field.","title":"Rule Objects"},{"location":"pythonv1/cape_privacy.policy/data/#namedtransform-objects","text":"class NamedTransform () A named transformation that captures the args. Attributes : name - The name of the named transformation. type - The builtin type (i.e. transform) that the named transform initializes to. kwargs - The args that are captured by the named transform.","title":"NamedTransform Objects"},{"location":"pythonv1/cape_privacy.policy/data/#policy-objects","text":"class Policy () Top level policy object. The top level policy object holds the all of the relevant information for applying policy to data. Attributes : label - The label of the policy. version - The version of the policy. rules - List of rules that will be applied to a data frame. transformations - The named transformations for this policy.","title":"Policy Objects"},{"location":"pythonv1/cape_privacy.policy/policy/","text":"cape_privacy.policy.policy # Utils for parsing policy and applying them. The module reads in policy as yaml and then through apply_policy applies them to dataframes. Example policy yaml: label: test_policy version: 1 rules: - match: name: value actions: # Tells the policy runner to apply the transformation # plusN with the specified arguments. - transform: type: plusN n: 1 # Tells the policy runner to apply another plusN # transformation. - transform: type: plusN n: 2 Applying policy: policy = parse_policy(\"policy.yaml\") df = pd.DataFrame(np.ones(5,), columns=[\"value\"]) df = apply_policy(policy, df) apply_policy # apply_policy ( policy : data . Policy , df , inplace = False ) Applies a Policy to some DataFrame. This function is responsible for inferring the type of the DataFrame, preparing the relevant Spark or Pandas Transformations, and applying them to produce a transformed DataFrame that conforms to the Policy. Arguments : policy - The Policy object that the transformed DataFrame will conform to, e.g. as returned by cape_privacy.parse_policy . df - The DataFrame object to transform according to policies . Must be of type pandas.DataFrame or pyspark.sql.DataFrame. inplace - Whether to mutate the df or produce a new one. This argument is only relevant for Pandas DataFrames, as Spark DataFrames do not support mutation. Raises : ValueError - If df is a Spark DataFrame and inplace=True, or if df is something other than a Pandas or Spark DataFrame. DependencyError - If Spark is not configured correctly in the Python environment. TransformNotFound, NamedTransformNotFound: If the Policy contains a reference to a Transformation or NamedTransformation that is unrecognized in the Transformation registry. parse_policy # parse_policy ( p : Union [ str , Dict [ Any , Any ]]) -> data . Policy Parses a policy YAML file. The passed in string can either be a path to a local file, a URL pointing to a file or a dictionary representing the policy. If it is a URL then requests attempts to download it. Arguments : p - a path string, a URL string or a dictionary representing the policy. Returns : The Policy object initialized by the YAML. reverse # reverse ( policy : data . Policy ) -> data . Policy Turns reversible tokenizations into token reversers If any named transformations contain a reversible tokenization transformation this helper function turns them into token reverser transformations. Arguments : policy - Top level policy object. Returns : The modified policy.","title":"policy"},{"location":"pythonv1/cape_privacy.policy/policy/#cape_privacypolicypolicy","text":"Utils for parsing policy and applying them. The module reads in policy as yaml and then through apply_policy applies them to dataframes. Example policy yaml: label: test_policy version: 1 rules: - match: name: value actions: # Tells the policy runner to apply the transformation # plusN with the specified arguments. - transform: type: plusN n: 1 # Tells the policy runner to apply another plusN # transformation. - transform: type: plusN n: 2 Applying policy: policy = parse_policy(\"policy.yaml\") df = pd.DataFrame(np.ones(5,), columns=[\"value\"]) df = apply_policy(policy, df)","title":"cape_privacy.policy.policy"},{"location":"pythonv1/cape_privacy.policy/policy/#apply_policy","text":"apply_policy ( policy : data . Policy , df , inplace = False ) Applies a Policy to some DataFrame. This function is responsible for inferring the type of the DataFrame, preparing the relevant Spark or Pandas Transformations, and applying them to produce a transformed DataFrame that conforms to the Policy. Arguments : policy - The Policy object that the transformed DataFrame will conform to, e.g. as returned by cape_privacy.parse_policy . df - The DataFrame object to transform according to policies . Must be of type pandas.DataFrame or pyspark.sql.DataFrame. inplace - Whether to mutate the df or produce a new one. This argument is only relevant for Pandas DataFrames, as Spark DataFrames do not support mutation. Raises : ValueError - If df is a Spark DataFrame and inplace=True, or if df is something other than a Pandas or Spark DataFrame. DependencyError - If Spark is not configured correctly in the Python environment. TransformNotFound, NamedTransformNotFound: If the Policy contains a reference to a Transformation or NamedTransformation that is unrecognized in the Transformation registry.","title":"apply_policy"},{"location":"pythonv1/cape_privacy.policy/policy/#parse_policy","text":"parse_policy ( p : Union [ str , Dict [ Any , Any ]]) -> data . Policy Parses a policy YAML file. The passed in string can either be a path to a local file, a URL pointing to a file or a dictionary representing the policy. If it is a URL then requests attempts to download it. Arguments : p - a path string, a URL string or a dictionary representing the policy. Returns : The Policy object initialized by the YAML.","title":"parse_policy"},{"location":"pythonv1/cape_privacy.policy/policy/#reverse","text":"reverse ( policy : data . Policy ) -> data . Policy Turns reversible tokenizations into token reversers If any named transformations contain a reversible tokenization transformation this helper function turns them into token reverser transformations. Arguments : policy - Top level policy object. Returns : The modified policy.","title":"reverse"},{"location":"pythonv1/cape_privacy.spark/dtypes/","text":"cape_privacy.spark.dtypes #","title":"dtypes"},{"location":"pythonv1/cape_privacy.spark/dtypes/#cape_privacysparkdtypes","text":"","title":"cape_privacy.spark.dtypes"},{"location":"pythonv1/cape_privacy.spark/registry/","text":"cape_privacy.spark.registry # get # get ( transformation : str ) -> TransformationCtor Returns the constructor for the given key. Arguments : transformation - The key of transformation to retrieve. register # register ( label : str , ctor : TransformationCtor ) Registers a new transformation constructor under the label provided. Arguments : label - The label that will be used as the key in the registry ctor - The transformation constructor","title":"registry"},{"location":"pythonv1/cape_privacy.spark/registry/#cape_privacysparkregistry","text":"","title":"cape_privacy.spark.registry"},{"location":"pythonv1/cape_privacy.spark/registry/#get","text":"get ( transformation : str ) -> TransformationCtor Returns the constructor for the given key. Arguments : transformation - The key of transformation to retrieve.","title":"get"},{"location":"pythonv1/cape_privacy.spark/registry/#register","text":"register ( label : str , ctor : TransformationCtor ) Registers a new transformation constructor under the label provided. Arguments : label - The label that will be used as the key in the registry ctor - The transformation constructor","title":"register"},{"location":"pythonv1/cape_privacy.spark/transformations/perturbation/","text":"cape_privacy.spark.transformations.perturbation # NumericPerturbation Objects # class NumericPerturbation ( base . Transformation ) Add uniform random noise to a numeric series Mask a numeric series by adding uniform random noise to each value. The amount of noise is drawn from the interval [min, max). Attributes : dtype dtypes.Numerics - series type min int, float - the values generated will be greater or equal to min max int, float - the values generated will be less than max seed (int), optional: a seed to initialize the random generator DatePerturbation Objects # class DatePerturbation ( base . Transformation ) Add uniform random noise to a Pandas series of timestamps Mask a series by adding uniform random noise to the specified frequencies of timestamps. The amount of noise for each frequency is drawn from the internal [min_freq, max_freq). Note that seeds are currently not supported. Attributes : frequency str, str list - one or more frequencies to perturbate min int, int list - the frequency value will be greater or equal to min max int, int list - the frequency value will be less than max","title":"perturbation"},{"location":"pythonv1/cape_privacy.spark/transformations/perturbation/#cape_privacysparktransformationsperturbation","text":"","title":"cape_privacy.spark.transformations.perturbation"},{"location":"pythonv1/cape_privacy.spark/transformations/perturbation/#numericperturbation-objects","text":"class NumericPerturbation ( base . Transformation ) Add uniform random noise to a numeric series Mask a numeric series by adding uniform random noise to each value. The amount of noise is drawn from the interval [min, max). Attributes : dtype dtypes.Numerics - series type min int, float - the values generated will be greater or equal to min max int, float - the values generated will be less than max seed (int), optional: a seed to initialize the random generator","title":"NumericPerturbation Objects"},{"location":"pythonv1/cape_privacy.spark/transformations/perturbation/#dateperturbation-objects","text":"class DatePerturbation ( base . Transformation ) Add uniform random noise to a Pandas series of timestamps Mask a series by adding uniform random noise to the specified frequencies of timestamps. The amount of noise for each frequency is drawn from the internal [min_freq, max_freq). Note that seeds are currently not supported. Attributes : frequency str, str list - one or more frequencies to perturbate min int, int list - the frequency value will be greater or equal to min max int, int list - the frequency value will be less than max","title":"DatePerturbation Objects"},{"location":"pythonv1/cape_privacy.spark/transformations/redaction/","text":"cape_privacy.spark.transformations.redaction # ColumnRedact Objects # class ColumnRedact () Redacts columns from a Spark dataframe. Attributes : columns - Which columns are redacted. RowRedact Objects # class RowRedact () Redacts rows satisfying some condition from a Spark DataFrame. Attributes : condition - When this condition evaluates to True for a row, that row will be dropped.","title":"redaction"},{"location":"pythonv1/cape_privacy.spark/transformations/redaction/#cape_privacysparktransformationsredaction","text":"","title":"cape_privacy.spark.transformations.redaction"},{"location":"pythonv1/cape_privacy.spark/transformations/redaction/#columnredact-objects","text":"class ColumnRedact () Redacts columns from a Spark dataframe. Attributes : columns - Which columns are redacted.","title":"ColumnRedact Objects"},{"location":"pythonv1/cape_privacy.spark/transformations/redaction/#rowredact-objects","text":"class RowRedact () Redacts rows satisfying some condition from a Spark DataFrame. Attributes : condition - When this condition evaluates to True for a row, that row will be dropped.","title":"RowRedact Objects"},{"location":"pythonv1/cape_privacy.spark/transformations/rounding/","text":"cape_privacy.spark.transformations.rounding # NumericRounding Objects # class NumericRounding ( base . Transformation ) Reduce the precision of a numeric series Round each value in the series to the given number of digits. Attributes : dtypes dtypes.Numerics - series type. precision int - set the number of digits. DateTruncation Objects # class DateTruncation ( base . Transformation ) Reduce the precision of a date series Truncate each date in a series to the unit (year or month) specified by frequency. Attributes : frequency string - expect to be 'year' or 'month'","title":"rounding"},{"location":"pythonv1/cape_privacy.spark/transformations/rounding/#cape_privacysparktransformationsrounding","text":"","title":"cape_privacy.spark.transformations.rounding"},{"location":"pythonv1/cape_privacy.spark/transformations/rounding/#numericrounding-objects","text":"class NumericRounding ( base . Transformation ) Reduce the precision of a numeric series Round each value in the series to the given number of digits. Attributes : dtypes dtypes.Numerics - series type. precision int - set the number of digits.","title":"NumericRounding Objects"},{"location":"pythonv1/cape_privacy.spark/transformations/rounding/#datetruncation-objects","text":"class DateTruncation ( base . Transformation ) Reduce the precision of a date series Truncate each date in a series to the unit (year or month) specified by frequency. Attributes : frequency string - expect to be 'year' or 'month'","title":"DateTruncation Objects"},{"location":"pythonv1/cape_privacy.spark/transformations/tokenizer/","text":"cape_privacy.spark.transformations.tokenizer # Tokenizer Objects # class Tokenizer ( base . Transformation ) Tokenizer: map a string to a token to obfuscate it. When applying the tokenizer to a Spark series of type string, each value gets mapped to a token (hexadecimal string). If a value is repeated several times across the series, it always get mapped to the same token in order to maintain the count. A value can be mapped to different tokens by setting the key to a different value. Attributes : max_token_len int or bytes - control the token length (default length is 64) key - expect a string or byte string. if not specified, key will be set to a random byte string. ReversibleTokenizer Objects # class ReversibleTokenizer ( base . Transformation ) ReversibleTokenizer: map a string to a token to obfuscate it. When applying the Tokenizer to a Spark series of type string, each value gets mapped to a token (hexadecimal string). If a value is repeated several times across the series, it always get mapped to the same token in order to maintain the count. A value can be mapped to different tokens by setting the key to a different value. This tokenizer allows tokens to be reversed to their original data when the secret key is known. Attributes : key - expect a string or byte string of length exactly 32 bytes. encoding - string identifying the Python encoding used for inputs. TokenReverser Objects # class TokenReverser ( base . Transformation ) TokenReverser: recover string from token. When applying the TokenReverser to a Spark series of tokens, each token is mapped back to the string that was originally used by ReversibleTokenizer to construct the token. The same key must be used. Attributes : key - expect a string or byte string of length exactly 32 bytes. encoding - string identifying the Python encoding used for outputs.","title":"tokenizer"},{"location":"pythonv1/cape_privacy.spark/transformations/tokenizer/#cape_privacysparktransformationstokenizer","text":"","title":"cape_privacy.spark.transformations.tokenizer"},{"location":"pythonv1/cape_privacy.spark/transformations/tokenizer/#tokenizer-objects","text":"class Tokenizer ( base . Transformation ) Tokenizer: map a string to a token to obfuscate it. When applying the tokenizer to a Spark series of type string, each value gets mapped to a token (hexadecimal string). If a value is repeated several times across the series, it always get mapped to the same token in order to maintain the count. A value can be mapped to different tokens by setting the key to a different value. Attributes : max_token_len int or bytes - control the token length (default length is 64) key - expect a string or byte string. if not specified, key will be set to a random byte string.","title":"Tokenizer Objects"},{"location":"pythonv1/cape_privacy.spark/transformations/tokenizer/#reversibletokenizer-objects","text":"class ReversibleTokenizer ( base . Transformation ) ReversibleTokenizer: map a string to a token to obfuscate it. When applying the Tokenizer to a Spark series of type string, each value gets mapped to a token (hexadecimal string). If a value is repeated several times across the series, it always get mapped to the same token in order to maintain the count. A value can be mapped to different tokens by setting the key to a different value. This tokenizer allows tokens to be reversed to their original data when the secret key is known. Attributes : key - expect a string or byte string of length exactly 32 bytes. encoding - string identifying the Python encoding used for inputs.","title":"ReversibleTokenizer Objects"},{"location":"pythonv1/cape_privacy.spark/transformations/tokenizer/#tokenreverser-objects","text":"class TokenReverser ( base . Transformation ) TokenReverser: recover string from token. When applying the TokenReverser to a Spark series of tokens, each token is mapped back to the string that was originally used by ReversibleTokenizer to construct the token. The same key must be used. Attributes : key - expect a string or byte string of length exactly 32 bytes. encoding - string identifying the Python encoding used for outputs.","title":"TokenReverser Objects"},{"location":"release-notes/","text":"Release notes # This section contains a complete history of Cape Privacy's release notes.","title":"Introduction"},{"location":"release-notes/#release-notes","text":"This section contains a complete history of Cape Privacy's release notes.","title":"Release notes"},{"location":"release-notes/26062020/","text":"26 June 2020 - Cape Python 0.1.1 - \"Snack\" # This release offers a preview of Cape Privacy's functionality, using Cape Python. It gives data scientists a first look at some key elements of Cape Privacy's tooling, including writing policies and using Cape Privacy's built-in transformations. Refer to the Quickstart for information on installing and using Cape Python. You can view the source code in the Cape Python GitHub repository . Writing data policies # Cape Python supports data policies written in YAML. Refer to Policies for more information. Five built-in transformations # Cape Python includes five built-in transformations: date-pertubation number-pertubation date-truncation numeric-rounding tokenizer Refer to Transformations for more information.","title":"June 2020"},{"location":"release-notes/26062020/#26-june-2020-cape-python-011-snack","text":"This release offers a preview of Cape Privacy's functionality, using Cape Python. It gives data scientists a first look at some key elements of Cape Privacy's tooling, including writing policies and using Cape Privacy's built-in transformations. Refer to the Quickstart for information on installing and using Cape Python. You can view the source code in the Cape Python GitHub repository .","title":"26 June 2020 - Cape Python 0.1.1 - \"Snack\""},{"location":"release-notes/26062020/#writing-data-policies","text":"Cape Python supports data policies written in YAML. Refer to Policies for more information.","title":"Writing data policies"},{"location":"release-notes/26062020/#five-built-in-transformations","text":"Cape Python includes five built-in transformations: date-pertubation number-pertubation date-truncation numeric-rounding tokenizer Refer to Transformations for more information.","title":"Five built-in transformations"},{"location":"release-notes/30072020/","text":"30 July 2020 - Cape Core 0.0.1 and Cape Python 0.2.0 # This release offers to elements of Cape Core: the Cape Coordinator service, and the command line interface. Refer to Cape Core for installation and usage guides. You can view the source code in the Cape GitHub repository . Cape Core 0.0.1 # Manage data policies with Cape Coordinator # Cape Coordinator is a server application that manages access to policies and projects based on user authentication and user roles. This release makes it available to deploy to Kubernetes , or install locally using a CLI to test it out. A CLI interface # The CLI allows users to interact with Cape Coordinator in order to manage users, projects, and policies. Cape Python 0.2.0 # Cape Core Integration # This release offers integrations to elements of Cape Core: the Cape Coordinator service. Refer to Cape Core for installation and usage guides. There is a tutorial showing how to use Cape Python with Cape Core","title":"30 July 2020 - Cape Core 0.0.1 and Cape Python 0.2.0"},{"location":"release-notes/30072020/#30-july-2020-cape-core-001-and-cape-python-020","text":"This release offers to elements of Cape Core: the Cape Coordinator service, and the command line interface. Refer to Cape Core for installation and usage guides. You can view the source code in the Cape GitHub repository .","title":"30 July 2020 - Cape Core 0.0.1 and Cape Python 0.2.0"},{"location":"release-notes/30072020/#cape-core-001","text":"","title":"Cape Core 0.0.1"},{"location":"release-notes/30072020/#manage-data-policies-with-cape-coordinator","text":"Cape Coordinator is a server application that manages access to policies and projects based on user authentication and user roles. This release makes it available to deploy to Kubernetes , or install locally using a CLI to test it out.","title":"Manage data policies with Cape Coordinator"},{"location":"release-notes/30072020/#a-cli-interface","text":"The CLI allows users to interact with Cape Coordinator in order to manage users, projects, and policies.","title":"A CLI interface"},{"location":"release-notes/30072020/#cape-python-020","text":"","title":"Cape Python 0.2.0"},{"location":"release-notes/30072020/#cape-core-integration","text":"This release offers integrations to elements of Cape Core: the Cape Coordinator service. Refer to Cape Core for installation and usage guides. There is a tutorial showing how to use Cape Python with Cape Core","title":"Cape Core Integration"},{"location":"understand/","text":"Understand Cape Privacy # This section provides high-level information to help you understand what Cape Privacy offers, its underlying principles, and how the different libraries and services work together.","title":"Introduction"},{"location":"understand/#understand-cape-privacy","text":"This section provides high-level information to help you understand what Cape Privacy offers, its underlying principles, and how the different libraries and services work together.","title":"Understand Cape Privacy"},{"location":"understand/roadmap/","text":"Roadmap # Cape Privacy's software is currently in alpha state. This document describes Cape Privacy's goals, and some upcoming features. The goal # Cape Privacy aims to enable better and more privacy compliant data science, and to make this accessible to a wide range of users (not just engineers). Through our open source approach, we ensure our own code and tools are transparent and auditable. Architecture # Cape Privacy provides Cape Coordinator , to manage policy and users. This will interact with the Cape Privacy libraries (such as Cape Python ) through a workers interface, and with your own data services through an API. Data flow # Data will flow between the following elements of Cape Privacy's architecture: Cape workers pass policy information to Cape libraries. Cape Coordinator has an internal policy management workflow, from a request for new policy, through collaborating review, to using the policy to control how the libraries transform your data. The Cape API will exchange information relevant to auditors with your own monitoring tools. Upcoming features # Cape Python # Reversible tokenisation: allow reversing of tokenization to reveal the raw value. Policy audit logging: create logging hooks to allow audit logs for policy downloads and usage in Cape Python. Expand pipeline integrations: add Apache Beam, Apache Flink, Apache Arrow Flight, or Dask integration as another pipeline we can support, either as part of Cape Python or in its own separate project. Cape Core # Audit logging configuration: set up configuration for how and where you log actions in Cape Coordinator, such as project and policy creation, user changes, and user actions in Cape. Governance tooling: integrate basic data governance information to be used within Cape Coordinator for writing better policy, with a possible integration with Apache Atlas or other open-source governance tools. Pipeline orchestrator integration: ability to connect with Spark orchestration tools (such as YARN, Mesos, and Airflow) and pull information on jobs that are running for easier management of running Spark installations.","title":"Roadmap"},{"location":"understand/roadmap/#roadmap","text":"Cape Privacy's software is currently in alpha state. This document describes Cape Privacy's goals, and some upcoming features.","title":"Roadmap"},{"location":"understand/roadmap/#the-goal","text":"Cape Privacy aims to enable better and more privacy compliant data science, and to make this accessible to a wide range of users (not just engineers). Through our open source approach, we ensure our own code and tools are transparent and auditable.","title":"The goal"},{"location":"understand/roadmap/#architecture","text":"Cape Privacy provides Cape Coordinator , to manage policy and users. This will interact with the Cape Privacy libraries (such as Cape Python ) through a workers interface, and with your own data services through an API.","title":"Architecture"},{"location":"understand/roadmap/#data-flow","text":"Data will flow between the following elements of Cape Privacy's architecture: Cape workers pass policy information to Cape libraries. Cape Coordinator has an internal policy management workflow, from a request for new policy, through collaborating review, to using the policy to control how the libraries transform your data. The Cape API will exchange information relevant to auditors with your own monitoring tools.","title":"Data flow"},{"location":"understand/roadmap/#upcoming-features","text":"","title":"Upcoming features"},{"location":"understand/roadmap/#cape-python","text":"Reversible tokenisation: allow reversing of tokenization to reveal the raw value. Policy audit logging: create logging hooks to allow audit logs for policy downloads and usage in Cape Python. Expand pipeline integrations: add Apache Beam, Apache Flink, Apache Arrow Flight, or Dask integration as another pipeline we can support, either as part of Cape Python or in its own separate project.","title":"Cape Python"},{"location":"understand/roadmap/#cape-core","text":"Audit logging configuration: set up configuration for how and where you log actions in Cape Coordinator, such as project and policy creation, user changes, and user actions in Cape. Governance tooling: integrate basic data governance information to be used within Cape Coordinator for writing better policy, with a possible integration with Apache Atlas or other open-source governance tools. Pipeline orchestrator integration: ability to connect with Spark orchestration tools (such as YARN, Mesos, and Airflow) and pull information on jobs that are running for easier management of running Spark installations.","title":"Cape Core"},{"location":"understand/support/","text":"Support matrix # This document provides information about supported operating systems, language versions, and other dependencies. Operating systems # We expect Cape Privacy services and libraries to work on: Linux (all distros) macOS (10.15 - Catalina and above) Windows 10 We test against the following operating systems: Ubuntu 18.04 Container and database versions # We expect Cape Coordinator to work with the following: Kubernetes 1.16+ Helm installation 3.0+ PostgreSQL 11.0+ We test against the following: Kubernetes 1.18 Helm 3.2 PostgreSQL 11.7 Python environments # Cape Python supports Python 3.6, 3.7, and 3.8. We expect Cape Python to work with most versions of Spark (v2.4.5+) and Pandas (v1.0.0+). We test on: Pandas v1.0.3 Spark v3.0.0 Note We recommend using Spark 3.0 or above. Cape Python has an optional dependency on PyArrow, which works with Spark 3.0, but requires additional configuration on Spark 2.x.","title":"Support matrix"},{"location":"understand/support/#support-matrix","text":"This document provides information about supported operating systems, language versions, and other dependencies.","title":"Support matrix"},{"location":"understand/support/#operating-systems","text":"We expect Cape Privacy services and libraries to work on: Linux (all distros) macOS (10.15 - Catalina and above) Windows 10 We test against the following operating systems: Ubuntu 18.04","title":"Operating systems"},{"location":"understand/support/#container-and-database-versions","text":"We expect Cape Coordinator to work with the following: Kubernetes 1.16+ Helm installation 3.0+ PostgreSQL 11.0+ We test against the following: Kubernetes 1.18 Helm 3.2 PostgreSQL 11.7","title":"Container and database versions"},{"location":"understand/support/#python-environments","text":"Cape Python supports Python 3.6, 3.7, and 3.8. We expect Cape Python to work with most versions of Spark (v2.4.5+) and Pandas (v1.0.0+). We test on: Pandas v1.0.3 Spark v3.0.0 Note We recommend using Spark 3.0 or above. Cape Python has an optional dependency on PyArrow, which works with Spark 3.0, but requires additional configuration on Spark 2.x.","title":"Python environments"},{"location":"understand/architecture/","text":"Cape Privacy architecture # Cape Privacy's architecture is a collection of services and libraries that work with each other and your own data management tooling. Components # Cape Privacy currently offers three components: Cape Coordinator . This provides policy management, and can integrate with Cape Privacy's libraries. Cape Python . The first Cape Privacy library to be released, providing Python-based data transformations for Pandas and Apache Spark (PySpark). The command line interface . A CLI to allow users to interact with Cape Coordinator. Roadmap # More services and libraries are coming soon. Refer to the roadmap for more information.","title":"Overview"},{"location":"understand/architecture/#cape-privacy-architecture","text":"Cape Privacy's architecture is a collection of services and libraries that work with each other and your own data management tooling.","title":"Cape Privacy architecture"},{"location":"understand/architecture/#components","text":"Cape Privacy currently offers three components: Cape Coordinator . This provides policy management, and can integrate with Cape Privacy's libraries. Cape Python . The first Cape Privacy library to be released, providing Python-based data transformations for Pandas and Apache Spark (PySpark). The command line interface . A CLI to allow users to interact with Cape Coordinator.","title":"Components"},{"location":"understand/architecture/#roadmap","text":"More services and libraries are coming soon. Refer to the roadmap for more information.","title":"Roadmap"},{"location":"understand/architecture/cape-workers/","text":"Cape Workers # Cape workers are Docker images that can be built and run in the environment of your choice. Each party collaborating on training a model using Cape will deploy a separate cape worker as a Docker container. Cape workers are designed to facilitate the sharing of datasets between each other using public-key encryption, while keeping all other data isolated to the context of each customized, containerized service. Prerequisites # Docker A Cape Cloud Account A Cape Cloud organization token An S3 bucket for retrieving data Supported SHA Commits # bcc4e3b Starting your cape-worker instance # Starting your Cape worker is as simple as running a Docker image using docker run : export CAPE_TOKEN = my-cape-org-token docker run -d --rm \\ --name cape-worker \\ -e CAPE_TOKEN \\ -e CAPE_BUCKET = my-s3-bucket-location \\ -e AWS_ACCESS_KEY_ID \\ -e AWS_SECRET_ACCESS_KEY \\ -e AWS_REGION \\ capeprivacy/cape-worker:sha-1234567 In this command: cape-worker is the name you want to assign your container. my-cape-org-token is your Cape organization token . s3-bucket-location is the URI of the S3 bucket that you would like Cape to write the results of the computation to. sha-1234567 is the commit SHA specifying the commit you would like to use to run your worker. Environment Variables # When you run the cape-worker image, you can specify the following environment variables, either by passing on the docker run command, or by setting them in a separate .env file. Refer to the Docker documentation for options for configuring Docker environment variables . CAPE_TOKEN (required) # This is your Cape organization token. You can generate Cape organization tokens from your \"Organization Settings\" in the Cape UI. Refer to our tokens usage documentation for more information on how tokens are used in Cape. CAPE_BUCKET (required) # Set this variable to the URI of the S3 bucket that you would like Cape to write the results of the computation to. Currently Cape only supports writing to Amazon S3 blob storage. In order for your cape-worker to authenticate and communicate with your S3 bucket, you'll need to let your instance know about the following AWS configuration variables. AWS_ACCESS_KEY_ID # Set this to the AWS access key associated with an IAM user that has s3 write permissions to the location specified by CAPE_BUCKET . This variable is only required if your container doesn't already have S3 permissions, for example if you run your container on an EC2 instance using an IAM role that has the correct permissions. AWS_SECRET_ACCESS_KEY # Set this to the secret key associated with the access key. This is essentially the \"password\" for the access key. This variable is only required if your container doesn't already have S3 permissions, for example if you run your container on an EC2 instance using an IAM role that has the correct permissions. AWS_REGION # Set this to the AWS Region you would like to send the request to. Note These AWS configuration variables line up with the AWS CLI environment variables. For up-to-date info on these variables, refer to the AWS documentation . (Optional) Configure your container to restart automatically # Add the flag --restart unless-stopped to your docker command-line when launching your container to have it restart automatically. Alternatively, use a process manager like supervisor or systemd if you need more control. For more information, refer to the Docker documentation . (Optional) Send your logs to an aggregation service like AWS CloudWatch # It is highly recommended that you send container logs to a log aggregation service. For example, if the worker is running in an AWS environment, the Docker daemon or the individual container can send logs directly to AWS CloudWatch. See here for detailed documentation, along with other drivers provided by docker.","title":"Cape Workers"},{"location":"understand/architecture/cape-workers/#cape-workers","text":"Cape workers are Docker images that can be built and run in the environment of your choice. Each party collaborating on training a model using Cape will deploy a separate cape worker as a Docker container. Cape workers are designed to facilitate the sharing of datasets between each other using public-key encryption, while keeping all other data isolated to the context of each customized, containerized service.","title":"Cape Workers"},{"location":"understand/architecture/cape-workers/#prerequisites","text":"Docker A Cape Cloud Account A Cape Cloud organization token An S3 bucket for retrieving data","title":"Prerequisites"},{"location":"understand/architecture/cape-workers/#supported-sha-commits","text":"bcc4e3b","title":"Supported SHA Commits"},{"location":"understand/architecture/cape-workers/#starting-your-cape-worker-instance","text":"Starting your Cape worker is as simple as running a Docker image using docker run : export CAPE_TOKEN = my-cape-org-token docker run -d --rm \\ --name cape-worker \\ -e CAPE_TOKEN \\ -e CAPE_BUCKET = my-s3-bucket-location \\ -e AWS_ACCESS_KEY_ID \\ -e AWS_SECRET_ACCESS_KEY \\ -e AWS_REGION \\ capeprivacy/cape-worker:sha-1234567 In this command: cape-worker is the name you want to assign your container. my-cape-org-token is your Cape organization token . s3-bucket-location is the URI of the S3 bucket that you would like Cape to write the results of the computation to. sha-1234567 is the commit SHA specifying the commit you would like to use to run your worker.","title":"Starting your cape-worker instance"},{"location":"understand/architecture/cape-workers/#environment-variables","text":"When you run the cape-worker image, you can specify the following environment variables, either by passing on the docker run command, or by setting them in a separate .env file. Refer to the Docker documentation for options for configuring Docker environment variables .","title":"Environment Variables"},{"location":"understand/architecture/cape-workers/#cape_token-required","text":"This is your Cape organization token. You can generate Cape organization tokens from your \"Organization Settings\" in the Cape UI. Refer to our tokens usage documentation for more information on how tokens are used in Cape.","title":"CAPE_TOKEN (required)"},{"location":"understand/architecture/cape-workers/#cape_bucket-required","text":"Set this variable to the URI of the S3 bucket that you would like Cape to write the results of the computation to. Currently Cape only supports writing to Amazon S3 blob storage. In order for your cape-worker to authenticate and communicate with your S3 bucket, you'll need to let your instance know about the following AWS configuration variables.","title":"CAPE_BUCKET (required)"},{"location":"understand/architecture/cape-workers/#aws_access_key_id","text":"Set this to the AWS access key associated with an IAM user that has s3 write permissions to the location specified by CAPE_BUCKET . This variable is only required if your container doesn't already have S3 permissions, for example if you run your container on an EC2 instance using an IAM role that has the correct permissions.","title":"AWS_ACCESS_KEY_ID"},{"location":"understand/architecture/cape-workers/#aws_secret_access_key","text":"Set this to the secret key associated with the access key. This is essentially the \"password\" for the access key. This variable is only required if your container doesn't already have S3 permissions, for example if you run your container on an EC2 instance using an IAM role that has the correct permissions.","title":"AWS_SECRET_ACCESS_KEY"},{"location":"understand/architecture/cape-workers/#aws_region","text":"Set this to the AWS Region you would like to send the request to. Note These AWS configuration variables line up with the AWS CLI environment variables. For up-to-date info on these variables, refer to the AWS documentation .","title":"AWS_REGION"},{"location":"understand/architecture/cape-workers/#optional-configure-your-container-to-restart-automatically","text":"Add the flag --restart unless-stopped to your docker command-line when launching your container to have it restart automatically. Alternatively, use a process manager like supervisor or systemd if you need more control. For more information, refer to the Docker documentation .","title":"(Optional) Configure your container to restart automatically"},{"location":"understand/architecture/cape-workers/#optional-send-your-logs-to-an-aggregation-service-like-aws-cloudwatch","text":"It is highly recommended that you send container logs to a log aggregation service. For example, if the worker is running in an AWS environment, the Docker daemon or the individual container can send logs directly to AWS CloudWatch. See here for detailed documentation, along with other drivers provided by docker.","title":"(Optional) Send your logs to an aggregation service like AWS CloudWatch"},{"location":"understand/best-practices/","text":"Best practice guides # This section of the documentation provides guidance and advice on how to use Cape Privacy's tools in your own workflow. These are not prescriptive user manuals. Instead, they are articles drawing on the expertise available at Cape Privacy, designed to help you implement Cape Privacy tools in a way that works for you.","title":"Overview"},{"location":"understand/best-practices/#best-practice-guides","text":"This section of the documentation provides guidance and advice on how to use Cape Privacy's tools in your own workflow. These are not prescriptive user manuals. Instead, they are articles drawing on the expertise available at Cape Privacy, designed to help you implement Cape Privacy tools in a way that works for you.","title":"Best practice guides"},{"location":"understand/best-practices/running-transformations/","text":"Applying transformations to your data # Cape Privacy's libraries include built-in transformations. This document provides recommendations on how to use them. We encourage community contributions to this guidance as the transformation techniques grow. Recommendations # Ensure that you have your data collected and joined before applying transformations, especially in the case of multiple sensitive columns. Some transformations require sensitive data to be contained in the policy files. For this reason, keep your policy files stored securely. In a future release, we will support pulling transformation keys from key storage software, such as Hashicorp Vault. Consider using transformations as the final step in your pre-processing before creating a \"clean sink\" or \"safe dataset\". This means that you can begin your work on that clean dataset. Experiment with the transformations directly on your data to learn how they impact your data utility. Figure out the right utility vs. privacy tradeoff for the task at hand, and amend your policy accordingly.","title":"Running transformations"},{"location":"understand/best-practices/running-transformations/#applying-transformations-to-your-data","text":"Cape Privacy's libraries include built-in transformations. This document provides recommendations on how to use them. We encourage community contributions to this guidance as the transformation techniques grow.","title":"Applying transformations to your data"},{"location":"understand/best-practices/running-transformations/#recommendations","text":"Ensure that you have your data collected and joined before applying transformations, especially in the case of multiple sensitive columns. Some transformations require sensitive data to be contained in the policy files. For this reason, keep your policy files stored securely. In a future release, we will support pulling transformation keys from key storage software, such as Hashicorp Vault. Consider using transformations as the final step in your pre-processing before creating a \"clean sink\" or \"safe dataset\". This means that you can begin your work on that clean dataset. Experiment with the transformations directly on your data to learn how they impact your data utility. Figure out the right utility vs. privacy tradeoff for the task at hand, and amend your policy accordingly.","title":"Recommendations"},{"location":"understand/features/","text":"Cape Features & Concepts # This section outlines the various features and concepts found within Cape.","title":"Overview"},{"location":"understand/features/#cape-features-concepts","text":"This section outlines the various features and concepts found within Cape.","title":"Cape Features &amp; Concepts"},{"location":"understand/features/auditing/","text":"Audit Log # All user actions in Cape are captured and viewable in the audit log. Examples of a tracked action include getting a policy , logging in, and creating a new project . You must be a global admin to view the audit log. The audit command has its own namespace in the Cape CLI $ cape audit view NAME: cape audit - Commands for querying the audit log. USAGE: cape audit command [command options] [arguments...] DESCRIPTION: Commands for querying the audit log. COMMANDS: view View the audit log. help, h Shows a list of commands or help for one command OPTIONS: --help, -h Display documentation and examples for this command. (default: false) --version, -v Display the current version of Cape. (default: false) Audit log entries have the following attributes: event_name : A string representing the action (e.g. create-project ). user_id : The ID of the user who performed this action. user_email : The email of the user who performed this action. time : A timestamp indicating when the action happened. target_type : What kind of object did this action happen against. For example, if a user created a project, you would see project as the target type. target_id : The ID of the target target_label : If the target has a label, it will be recorded. For example, if a user created a project called my-project , that label would appear here.","title":"Auditing"},{"location":"understand/features/auditing/#audit-log","text":"All user actions in Cape are captured and viewable in the audit log. Examples of a tracked action include getting a policy , logging in, and creating a new project . You must be a global admin to view the audit log. The audit command has its own namespace in the Cape CLI $ cape audit view NAME: cape audit - Commands for querying the audit log. USAGE: cape audit command [command options] [arguments...] DESCRIPTION: Commands for querying the audit log. COMMANDS: view View the audit log. help, h Shows a list of commands or help for one command OPTIONS: --help, -h Display documentation and examples for this command. (default: false) --version, -v Display the current version of Cape. (default: false) Audit log entries have the following attributes: event_name : A string representing the action (e.g. create-project ). user_id : The ID of the user who performed this action. user_email : The email of the user who performed this action. time : A timestamp indicating when the action happened. target_type : What kind of object did this action happen against. For example, if a user created a project, you would see project as the target type. target_id : The ID of the target target_label : If the target has a label, it will be recorded. For example, if a user created a project called my-project , that label would appear here.","title":"Audit Log"},{"location":"understand/features/policy/","text":"Policy # Policy in Cape is how you describe how users can have access to your data. Policy is described in yaml, this example describes all the available YAML objects: # Required. The policy name. label : test_policy # Required. The Cape Privacy specification version. Must be 1. version : 1 # Configure your named transformations. # Named transformations allow you to reuse a transformation # with a set value throughout your policy. transformations : # This named transformation uses the built-in tokenizer transformation - name : my_tokenizer type : tokenizer max_token_len : 10 key : \"my secret\" rules : # Required. The column name. - match : name : fruit actions : # This example shows a named transformation. # It tells the policy runner to apply the my_tokenizer transformation # to all fields in the \"fruit\" column. - transform : name : my_tokenizer - match : name : weight actions : - transform : # This example shows an unnamed transformation. # It tells the policy runner to: # (1) Apply the transformation numeric-rounding # (2) Round to one decimal place type : numeric-rounding dtype : Double precision : 1","title":"Policy"},{"location":"understand/features/policy/#policy","text":"Policy in Cape is how you describe how users can have access to your data. Policy is described in yaml, this example describes all the available YAML objects: # Required. The policy name. label : test_policy # Required. The Cape Privacy specification version. Must be 1. version : 1 # Configure your named transformations. # Named transformations allow you to reuse a transformation # with a set value throughout your policy. transformations : # This named transformation uses the built-in tokenizer transformation - name : my_tokenizer type : tokenizer max_token_len : 10 key : \"my secret\" rules : # Required. The column name. - match : name : fruit actions : # This example shows a named transformation. # It tells the policy runner to apply the my_tokenizer transformation # to all fields in the \"fruit\" column. - transform : name : my_tokenizer - match : name : weight actions : - transform : # This example shows an unnamed transformation. # It tells the policy runner to: # (1) Apply the transformation numeric-rounding # (2) Round to one decimal place type : numeric-rounding dtype : Double precision : 1","title":"Policy"},{"location":"understand/features/projects/","text":"Projects # Projects are the building block for Policy within Cape. In the real world, most data access is granted depending on the use case and what the user is trying to achieve. Cape groups all of those different use cases into projects. For example, if you are a data scientist working for a bank in the fraud department who is tasked with creating a model to detect fraud. You may be able to train a better model if you can use other customer data that belongs to the sales department. In this context, you would create a project for this use case, and grant access (with whatever privacy guarantees you need) to the scientists tasked with training a model. Projects have the following attributes: ID : A unique identifier for the project Name : The name of the project Label : A url safe version of the name Description : A blob describing what this project is for Created At : When this project was first created Updated At : When any attributes about this project last changed Commands to interact with projects exist under the cape projects namespace on the CLI. $ cape projects NAME: cape projects - Commands for interacting with Cape projects. USAGE: cape projects command [command options] [arguments...] DESCRIPTION: Commands for interacting with Cape projects. COMMANDS: contributors Commands for interacting with Cape Project Contributors. create Creates a project in Cape. list List your Cape projects. update Update a projects attributes. get Get a details of a project. policy Commands for interacting with Cape Policy within a Project. help, h Shows a list of commands or help for one command OPTIONS: --help, -h Display documentation and examples for this command. (default: false) --version, -v Display the current version of Cape. (default: false)","title":"Projects"},{"location":"understand/features/projects/#projects","text":"Projects are the building block for Policy within Cape. In the real world, most data access is granted depending on the use case and what the user is trying to achieve. Cape groups all of those different use cases into projects. For example, if you are a data scientist working for a bank in the fraud department who is tasked with creating a model to detect fraud. You may be able to train a better model if you can use other customer data that belongs to the sales department. In this context, you would create a project for this use case, and grant access (with whatever privacy guarantees you need) to the scientists tasked with training a model. Projects have the following attributes: ID : A unique identifier for the project Name : The name of the project Label : A url safe version of the name Description : A blob describing what this project is for Created At : When this project was first created Updated At : When any attributes about this project last changed Commands to interact with projects exist under the cape projects namespace on the CLI. $ cape projects NAME: cape projects - Commands for interacting with Cape projects. USAGE: cape projects command [command options] [arguments...] DESCRIPTION: Commands for interacting with Cape projects. COMMANDS: contributors Commands for interacting with Cape Project Contributors. create Creates a project in Cape. list List your Cape projects. update Update a projects attributes. get Get a details of a project. policy Commands for interacting with Cape Policy within a Project. help, h Shows a list of commands or help for one command OPTIONS: --help, -h Display documentation and examples for this command. (default: false) --version, -v Display the current version of Cape. (default: false)","title":"Projects"},{"location":"understand/features/roles/","text":"Roles # In Cape, there are two contexts you can operate in: Global commands: these are things that affect how cape functions in your entire organization. For example, creating a user. Project commands: these are things that only affect the specific project you are operating on. For example, setting a policy on a project. Within each of these spaces, there are different roles that you can have. Globally, you can either be an admin , or a user . Admins can do anything in the system. It is much like being a superuser on a traditional linux system. user has much less access. They can read various details about the cape deployment, but not much more. Within a project, there are three roles: project-reader : a project-reader can see what project details (for example, the title and description), and read what the policy is. project-contributor : a project-contributor can do everything a reader can, plus suggest policy changes. project-owner : a project-owner can do everything a contributor can do, plus approve or reject policy suggestions, add users to the project, and so on. Note Cape refers to all users associated with a project as \"contributors\" to that project, and manages project users with the contributors command. In the documentation, \"contributor\" refers to any user in the project, while project-contributor refers to a user with the project-contributor role described in this document.","title":"Roles"},{"location":"understand/features/roles/#roles","text":"In Cape, there are two contexts you can operate in: Global commands: these are things that affect how cape functions in your entire organization. For example, creating a user. Project commands: these are things that only affect the specific project you are operating on. For example, setting a policy on a project. Within each of these spaces, there are different roles that you can have. Globally, you can either be an admin , or a user . Admins can do anything in the system. It is much like being a superuser on a traditional linux system. user has much less access. They can read various details about the cape deployment, but not much more. Within a project, there are three roles: project-reader : a project-reader can see what project details (for example, the title and description), and read what the policy is. project-contributor : a project-contributor can do everything a reader can, plus suggest policy changes. project-owner : a project-owner can do everything a contributor can do, plus approve or reject policy suggestions, add users to the project, and so on. Note Cape refers to all users associated with a project as \"contributors\" to that project, and manages project users with the contributors command. In the documentation, \"contributor\" refers to any user in the project, while project-contributor refers to a user with the project-contributor role described in this document.","title":"Roles"},{"location":"understand/features/tokens/","text":"Tokens # In Cape there are two types of tokens that can be created to authenticate with different parts of Cape's architecture- user tokens and organization tokens. User Tokens # Individual user tokens are used to identify you to the Cape Coordinator, so you can register DataViews , create and run Cape Jobs and perform other necessary functions. See the pycape Usage docs for instructions on how to use your user token to log into the pycape Python library . User tokens can be generated from your User Settings . Organization Tokens # Organizational tokens are used to run Cape Workers . These tokens facilitate the secure sharing of data across all workers that are collaborating on a project using Cape privacy. and are stored as environment variables. Organizational tokens can be generated from your Organization Settings .","title":"Tokens"},{"location":"understand/features/tokens/#tokens","text":"In Cape there are two types of tokens that can be created to authenticate with different parts of Cape's architecture- user tokens and organization tokens.","title":"Tokens"},{"location":"understand/features/tokens/#user-tokens","text":"Individual user tokens are used to identify you to the Cape Coordinator, so you can register DataViews , create and run Cape Jobs and perform other necessary functions. See the pycape Usage docs for instructions on how to use your user token to log into the pycape Python library . User tokens can be generated from your User Settings .","title":"User Tokens"},{"location":"understand/features/tokens/#organization-tokens","text":"Organizational tokens are used to run Cape Workers . These tokens facilitate the secure sharing of data across all workers that are collaborating on a project using Cape privacy. and are stored as environment variables. Organizational tokens can be generated from your Organization Settings .","title":"Organization Tokens"},{"location":"understand/features/users/","text":"Users # Users in Cape are much like users in other applications. All actions performed within Cape must be performed by an authenticated user. Users have the following attributes: ID : A unique identifier for the user Name : The full name of the user Email : An email address associated with the user Created At : When this user was first created Updated At : When any attributes about this user last changed Commands to interact with users exists under the cape users namespace on the CLI. $ cape users NAME: cape users - Commands for querying information about users and modifying them. USAGE: cape users command [command options] [arguments...] DESCRIPTION: Commands for querying information about users and modifying them. COMMANDS: create Create a new user. help, h Shows a list of commands or help for one command OPTIONS: --help, -h Display documentation and examples for this command. (default: false) --version, -v Display the current version of Cape. (default: false)","title":"Users"},{"location":"understand/features/users/#users","text":"Users in Cape are much like users in other applications. All actions performed within Cape must be performed by an authenticated user. Users have the following attributes: ID : A unique identifier for the user Name : The full name of the user Email : An email address associated with the user Created At : When this user was first created Updated At : When any attributes about this user last changed Commands to interact with users exists under the cape users namespace on the CLI. $ cape users NAME: cape users - Commands for querying information about users and modifying them. USAGE: cape users command [command options] [arguments...] DESCRIPTION: Commands for querying information about users and modifying them. COMMANDS: create Create a new user. help, h Shows a list of commands or help for one command OPTIONS: --help, -h Display documentation and examples for this command. (default: false) --version, -v Display the current version of Cape. (default: false)","title":"Users"}]}