{"config":{"lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Welcome to Cape Privacy's documentation # Cape Privacy offers data scientists and data engineers a policy-based interface for applying privacy-enhancing techniques. This documentation site includes: A guide to help you understand the different elements of Cape Privacy , including architecture and best practices. Information on how to use Cape Privacy's code libraries . Best practice guidance on integrating Cape Privacy into your data workflow. News about new features in our release notes .","title":"Home"},{"location":"#welcome-to-cape-privacys-documentation","text":"Cape Privacy offers data scientists and data engineers a policy-based interface for applying privacy-enhancing techniques. This documentation site includes: A guide to help you understand the different elements of Cape Privacy , including architecture and best practices. Information on how to use Cape Privacy's code libraries . Best practice guidance on integrating Cape Privacy into your data workflow. News about new features in our release notes .","title":"Welcome to Cape Privacy's documentation"},{"location":"libraries/","text":"Libraries # Cape Privacy provides libraries that allow you to: Train models in collaboration with other organizations without revealing the contents of your datasets. Get started using encrypted learning without having to learn cryptography. Access the right data while protecting privacy and enhancing data security.","title":"Introduction"},{"location":"libraries/#libraries","text":"Cape Privacy provides libraries that allow you to: Train models in collaboration with other organizations without revealing the contents of your datasets. Get started using encrypted learning without having to learn cryptography. Access the right data while protecting privacy and enhancing data security.","title":"Libraries"},{"location":"libraries/cape-python/","text":"Cape Python overview # Cape Python allows you to write data privacy policies and data transformations to integrate with Pandas and Spark . You can view the source code in the Cape Python GitHub Repository . Use cases # Review the transformations and decide which are a good fit for your data science needs. The 0.1.0 release includes five transformations that provide some common privacy protections. Use case Text data Numeric data Inconsistent data EDA Tokenization Rounding or pertubation Tokenization Analytics Tokenization Rounding or pertubation - ML development - Rounding or pertubation Tokenization ML training/serving No transformation No transformation No transformation Cape Privacy will support more use cases through additional transformations in future releases.","title":"Overview"},{"location":"libraries/cape-python/#cape-python-overview","text":"Cape Python allows you to write data privacy policies and data transformations to integrate with Pandas and Spark . You can view the source code in the Cape Python GitHub Repository .","title":"Cape Python overview"},{"location":"libraries/cape-python/#use-cases","text":"Review the transformations and decide which are a good fit for your data science needs. The 0.1.0 release includes five transformations that provide some common privacy protections. Use case Text data Numeric data Inconsistent data EDA Tokenization Rounding or pertubation Tokenization Analytics Tokenization Rounding or pertubation - ML development - Rounding or pertubation Tokenization ML training/serving No transformation No transformation No transformation Cape Privacy will support more use cases through additional transformations in future releases.","title":"Use cases"},{"location":"libraries/cape-python/coordinator-quickstart/","text":"Cape Python API with Coordinator # This document describes how to use Cape Python with Cape Coordinator. It builds on the Cape Python quickstart guide installation guide. Quickstart # Prerequisites # Cape Coordinator installed. You need the password and email that you set during Cape Coordinator installation. Cape Python installed. We recommend working through the Cape Python quickstart guide before following the steps below. Create your first project # Authenticate with Cape Coordinator by running: $ CAPE_PASSWORD = <PASSWORD> cape login --email <EMAIL> Replace <PASSWORD> and <EMAIL> with the values set during Cape Coordinator installation. Cape Coordinator uses projects to manage which version of a policy is applied to a dataset. To create your first project, run: $ cape projects create first-project \"Hello Project World\" You can now add a policy to the project from a policy specification file. Copy the following YAML to a file called first-policy.yaml : version : 1 rules : # Set the column name - match : name : weight actions : - transform : # This example shows an unnamed transformation. # It tells the policy runner to: # (1) Apply the transformation numeric-rounding # (2) Round to one decimal place type : numeric-rounding dtype : Double precision : 1 Update the project with the first version of policy: $ cape projects update --from-spec first-policy.yaml first-project Get an API token # To connect to Cape Coordinator from Cape Python you need an API token. Obtain the token by running: $ cape tokens create You'll see output like: A token for cape_user@mycape.com has been created! Token: <REDACTED TOKEN> \u203c Remember: Please keep the token safe and share it only over secure channels. Copy the token from the CLI. You will need it in the next section. Write the policy application script # The following script is very similar to the Cape Python quickstart guide , except in this case we're connecting to Cape Coordinator to retrieve the policy. Create a coordinator-policy.py file in your project, with the following content: import cape_privacy as cape import pandas as pd # In the Coordinator installation instructions you should have run the # Coordinator on port 8080. If not, edit the line below with the proper # port. client = cape . Client ( \"http://localhost:8080\" ) client . login ( \"<PASTE TOKEN HERE>\" ) p_dict = client . get_policy ( \"first-project\" ) policy = cape . parse_policy ( p_dict ) # Create a simple Pandas DataFrame df = pd . DataFrame ([ 114.432 , 134.622 , 142.984 ], columns = [ \"weight\" ]) df = cape . apply_policy ( policy , df , inplace = False ) print ( df . head ()) Make sure to replace <PASTE TOKEN HERE> with your API token. Run your transformations # In coordinator-policy.py we create a dataset programatically, so there are no further steps to load a dataset. Run the policy application script and view the output: $ python coordinator-policy.py","title":"Coordinator quickstart"},{"location":"libraries/cape-python/coordinator-quickstart/#cape-python-api-with-coordinator","text":"This document describes how to use Cape Python with Cape Coordinator. It builds on the Cape Python quickstart guide installation guide.","title":"Cape Python API with Coordinator"},{"location":"libraries/cape-python/coordinator-quickstart/#quickstart","text":"","title":"Quickstart"},{"location":"libraries/cape-python/coordinator-quickstart/#prerequisites","text":"Cape Coordinator installed. You need the password and email that you set during Cape Coordinator installation. Cape Python installed. We recommend working through the Cape Python quickstart guide before following the steps below.","title":"Prerequisites"},{"location":"libraries/cape-python/coordinator-quickstart/#create-your-first-project","text":"Authenticate with Cape Coordinator by running: $ CAPE_PASSWORD = <PASSWORD> cape login --email <EMAIL> Replace <PASSWORD> and <EMAIL> with the values set during Cape Coordinator installation. Cape Coordinator uses projects to manage which version of a policy is applied to a dataset. To create your first project, run: $ cape projects create first-project \"Hello Project World\" You can now add a policy to the project from a policy specification file. Copy the following YAML to a file called first-policy.yaml : version : 1 rules : # Set the column name - match : name : weight actions : - transform : # This example shows an unnamed transformation. # It tells the policy runner to: # (1) Apply the transformation numeric-rounding # (2) Round to one decimal place type : numeric-rounding dtype : Double precision : 1 Update the project with the first version of policy: $ cape projects update --from-spec first-policy.yaml first-project","title":"Create your first project"},{"location":"libraries/cape-python/coordinator-quickstart/#get-an-api-token","text":"To connect to Cape Coordinator from Cape Python you need an API token. Obtain the token by running: $ cape tokens create You'll see output like: A token for cape_user@mycape.com has been created! Token: <REDACTED TOKEN> \u203c Remember: Please keep the token safe and share it only over secure channels. Copy the token from the CLI. You will need it in the next section.","title":"Get an API token"},{"location":"libraries/cape-python/coordinator-quickstart/#write-the-policy-application-script","text":"The following script is very similar to the Cape Python quickstart guide , except in this case we're connecting to Cape Coordinator to retrieve the policy. Create a coordinator-policy.py file in your project, with the following content: import cape_privacy as cape import pandas as pd # In the Coordinator installation instructions you should have run the # Coordinator on port 8080. If not, edit the line below with the proper # port. client = cape . Client ( \"http://localhost:8080\" ) client . login ( \"<PASTE TOKEN HERE>\" ) p_dict = client . get_policy ( \"first-project\" ) policy = cape . parse_policy ( p_dict ) # Create a simple Pandas DataFrame df = pd . DataFrame ([ 114.432 , 134.622 , 142.984 ], columns = [ \"weight\" ]) df = cape . apply_policy ( policy , df , inplace = False ) print ( df . head ()) Make sure to replace <PASTE TOKEN HERE> with your API token.","title":"Write the policy application script"},{"location":"libraries/cape-python/coordinator-quickstart/#run-your-transformations","text":"In coordinator-policy.py we create a dataset programatically, so there are no further steps to load a dataset. Run the policy application script and view the output: $ python coordinator-policy.py","title":"Run your transformations"},{"location":"libraries/cape-python/policies/","text":"Policies # The data policy defines the data you want to change, and the transformations or redactions you want to apply. Cape Python requires data policies in YAML format. This example describes all the available YAML objects: # Required. The policy name. label : test_policy # Required. The Cape Privacy specification version. Must be 1. version : 1 # Configure your named transformations. # Named transformations allow you to reuse a transformation # with a set value throughout your policy. transformations : # This named transformation uses the built-in tokenizer transformation - name : my_tokenizer type : tokenizer max_token_len : 10 key : \"my secret\" rules : # Required. The column name. - match : name : fruit actions : # This example shows a named transformation. # It tells the policy runner to apply the my_tokenizer transformation # to all fields in the \"fruit\" column. - transform : name : my_tokenizer - match : name : weight actions : - transform : # This example shows an unnamed transformation. # It tells the policy runner to: # (1) Apply the transformation numeric-rounding # (2) Round to one decimal place type : numeric-rounding dtype : Double precision : 1","title":"Policies"},{"location":"libraries/cape-python/policies/#policies","text":"The data policy defines the data you want to change, and the transformations or redactions you want to apply. Cape Python requires data policies in YAML format. This example describes all the available YAML objects: # Required. The policy name. label : test_policy # Required. The Cape Privacy specification version. Must be 1. version : 1 # Configure your named transformations. # Named transformations allow you to reuse a transformation # with a set value throughout your policy. transformations : # This named transformation uses the built-in tokenizer transformation - name : my_tokenizer type : tokenizer max_token_len : 10 key : \"my secret\" rules : # Required. The column name. - match : name : fruit actions : # This example shows a named transformation. # It tells the policy runner to apply the my_tokenizer transformation # to all fields in the \"fruit\" column. - transform : name : my_tokenizer - match : name : weight actions : - transform : # This example shows an unnamed transformation. # It tells the policy runner to: # (1) Apply the transformation numeric-rounding # (2) Round to one decimal place type : numeric-rounding dtype : Double precision : 1","title":"Policies"},{"location":"libraries/cape-python/quickstart/","text":"Cape Python API # This guide provides an example of using Cape Python with either Pandas or Spark. Prerequisites # Python 3.6 or above. Cape Privacy recommends using a virtual environment such as venv . Installation # You can install Cape Python with pip: pip install cape-privacy Quickstart # Write the policy # The data policy file defines the target data and permissions. It is written in YAML. Cape Python reads the .yaml policy file and applies the policies based on your policy application script . Create a test-policy.yaml file in your project, with the following content: label : test-policy version : 1 rules : # Set the column name - match : name : weight actions : - transform : # This example shows an unnamed transformation. # It tells the policy runner to: # (1) Apply the transformation numeric-rounding # (2) Round to one decimal place type : numeric-rounding dtype : Double precision : 1 Write the policy application script # To apply the policy .yaml to your data, you must run a script that defines which policy you apply to which data target. Create a test-transformation.py file in your project, with the following content: Pandas import cape_privacy as cape import pandas as pd # Create a simple Pandas DataFrame df = pd . DataFrame ([ 114.432 , 134.622 , 142.984 ], columns = [ \"weight\" ]) # Load the privacy policy policy = cape . parse_policy ( \"test-policy.yaml\" ) # Apply the policy to the DataFrame df = cape . apply_policy ( policy , df , inplace = False ) # Output the altered data print ( df . head ()) Spark import cape_privacy as cape from pyspark import sql sess_builder = sql . SparkSession . builder sess_builder = sess_builder . appName ( 'cape.examples.rounding' ) sess_builder = sess_builder . config ( 'spark.sql.execution.arrow.enabled' , 'true' ) sess = sess_builder . getOrCreate () # Create a simple Spark DataFrame df = sess . createDataFrame ([ 114.432 , 134.622 , 142.984 ], \"double\" ) . toDF ( \"weight\" ) # Load the privacy policy policy = cape . parse_policy ( \"test-policy.yaml\" ) # Apply the policy to the DataFrame df = cape . apply_policy ( policy , df , inplace = False ) # Output the altered data print ( df . show ()) Run your transformations # The quickstart example creates a dataset programatically, so you can run the policy application script and view the output: python test-transformation.py Usage Best Practices # Ensure that you have your data collected and joined before applying transformations, especially in the case of multiple sensitive columns. Some transformations require sensitive data to be contained in the policy files. For this reason, keep your policy files stored securely. In a future release, we will support pulling transformation keys from key storage software, such as Hashicorp Vault. Consider using transformations as the final step in your pre-processing before creating a \"clean sink\" or \"safe dataset\". This means that you can begin your work on that clean dataset. Experiment with the transformations directly on your data to learn how they impact your data utility. Figure out the right utility vs. privacy tradeoff for the task at hand, and amend your policy accordingly.","title":"Quickstart"},{"location":"libraries/cape-python/quickstart/#cape-python-api","text":"This guide provides an example of using Cape Python with either Pandas or Spark.","title":"Cape Python API"},{"location":"libraries/cape-python/quickstart/#prerequisites","text":"Python 3.6 or above. Cape Privacy recommends using a virtual environment such as venv .","title":"Prerequisites"},{"location":"libraries/cape-python/quickstart/#installation","text":"You can install Cape Python with pip: pip install cape-privacy","title":"Installation"},{"location":"libraries/cape-python/quickstart/#quickstart","text":"","title":"Quickstart"},{"location":"libraries/cape-python/quickstart/#write-the-policy","text":"The data policy file defines the target data and permissions. It is written in YAML. Cape Python reads the .yaml policy file and applies the policies based on your policy application script . Create a test-policy.yaml file in your project, with the following content: label : test-policy version : 1 rules : # Set the column name - match : name : weight actions : - transform : # This example shows an unnamed transformation. # It tells the policy runner to: # (1) Apply the transformation numeric-rounding # (2) Round to one decimal place type : numeric-rounding dtype : Double precision : 1","title":"Write the policy"},{"location":"libraries/cape-python/quickstart/#write-the-policy-application-script","text":"To apply the policy .yaml to your data, you must run a script that defines which policy you apply to which data target. Create a test-transformation.py file in your project, with the following content: Pandas import cape_privacy as cape import pandas as pd # Create a simple Pandas DataFrame df = pd . DataFrame ([ 114.432 , 134.622 , 142.984 ], columns = [ \"weight\" ]) # Load the privacy policy policy = cape . parse_policy ( \"test-policy.yaml\" ) # Apply the policy to the DataFrame df = cape . apply_policy ( policy , df , inplace = False ) # Output the altered data print ( df . head ()) Spark import cape_privacy as cape from pyspark import sql sess_builder = sql . SparkSession . builder sess_builder = sess_builder . appName ( 'cape.examples.rounding' ) sess_builder = sess_builder . config ( 'spark.sql.execution.arrow.enabled' , 'true' ) sess = sess_builder . getOrCreate () # Create a simple Spark DataFrame df = sess . createDataFrame ([ 114.432 , 134.622 , 142.984 ], \"double\" ) . toDF ( \"weight\" ) # Load the privacy policy policy = cape . parse_policy ( \"test-policy.yaml\" ) # Apply the policy to the DataFrame df = cape . apply_policy ( policy , df , inplace = False ) # Output the altered data print ( df . show ())","title":"Write the policy application script"},{"location":"libraries/cape-python/quickstart/#run-your-transformations","text":"The quickstart example creates a dataset programatically, so you can run the policy application script and view the output: python test-transformation.py","title":"Run your transformations"},{"location":"libraries/cape-python/quickstart/#usage-best-practices","text":"Ensure that you have your data collected and joined before applying transformations, especially in the case of multiple sensitive columns. Some transformations require sensitive data to be contained in the policy files. For this reason, keep your policy files stored securely. In a future release, we will support pulling transformation keys from key storage software, such as Hashicorp Vault. Consider using transformations as the final step in your pre-processing before creating a \"clean sink\" or \"safe dataset\". This means that you can begin your work on that clean dataset. Experiment with the transformations directly on your data to learn how they impact your data utility. Figure out the right utility vs. privacy tradeoff for the task at hand, and amend your policy accordingly.","title":"Usage Best Practices"},{"location":"libraries/cape-python/redactions/","text":"Redactions # Redactions involve dropping the matched data. Unlike transformations , which modify but preserve data, redactions will change the shape of your dataframes. Cape Python has one built-in redaction function. This document describes what it does, and provides an example of how to use it in your policy. Warning Redactions change the shape of your data. Column redaction # The column-redact redaction deletes matching columns. - transform : type : \"column-redact\" # Replace <COLUMN_NAME> with the column name you want to redact. columns : [ \"<COLUMN_NAME>\" ]","title":"Redactions"},{"location":"libraries/cape-python/redactions/#redactions","text":"Redactions involve dropping the matched data. Unlike transformations , which modify but preserve data, redactions will change the shape of your dataframes. Cape Python has one built-in redaction function. This document describes what it does, and provides an example of how to use it in your policy. Warning Redactions change the shape of your data.","title":"Redactions"},{"location":"libraries/cape-python/redactions/#column-redaction","text":"The column-redact redaction deletes matching columns. - transform : type : \"column-redact\" # Replace <COLUMN_NAME> with the column name you want to redact. columns : [ \"<COLUMN_NAME>\" ]","title":"Column redaction"},{"location":"libraries/cape-python/transformations/","text":"Transformations # Transformations are functions that alter your data, ensuring it is free of sensitive information. Cape Python has five built-in transformation functions. This document describes what they do, and provides an example of how to use each transformation in your policy. Date perturbation # The date-perturbation transformation adds random noise to dates. The amount of noise depends on the min and max values that you set in the policy. - transform : type : date-pertubation frequency : <one of : 'year' , 'month' , 'day' , 'hour' , 'minute' , 'second' > min : <int or float> max : <int or float> # Optional. The base number to initialize the random number generator. # Pandas only (Spark does not currently support seeding) seed : <int> Date truncation # The date-truncation transformation shortens dates to a unit (year or month). Set the unit in frequency . - transform : type : date-truncation frequency : <one of : 'year' , 'month' , 'day' , 'hour' , 'minute' , 'second' > Numeric pertubation # The numeric-pertubation transformation adds random noise to numeric data sets. The amount of noise depends on the min and max values that you set in the policy. - transform : type : numeric-pertubation dtype : <Pandas Series type or Spark Series type> min : <int or float> max : <int or float> # Optional. The base number to initialize the random number generator. seed : <int> Numeric rounding # The numeric-rounding transformation rounds numeric values to a given number of decimal places. Use precision to set the number of decimal places. - transform : type : numeric-rounding dtype : <Pandas Series type or Spark Series type> precision : <int> Tokenizer # The tokenizer transformation maps a string to a token to obfuscate it. Warning Linkable tokenization for sensitive data is vulnerable to privacy attacks. Cape Privacy does not recommend sharing tokenized data with preserved linkability with untrusted or outside parties. Cape Python does not support anonymized transformations. - transform : type : tokenizer # Default is 64 max_token_len : <int or bytes> # If unspecified, Cape Python uses a random byte string key : <string or byte string> ReversibleTokenizer # The ReversibleTokenizer transformation maps a sting to a token to obfuscate it. However, when using the ReversibleTokenizer , the tokens can be reverted back to their plaintext form by using the TokenReverser . - transform : type : reversible-tokenizer # If unspecified, Cape Python uses a random byte string key : <string or byte string> TokenReverser # The TokenReverser is designed to be used with the ReversibleTokenizer . The TokenReverser reverts tokens produced by the ReversibleTokenizer back to their plaintext form. - transform : type : token-reverser # If unspecified, Cape Python uses a random byte string key : <string or byte string>","title":"Transformations"},{"location":"libraries/cape-python/transformations/#transformations","text":"Transformations are functions that alter your data, ensuring it is free of sensitive information. Cape Python has five built-in transformation functions. This document describes what they do, and provides an example of how to use each transformation in your policy.","title":"Transformations"},{"location":"libraries/cape-python/transformations/#date-perturbation","text":"The date-perturbation transformation adds random noise to dates. The amount of noise depends on the min and max values that you set in the policy. - transform : type : date-pertubation frequency : <one of : 'year' , 'month' , 'day' , 'hour' , 'minute' , 'second' > min : <int or float> max : <int or float> # Optional. The base number to initialize the random number generator. # Pandas only (Spark does not currently support seeding) seed : <int>","title":"Date perturbation"},{"location":"libraries/cape-python/transformations/#date-truncation","text":"The date-truncation transformation shortens dates to a unit (year or month). Set the unit in frequency . - transform : type : date-truncation frequency : <one of : 'year' , 'month' , 'day' , 'hour' , 'minute' , 'second' >","title":"Date truncation"},{"location":"libraries/cape-python/transformations/#numeric-pertubation","text":"The numeric-pertubation transformation adds random noise to numeric data sets. The amount of noise depends on the min and max values that you set in the policy. - transform : type : numeric-pertubation dtype : <Pandas Series type or Spark Series type> min : <int or float> max : <int or float> # Optional. The base number to initialize the random number generator. seed : <int>","title":"Numeric pertubation"},{"location":"libraries/cape-python/transformations/#numeric-rounding","text":"The numeric-rounding transformation rounds numeric values to a given number of decimal places. Use precision to set the number of decimal places. - transform : type : numeric-rounding dtype : <Pandas Series type or Spark Series type> precision : <int>","title":"Numeric rounding"},{"location":"libraries/cape-python/transformations/#tokenizer","text":"The tokenizer transformation maps a string to a token to obfuscate it. Warning Linkable tokenization for sensitive data is vulnerable to privacy attacks. Cape Privacy does not recommend sharing tokenized data with preserved linkability with untrusted or outside parties. Cape Python does not support anonymized transformations. - transform : type : tokenizer # Default is 64 max_token_len : <int or bytes> # If unspecified, Cape Python uses a random byte string key : <string or byte string>","title":"Tokenizer"},{"location":"libraries/cape-python/transformations/#reversibletokenizer","text":"The ReversibleTokenizer transformation maps a sting to a token to obfuscate it. However, when using the ReversibleTokenizer , the tokens can be reverted back to their plaintext form by using the TokenReverser . - transform : type : reversible-tokenizer # If unspecified, Cape Python uses a random byte string key : <string or byte string>","title":"ReversibleTokenizer"},{"location":"libraries/cape-python/transformations/#tokenreverser","text":"The TokenReverser is designed to be used with the ReversibleTokenizer . The TokenReverser reverts tokens produced by the ReversibleTokenizer back to their plaintext form. - transform : type : token-reverser # If unspecified, Cape Python uses a random byte string key : <string or byte string>","title":"TokenReverser"},{"location":"libraries/cape-python/tutorials/reversible-tokenization/","text":"Reversible Tokenizer # Here we show an example of how you can use the ReversibleTokenizer to tokenize data within a pandas dataframe. The ReversibleTokenizer will tokenize the input data so it can be used in a privacy preserving manner. The ReversibleTokenizer can be used in conjunction with the TokenReverser to recover the original data. Tokenizing Data # The ReversibleTokenizer and TokenReverser classes can be found in the pandas.transformations package. from cape_privacy.pandas.transformations import ReversibleTokenizer from cape_privacy.pandas.transformations import TokenReverser In this example, we will simply hide the names within our dataset. import pandas as pd plaintext_data = pd . DataFrame ({ 'name' : [ \"Alice\" , \"Bob\" , \"Carol\" ], \"# friends\" : [ 100 , 200 , 300 ]}) You instantiate a ReversibleTokenizer by passing it a key. For the TokenReverser to be able to reverse the tokens produced by the ReversibleTokenizer , you must use the same key. key = b \"5\" * 32 tokenizer = ReversibleTokenizer ( key = key ) tokenized = pd . DataFrame ( plaintext_data ) tokenized [ \"name\" ] = tokenizer ( plaintext_data [ \"name\" ]) Recovering Tokens # If we ever need to reveal the tokenized data, we can use the TokenReverser class. reverser = TokenReverser ( key = key ) recovered = pd . DataFrame ( tokenized ) recovered [ \"name\" ] = reverser ( tokenized [ \"name\" ]) You can see full code for this example on Github","title":"Reversible Tokenization"},{"location":"libraries/cape-python/tutorials/reversible-tokenization/#reversible-tokenizer","text":"Here we show an example of how you can use the ReversibleTokenizer to tokenize data within a pandas dataframe. The ReversibleTokenizer will tokenize the input data so it can be used in a privacy preserving manner. The ReversibleTokenizer can be used in conjunction with the TokenReverser to recover the original data.","title":"Reversible Tokenizer"},{"location":"libraries/cape-python/tutorials/reversible-tokenization/#tokenizing-data","text":"The ReversibleTokenizer and TokenReverser classes can be found in the pandas.transformations package. from cape_privacy.pandas.transformations import ReversibleTokenizer from cape_privacy.pandas.transformations import TokenReverser In this example, we will simply hide the names within our dataset. import pandas as pd plaintext_data = pd . DataFrame ({ 'name' : [ \"Alice\" , \"Bob\" , \"Carol\" ], \"# friends\" : [ 100 , 200 , 300 ]}) You instantiate a ReversibleTokenizer by passing it a key. For the TokenReverser to be able to reverse the tokens produced by the ReversibleTokenizer , you must use the same key. key = b \"5\" * 32 tokenizer = ReversibleTokenizer ( key = key ) tokenized = pd . DataFrame ( plaintext_data ) tokenized [ \"name\" ] = tokenizer ( plaintext_data [ \"name\" ])","title":"Tokenizing Data"},{"location":"libraries/cape-python/tutorials/reversible-tokenization/#recovering-tokens","text":"If we ever need to reveal the tokenized data, we can use the TokenReverser class. reverser = TokenReverser ( key = key ) recovered = pd . DataFrame ( tokenized ) recovered [ \"name\" ] = reverser ( tokenized [ \"name\" ]) You can see full code for this example on Github","title":"Recovering Tokens"},{"location":"libraries/pycape/","text":"pycape # pycape is a set of Python modules for interacting with your Cape Privacy data. Using pycape , you can: Create and query dataviews , or pointers to the data that you want to use to train a model using Cape's encrypted learning protocol. Submit and track jobs , which are computational sessions which contain instructions for how to train your model. Short Tutorial # Access your Cape projects by creating an instance of the main Cape class: from pycape import Cape c = Cape () c . login () my_projects = c . list_projects () Add dataviews to your project, review dataviews added by other organizations collaborating with you in the project, and submit your job. from pycape import VerticallyPartitionedLinearRegression my_project = c . get_project ( \"project_123\" ) my_project . create_dataview ( name = \"my-data\" , uri = \"s3://my-data.csv\" owner_label = \"my-org\" ) dvs = my_project . list_dataviews () vlr_job = VerticallyPartitionedLinearRegression ( train_dataview_x = dvs [ 0 ], train_dataview_y = dvs [ 1 ], model_owner = \"org_123\" , ) my_project . submit_job ( vlr_job ) See our example usage or a more in-depth tutorial . Installation # Prerequisites # Python 3.6+ pip Install via pip # We recommend that you use a Python \"Virtual Environment\" when running applications with pycape . Also ensure that your developement enviroment can access the Python Package Index (PyPI) via https. Once you've activated your virtual enviroment, use pip to install pycape and it's dependencies: $ pip install pycape License # Licensed under Apache License, Version 2.0. See LICENSE or http://www.apache.org/licenses/LICENSE-2.0 .","title":"Overview"},{"location":"libraries/pycape/#pycape","text":"pycape is a set of Python modules for interacting with your Cape Privacy data. Using pycape , you can: Create and query dataviews , or pointers to the data that you want to use to train a model using Cape's encrypted learning protocol. Submit and track jobs , which are computational sessions which contain instructions for how to train your model.","title":"pycape"},{"location":"libraries/pycape/#short-tutorial","text":"Access your Cape projects by creating an instance of the main Cape class: from pycape import Cape c = Cape () c . login () my_projects = c . list_projects () Add dataviews to your project, review dataviews added by other organizations collaborating with you in the project, and submit your job. from pycape import VerticallyPartitionedLinearRegression my_project = c . get_project ( \"project_123\" ) my_project . create_dataview ( name = \"my-data\" , uri = \"s3://my-data.csv\" owner_label = \"my-org\" ) dvs = my_project . list_dataviews () vlr_job = VerticallyPartitionedLinearRegression ( train_dataview_x = dvs [ 0 ], train_dataview_y = dvs [ 1 ], model_owner = \"org_123\" , ) my_project . submit_job ( vlr_job ) See our example usage or a more in-depth tutorial .","title":"Short Tutorial"},{"location":"libraries/pycape/#installation","text":"","title":"Installation"},{"location":"libraries/pycape/#prerequisites","text":"Python 3.6+ pip","title":"Prerequisites"},{"location":"libraries/pycape/#install-via-pip","text":"We recommend that you use a Python \"Virtual Environment\" when running applications with pycape . Also ensure that your developement enviroment can access the Python Package Index (PyPI) via https. Once you've activated your virtual enviroment, use pip to install pycape and it's dependencies: $ pip install pycape","title":"Install via pip"},{"location":"libraries/pycape/#license","text":"Licensed under Apache License, Version 2.0. See LICENSE or http://www.apache.org/licenses/LICENSE-2.0 .","title":"License"},{"location":"libraries/pycape/quickstart/","text":"Getting Started with PyCape # This guide provides an example of using pycape to run encrypted learning on Cape Cloud. Prerequisites # Python 3.6 or above. Cape Privacy recommends using a virtual environment such as venv . Installation # You can install Cape Python with pip: pip install pycape Quickstart # Register on Cape Cloud, Create an Organization and a Project # To get started using Cape, you will need to register on Cape Cloud. Once registered, you can create an organization (or join one you were invited to) and create your first project. Once your project is set up, you will want to generate an Organizational Token and a user token . These are managed on your organizational settings and user settings pages. Note Depending on your role and permissions in the organization, you may not be able to create and use organizational tokens. If someone else is an operator or an organizational administrator, they can create and use these tokens. Deploy a Cape Worker # If you are working alongside an operator, ensure they set up the Cape worker able to reach the Cape Cloud and the Amazon S3 bucket you would like to use. They will also need the organizational token to identify the worker to the Cape Cloud service. More information can be found in the Cape Worker documentation . Prepare and Add Data to Storage # Once the Cape worker is deployed, you'll want to add any files you plan on using for your project to the Amazon S3 bucket storage or another persistent storage that the worker can access. You will want to coordinate on indeces and aggregation with the other organization to ensure they line up when encrypted learning jobs are running. For more tips, please see our Linear Regression best practices . Login and Register Your Data Views # First, you'll need to login to Cape by following the library's login method and variables . Once you are logged in, you can register data views using a few short commands. # List projects my_projects = c . list_projects () # Register a Data View my_project = c . get_project ( id = \"project_123\" ) my_project . create_dataview ( name = \"my-data\" , uri = \"s3://my-data.csv\" , owner_label = \"my-org\" ) Run Your First Encrypted Learning Job # To create and run your first encrypted learning job, you'll use the pycape library. # List all data views my_project . list_dataviews () # Select the ones you want to use dataview_1 = my_project . get_dataview ( id = \"01EY48\" ) dataview_2 = my_project . get_dataview ( id = \"01EY49\" ) # Create a linear regression job vlr = VerticallyPartitionedLinearRegression ( x_train_dataview = dataview_1 , y_train_dataview = dataview_2 , model_location = \"s3://my-bucket\" , ) # Submit a job lr_job = my_project . submit_job ( job = vlr ) # Check job status and pull results lr_job . get_status () weights , metrics = lr_job . get_results () For a more detailed walkthrough, please review our Linear Regression tutorial .","title":"Quickstart"},{"location":"libraries/pycape/quickstart/#getting-started-with-pycape","text":"This guide provides an example of using pycape to run encrypted learning on Cape Cloud.","title":"Getting Started with PyCape"},{"location":"libraries/pycape/quickstart/#prerequisites","text":"Python 3.6 or above. Cape Privacy recommends using a virtual environment such as venv .","title":"Prerequisites"},{"location":"libraries/pycape/quickstart/#installation","text":"You can install Cape Python with pip: pip install pycape","title":"Installation"},{"location":"libraries/pycape/quickstart/#quickstart","text":"","title":"Quickstart"},{"location":"libraries/pycape/quickstart/#register-on-cape-cloud-create-an-organization-and-a-project","text":"To get started using Cape, you will need to register on Cape Cloud. Once registered, you can create an organization (or join one you were invited to) and create your first project. Once your project is set up, you will want to generate an Organizational Token and a user token . These are managed on your organizational settings and user settings pages. Note Depending on your role and permissions in the organization, you may not be able to create and use organizational tokens. If someone else is an operator or an organizational administrator, they can create and use these tokens.","title":"Register on Cape Cloud, Create an Organization and a Project"},{"location":"libraries/pycape/quickstart/#deploy-a-cape-worker","text":"If you are working alongside an operator, ensure they set up the Cape worker able to reach the Cape Cloud and the Amazon S3 bucket you would like to use. They will also need the organizational token to identify the worker to the Cape Cloud service. More information can be found in the Cape Worker documentation .","title":"Deploy a Cape Worker"},{"location":"libraries/pycape/quickstart/#prepare-and-add-data-to-storage","text":"Once the Cape worker is deployed, you'll want to add any files you plan on using for your project to the Amazon S3 bucket storage or another persistent storage that the worker can access. You will want to coordinate on indeces and aggregation with the other organization to ensure they line up when encrypted learning jobs are running. For more tips, please see our Linear Regression best practices .","title":"Prepare and Add Data to Storage"},{"location":"libraries/pycape/quickstart/#login-and-register-your-data-views","text":"First, you'll need to login to Cape by following the library's login method and variables . Once you are logged in, you can register data views using a few short commands. # List projects my_projects = c . list_projects () # Register a Data View my_project = c . get_project ( id = \"project_123\" ) my_project . create_dataview ( name = \"my-data\" , uri = \"s3://my-data.csv\" , owner_label = \"my-org\" )","title":"Login and Register Your Data Views"},{"location":"libraries/pycape/quickstart/#run-your-first-encrypted-learning-job","text":"To create and run your first encrypted learning job, you'll use the pycape library. # List all data views my_project . list_dataviews () # Select the ones you want to use dataview_1 = my_project . get_dataview ( id = \"01EY48\" ) dataview_2 = my_project . get_dataview ( id = \"01EY49\" ) # Create a linear regression job vlr = VerticallyPartitionedLinearRegression ( x_train_dataview = dataview_1 , y_train_dataview = dataview_2 , model_location = \"s3://my-bucket\" , ) # Submit a job lr_job = my_project . submit_job ( job = vlr ) # Check job status and pull results lr_job . get_status () weights , metrics = lr_job . get_results () For a more detailed walkthrough, please review our Linear Regression tutorial .","title":"Run Your First Encrypted Learning Job"},{"location":"libraries/pycape/reference/","text":"API Reference # pycape.Cape # This is the main class you instantiate to access the PyCape API. Use to authenticate with the Cape Cloud and manage top-level resources such as Project . delete_project ( self , id ) # Delete a Job by ID. Parameters: Name Type Description Default id str ID of Project . required Returns: Type Description str A success messsage write out. get_project ( self , id = None , label = None ) # Query a Project by either ID or label. Returns: Type Description Project A list of Project instances. Parameters: Name Type Description Default id Optional[str] ID of Project . None label Optional[str] Unique Project label. None Returns: Type Description Project A Project instance. list_projects ( self ) # Returns all list of projects that requesting user is a contributor of. Returns: Type Description str A list of Project instances. login ( self , token = None ) # Calls POST /v1/login . Authenticate with Cape Cloud in order to make subsequent requests. Parameters: Name Type Description Default token Optional[str] User authentication token. None Returns: Type Description None A success messsage write out. pycape.Project # Projects are the business contexts in which you collaborate with other organizations or Cape users to train models. Parameters: Name Type Description Default id str ID of Project . required name str name of Project . required label str label of Project . required description str description of Project . required owner dict Returned dictionary of fields related to the Project owner. required organizations list Returned list of fields related to the organizations associated with the Project . required dataviews list Returned list of DataViews added to the Project . required create_dataview ( self , name , uri , owner_id = None , owner_label = None , schema = None , development = False ) # Creates a DataView in Cape Cloud. Returns created Dataview Parameters: Name Type Description Default name str a name for the DataView . required uri str URI location of the dataset. required owner_id Optional[str] The ID of the organization that owns this dataset. None owner_label Optional[str] The label of the organization that owns this dataset. None schema Union[pandas.core.series.Series, List] The schema of the data that DataView points to. A string value for each column's datatype. Possible datatypes: string integer number datetime None development bool Whether the created dataview is in development mode or not. False Returns: Type Description DataView A DataView instance. delete_dataview ( self , id ) # Remove a DataView by ID. Parameters: Name Type Description Default id str ID of DataView . required get_dataview ( self , id = None , uri = None ) # Query a DataView for the scoped Project by DataView ID or URI. Parameters: Name Type Description Default id Optional[str] ID of DataView . None uri Optional[str] Unique DataView URI. None Returns: Type Description DataView A DataView instance. get_job ( self , id ) # Returns a Job given an ID. Parameters: Name Type Description Default id str ID of Job . required Returns: Type Description List[pycape.pycape.api.job.job.Job] A Job instance. list_dataviews ( self ) # Returns a list of dataviews for the scoped Project . Returns: Type Description List[pycape.pycape.api.dataview.dataview.DataView] A list of DataView instances. list_organizations ( self ) # Returns all list of organizations that requesting user is a contributor of. Returns: Type Description str A list of Organization instances. submit_job ( self , task , timeout = 600 ) # Submits a Job to be run by your Cape worker in collaboration with other organizations in your Project . Parameters: Name Type Description Default task Task Instance of class that inherits from Task . required timeout float How long (in ms) a Cape Worker should run before canceling the Job . 600 Returns: Type Description Job A Job instance. pycape.Organization # Organization represents an organization in Cape. Parameters: Name Type Description Default id str ID of Organization required name str name of Organization . required label str label of Organization . required pycape.DataView # Dataviews store metadata around datasets, including namely a pointer to the dataset's location. Parameters: Name Type Description Default id str ID of DataView required name str name of DataView . required schema list schema of the data that DataView points to. required location str URI of DataView . required owner dict Dictionary of fields related to the DataView owner. required user_id str User ID of requester. required development bool Whether this dataview is in development mode or not. required pycape.Job # Jobs track the status and eventually report the results of computation sessions run on Cape workers. Parameters: Name Type Description Default id str ID of Job required status str name of Job . required project_id str ID of Project . required approve ( self , org_id ) # Approve the Job on behalf of your organization. Once all organizations approve a job, the computation will run. Parameters: Name Type Description Default org_id str ID of Organization . required Returns: Type Description Job A Job instance. get_results ( self ) # Given the requesters project role and authorization level, returns the trained model's weights and metrics. Returns: Type Description Tuple[numpy.ndarray, dict] weights: A numpy array. metrics: A dictionary of different metric values. get_status ( self ) # Query the current status of the Cape Job . Returns: Type Description str A Job status string. Status Types: Status Description Initialized Job has been initialized. NeedsApproval Job is awaiting approval by at least one party. Approved Job has been approved, the computation will commence. Rejected Job has been rejected, the computation will not run. Started Job has started. Completed Job has completed. Stopped Job has been stopped. Error Error in running Job. pycape.Task # Tasks contain the instructions for how a Cape worker should run a job. Parameters: Name Type Description Default model_location str The AWS S3 bucket name to which Cape will write the output of the model training. required model_owner str The ID of the organization participating in the computation that will own the required pycape.VerticallyPartitionedLinearRegressionJob # Inherits from: Task . Contains instructions for encrypted training of linear regression models using vertically-partitioned datasets. Vertically-partitioned datasets refer to the joining of columns (i.e. features) from several parties. Note This task expects DataViews with floating-point inputs. Internally, values will be re-encoded by the Cape Worker into the fixed-point numbers necessary for encrypted computation. Note This task expects its input DataViews to be aligned by index (although indexing columns need not be present in either of the DataViews or their underlying datasets). Note This task expects its input DataViews to have max values scaled between 1.0 and 10.0. Currently, input data must be scaled to single digits; for any floating-point vector c in the input data views x and y , c must be scaled such that 1.0 <= max(c) < 10.0. This bound allows the Cape Worker to allocate all of its precision for significant digits throughout the linear regression computation, while still maintaining the guarantee that fixed-point numbers won't overflow. For logarithimically-distributed vectors, we recommend applying a log-transform before scaling to this bound. Parameters: Name Type Description Default x_train_dataview Union[`DataView`, `DataView`List[str]] DataView that points to a dataset that contains training set values. required y_train_dataview Union[`DataView`, `DataView`List[str]] DataView that points to a dataset that contains target values. required model_location str The AWS S3 bucket name to which Cape will write the output of the model training. required model_owner str The ID of the organization participating in the computation that will own the required","title":"Reference"},{"location":"libraries/pycape/reference/#api-reference","text":"","title":"API Reference"},{"location":"libraries/pycape/reference/#pycapecape","text":"This is the main class you instantiate to access the PyCape API. Use to authenticate with the Cape Cloud and manage top-level resources such as Project .","title":"pycape.Cape"},{"location":"libraries/pycape/reference/#pycape.pycape.api.cape.cape.Cape.delete_project","text":"Delete a Job by ID. Parameters: Name Type Description Default id str ID of Project . required Returns: Type Description str A success messsage write out.","title":"delete_project()"},{"location":"libraries/pycape/reference/#pycape.pycape.api.cape.cape.Cape.get_project","text":"Query a Project by either ID or label. Returns: Type Description Project A list of Project instances. Parameters: Name Type Description Default id Optional[str] ID of Project . None label Optional[str] Unique Project label. None Returns: Type Description Project A Project instance.","title":"get_project()"},{"location":"libraries/pycape/reference/#pycape.pycape.api.cape.cape.Cape.list_projects","text":"Returns all list of projects that requesting user is a contributor of. Returns: Type Description str A list of Project instances.","title":"list_projects()"},{"location":"libraries/pycape/reference/#pycape.pycape.api.cape.cape.Cape.login","text":"Calls POST /v1/login . Authenticate with Cape Cloud in order to make subsequent requests. Parameters: Name Type Description Default token Optional[str] User authentication token. None Returns: Type Description None A success messsage write out.","title":"login()"},{"location":"libraries/pycape/reference/#pycapeproject","text":"Projects are the business contexts in which you collaborate with other organizations or Cape users to train models. Parameters: Name Type Description Default id str ID of Project . required name str name of Project . required label str label of Project . required description str description of Project . required owner dict Returned dictionary of fields related to the Project owner. required organizations list Returned list of fields related to the organizations associated with the Project . required dataviews list Returned list of DataViews added to the Project . required","title":"pycape.Project"},{"location":"libraries/pycape/reference/#pycape.pycape.api.project.project.Project.create_dataview","text":"Creates a DataView in Cape Cloud. Returns created Dataview Parameters: Name Type Description Default name str a name for the DataView . required uri str URI location of the dataset. required owner_id Optional[str] The ID of the organization that owns this dataset. None owner_label Optional[str] The label of the organization that owns this dataset. None schema Union[pandas.core.series.Series, List] The schema of the data that DataView points to. A string value for each column's datatype. Possible datatypes: string integer number datetime None development bool Whether the created dataview is in development mode or not. False Returns: Type Description DataView A DataView instance.","title":"create_dataview()"},{"location":"libraries/pycape/reference/#pycape.pycape.api.project.project.Project.delete_dataview","text":"Remove a DataView by ID. Parameters: Name Type Description Default id str ID of DataView . required","title":"delete_dataview()"},{"location":"libraries/pycape/reference/#pycape.pycape.api.project.project.Project.get_dataview","text":"Query a DataView for the scoped Project by DataView ID or URI. Parameters: Name Type Description Default id Optional[str] ID of DataView . None uri Optional[str] Unique DataView URI. None Returns: Type Description DataView A DataView instance.","title":"get_dataview()"},{"location":"libraries/pycape/reference/#pycape.pycape.api.project.project.Project.get_job","text":"Returns a Job given an ID. Parameters: Name Type Description Default id str ID of Job . required Returns: Type Description List[pycape.pycape.api.job.job.Job] A Job instance.","title":"get_job()"},{"location":"libraries/pycape/reference/#pycape.pycape.api.project.project.Project.list_dataviews","text":"Returns a list of dataviews for the scoped Project . Returns: Type Description List[pycape.pycape.api.dataview.dataview.DataView] A list of DataView instances.","title":"list_dataviews()"},{"location":"libraries/pycape/reference/#pycape.pycape.api.project.project.Project.list_organizations","text":"Returns all list of organizations that requesting user is a contributor of. Returns: Type Description str A list of Organization instances.","title":"list_organizations()"},{"location":"libraries/pycape/reference/#pycape.pycape.api.project.project.Project.submit_job","text":"Submits a Job to be run by your Cape worker in collaboration with other organizations in your Project . Parameters: Name Type Description Default task Task Instance of class that inherits from Task . required timeout float How long (in ms) a Cape Worker should run before canceling the Job . 600 Returns: Type Description Job A Job instance.","title":"submit_job()"},{"location":"libraries/pycape/reference/#pycapeorganization","text":"Organization represents an organization in Cape. Parameters: Name Type Description Default id str ID of Organization required name str name of Organization . required label str label of Organization . required","title":"pycape.Organization"},{"location":"libraries/pycape/reference/#pycapedataview","text":"Dataviews store metadata around datasets, including namely a pointer to the dataset's location. Parameters: Name Type Description Default id str ID of DataView required name str name of DataView . required schema list schema of the data that DataView points to. required location str URI of DataView . required owner dict Dictionary of fields related to the DataView owner. required user_id str User ID of requester. required development bool Whether this dataview is in development mode or not. required","title":"pycape.DataView"},{"location":"libraries/pycape/reference/#pycapejob","text":"Jobs track the status and eventually report the results of computation sessions run on Cape workers. Parameters: Name Type Description Default id str ID of Job required status str name of Job . required project_id str ID of Project . required","title":"pycape.Job"},{"location":"libraries/pycape/reference/#pycape.pycape.api.job.job.Job.approve","text":"Approve the Job on behalf of your organization. Once all organizations approve a job, the computation will run. Parameters: Name Type Description Default org_id str ID of Organization . required Returns: Type Description Job A Job instance.","title":"approve()"},{"location":"libraries/pycape/reference/#pycape.pycape.api.job.job.Job.get_results","text":"Given the requesters project role and authorization level, returns the trained model's weights and metrics. Returns: Type Description Tuple[numpy.ndarray, dict] weights: A numpy array. metrics: A dictionary of different metric values.","title":"get_results()"},{"location":"libraries/pycape/reference/#pycape.pycape.api.job.job.Job.get_status","text":"Query the current status of the Cape Job . Returns: Type Description str A Job status string. Status Types: Status Description Initialized Job has been initialized. NeedsApproval Job is awaiting approval by at least one party. Approved Job has been approved, the computation will commence. Rejected Job has been rejected, the computation will not run. Started Job has started. Completed Job has completed. Stopped Job has been stopped. Error Error in running Job.","title":"get_status()"},{"location":"libraries/pycape/reference/#pycapetask","text":"Tasks contain the instructions for how a Cape worker should run a job. Parameters: Name Type Description Default model_location str The AWS S3 bucket name to which Cape will write the output of the model training. required model_owner str The ID of the organization participating in the computation that will own the required","title":"pycape.Task"},{"location":"libraries/pycape/reference/#pycapeverticallypartitionedlinearregressionjob","text":"Inherits from: Task . Contains instructions for encrypted training of linear regression models using vertically-partitioned datasets. Vertically-partitioned datasets refer to the joining of columns (i.e. features) from several parties. Note This task expects DataViews with floating-point inputs. Internally, values will be re-encoded by the Cape Worker into the fixed-point numbers necessary for encrypted computation. Note This task expects its input DataViews to be aligned by index (although indexing columns need not be present in either of the DataViews or their underlying datasets). Note This task expects its input DataViews to have max values scaled between 1.0 and 10.0. Currently, input data must be scaled to single digits; for any floating-point vector c in the input data views x and y , c must be scaled such that 1.0 <= max(c) < 10.0. This bound allows the Cape Worker to allocate all of its precision for significant digits throughout the linear regression computation, while still maintaining the guarantee that fixed-point numbers won't overflow. For logarithimically-distributed vectors, we recommend applying a log-transform before scaling to this bound. Parameters: Name Type Description Default x_train_dataview Union[`DataView`, `DataView`List[str]] DataView that points to a dataset that contains training set values. required y_train_dataview Union[`DataView`, `DataView`List[str]] DataView that points to a dataset that contains target values. required model_location str The AWS S3 bucket name to which Cape will write the output of the model training. required model_owner str The ID of the organization participating in the computation that will own the required","title":"pycape.VerticallyPartitionedLinearRegressionJob"},{"location":"libraries/pycape/tutorials/submit_linear_regression_job/","text":"Train a Linear Regression Model using Cape DataViews & Jobs # This tutorial will walk you through the process of training an encrypted linear regression model in collaboration with another organization using Cape Privacy. You'll learn how to: Register dataset pointers (i.e. DataViews) with Cape Cloud. Review DataViews from other organizations in your project. Approve and reject model computation jobs. View the metrics or weights of the trained model, depending on your role in the project. We'll use the Cape UI to set up and review activity in the project. We'll also use the pytest Python library to create and review pointers to datasets or DataViews , create Tasks , which are Cape Python objects that contain instructions on how to train a model using the data provided, and review Jobs in order to track the status of the training, and view the results of the trained model. Project Setup # Create an Organization # First you'll need to create an organization at demo.capeprivacy.com . Once you've created your organization, you can navigate to Organization Settings and generate a token for your organization. You'll need this token to configure your worker . Take note of this value as you cannot recover it after you reload the page. Create a Project # Next, create a Project within one of the organizations you just created. Projects serve as the context in which you can define and review Jobs with other organizations. Add organizations to your project in order to begin collaborating with them on training a model. Get a User Token # Finally, we will need a user token to authenticate against pycape . Ensure you are working within your user context and navigate to Account Settings to create a token. Take note of this value as, like the user token, you cannot recover it after you reload the page. That is it for the UI for now! We'll return later to review DataViews and approve Jobs . Next we will set up these DataViews and Jobs in pycape . Working with the PyCape Python Library # pycape is a set of Python modules for interacting with your Cape Privacy data. First, install pycape . Login to PyCape # Before you can make requests to Cape Cloud, you'll need to authenticate with the API. Follow these instructions to authenticate with our API using pycape . Once you've logged in successfully, you should see a success message. >>> c = Cape () >>> c . login () Login successful Add a DataView to your project # Use the list_projects method defined on the main Cape class to query a list of projects that belong to your organization. >>> my_projects = c . list_projects () PROJECT ID NAME LABEL ----------- ----------------------- ----------------------- project_123 Default Risk Assessment default - risk - assessment >>> my_projects [ Project ( id = project_123 , name = Default Risk Assessment , label = default - risk - assessment )] To create a DataView and add it to your project, simply call the create_dataview method defined on the Project class. >>> my_project = c . get_project ( id = \"project_123\" ) >>> my_project . create_dataview ( name = \"my-data\" , uri = \"s3://my-data.csv\" , owner_label = \"my-org\" ) All DataViews must be associated with an organization. This association can be made by passing either an owner_label or an owner_id to the create_dataview method. Note Use the list_organizations method defined on the Project class to get the metadata of the organizations collaborating on the project that you are a member of. Note Unless your dataset is accessible via HTTP or you've provided access to your S3 bucket you'll need to specify your schema . Review Your Collaborator's DataView # Before we can submit a job to train our linear regression model, we'll need to review the DataViews added to the project by our collaborators. Use the list_dataviews method defined on the Project class to inspect the name, owner (organization) and location of DataViews added to the project: >>> my_project = c . get_project ( id = \"project_123\" ) >>> dataviews = my_project . list_dataviews () DATAVIEW ID NAME LOCATION OWNER ----------- ------------ --------------- ------------- 01 EY48 orgacle - data s3 : // mydata . csv orgacle ( You ) 01 EY49 atlas - data atlas Note You'll only be able to see the locations or URIs of datasets that belong to your organization. You can also inspect the schema of each Dataview in your project in order to see the data types of the columns, and to assess which data columns should be used to train the linear regression model. >>> dataviews [ 1 ] . schema { 'debt equity ratio' : 'number' , 'operating margin' : 'number' , 'working capital' : 'integer' } You can also review the dataviews added to your project in the UI. Submitting a Linear Regression Job # Now that we've added our own DataView to the project, and vetted the DataView of our collaborator, we are ready to submit our Cape linear regression job. Pass the DataView that contains training data to x_train_dataview , and the DataView that contains the target values to y_train_dataview . To specify which organization participating in the computation will own the results of the trained model, pass the ID of the intended organization to the model_owner parameter. You can view the IDs of organizations collaborating on the project using the list_organizations method defined on the Project class. You must be a member of the organization to specify them as the model_owner . You'll also need to specify the S3 Bucket location that you would like Cape to save your model results to . >>> dataview_1 = my_project . get_dataview ( id = \"01EY48\" ) >>> dataview_2 = my_project . get_dataview ( id = \"01EY49\" ) >>> vlr = VerticallyPartitionedLinearRegression ( >>> x_train_dataview = dataview_1 , >>> y_train_dataview = dataview_2 , >>> model_location = \"s3://my-bucket\" , >>> model_owner = \"org_123\" , >>> ) >>> my_project . submit_job ( vlr ) You can specify which data columns the model should be trained on or evaluated against by passing the dataview to the VerticallyPartitionedLinearRegression class like so: >>> VerticallyPartitionedLinearRegression ( >>> x_train_dataview = dataview_1 [ \"debt equity ratio\" ], >>> y_train_dataview = dataview_2 [ \"debt equity ratio\" ], >>> model_location = \"s3://my-bucket\" , >>> model_owner = \"org_123\" , >>> ) VerticallyPartitionedLinearRegression ( x_train_dataview = Orgacle Dataview [ 'debt equity ratio' ], y_train_dataview = Atlas Dataview [ 'debt equity ratio' ], model_location = s3 : // my - bucket ) Note In order for your linear regression job to train a model using Cape's encrypted learning protocol, you'll need to run your own Cape workers. Read our documentation to get set up with Cape workers . Note VerticallyPartitionedLinearRegression currently expects a bound on its input data in order to avoid precision loss during model training. See its reference documentation for more details. Note DataView indices must be aligned across parties before being used for a VerticallyPartitionedLinearRegression . Tracking Job Status # After submitting your job, you should be able to see the status and details of your Job in the UI. To check the status of your submitted linear regression job using pycape , use the get_status method: >>> lr_job = my_project . get_job ( id = \"abc_123\" ) >>> lr_job . get_status () Success Approving Jobs # Before Cape can begin to train a linear regression model using the datasets submitted via submit_job method, both parties need to review and approve the Job. To approve, you'll need to head over to the UI and navigate to your Job's details page. Once you've reviewed the details of your Job are correct, you can click \"Approve Job\" to let Cape know the job looks good on your end. Note Before your job can run, both parties need to approve it. Getting Weights and Metrics from Trained Model # Once your job has successfully completed, you can view the results of the trained model. Whether you can view the weights or metrics of the trained model (or both!) depends on the role you and your organization play in the project. To view the weights and metrics of a job, use the get_results method: >>> lr_job = my_project . get_job ( id = \"abc_123\" ) >>> weights , metrics = lr_job . get_results () >>> weights array ([ 12.14955139 , 1.96560669 ]) >>> metrics { 'r_squared_result' : [ 0.8804865768463074 ], 'mse_result' : [ 37.94773864746094 ]} If you are the model owner, the first value in the returned tuple will be populated with a numpy array of weights from your trained model. The first element in the weights array is the intercept of the linear model, and subsequent elements are its feature coefficients. Note To access model weights you'll need to inform pycape about your AWS IAM authentication credentials .","title":"Train Linear Regression"},{"location":"libraries/pycape/tutorials/submit_linear_regression_job/#train-a-linear-regression-model-using-cape-dataviews-jobs","text":"This tutorial will walk you through the process of training an encrypted linear regression model in collaboration with another organization using Cape Privacy. You'll learn how to: Register dataset pointers (i.e. DataViews) with Cape Cloud. Review DataViews from other organizations in your project. Approve and reject model computation jobs. View the metrics or weights of the trained model, depending on your role in the project. We'll use the Cape UI to set up and review activity in the project. We'll also use the pytest Python library to create and review pointers to datasets or DataViews , create Tasks , which are Cape Python objects that contain instructions on how to train a model using the data provided, and review Jobs in order to track the status of the training, and view the results of the trained model.","title":"Train a Linear Regression Model using Cape DataViews &amp; Jobs"},{"location":"libraries/pycape/tutorials/submit_linear_regression_job/#project-setup","text":"","title":"Project Setup"},{"location":"libraries/pycape/tutorials/submit_linear_regression_job/#create-an-organization","text":"First you'll need to create an organization at demo.capeprivacy.com . Once you've created your organization, you can navigate to Organization Settings and generate a token for your organization. You'll need this token to configure your worker . Take note of this value as you cannot recover it after you reload the page.","title":"Create an Organization"},{"location":"libraries/pycape/tutorials/submit_linear_regression_job/#create-a-project","text":"Next, create a Project within one of the organizations you just created. Projects serve as the context in which you can define and review Jobs with other organizations. Add organizations to your project in order to begin collaborating with them on training a model.","title":"Create a Project"},{"location":"libraries/pycape/tutorials/submit_linear_regression_job/#get-a-user-token","text":"Finally, we will need a user token to authenticate against pycape . Ensure you are working within your user context and navigate to Account Settings to create a token. Take note of this value as, like the user token, you cannot recover it after you reload the page. That is it for the UI for now! We'll return later to review DataViews and approve Jobs . Next we will set up these DataViews and Jobs in pycape .","title":"Get a User Token"},{"location":"libraries/pycape/tutorials/submit_linear_regression_job/#working-with-the-pycape-python-library","text":"pycape is a set of Python modules for interacting with your Cape Privacy data. First, install pycape .","title":"Working with the PyCape Python Library"},{"location":"libraries/pycape/tutorials/submit_linear_regression_job/#login-to-pycape","text":"Before you can make requests to Cape Cloud, you'll need to authenticate with the API. Follow these instructions to authenticate with our API using pycape . Once you've logged in successfully, you should see a success message. >>> c = Cape () >>> c . login () Login successful","title":"Login to PyCape"},{"location":"libraries/pycape/tutorials/submit_linear_regression_job/#add-a-dataview-to-your-project","text":"Use the list_projects method defined on the main Cape class to query a list of projects that belong to your organization. >>> my_projects = c . list_projects () PROJECT ID NAME LABEL ----------- ----------------------- ----------------------- project_123 Default Risk Assessment default - risk - assessment >>> my_projects [ Project ( id = project_123 , name = Default Risk Assessment , label = default - risk - assessment )] To create a DataView and add it to your project, simply call the create_dataview method defined on the Project class. >>> my_project = c . get_project ( id = \"project_123\" ) >>> my_project . create_dataview ( name = \"my-data\" , uri = \"s3://my-data.csv\" , owner_label = \"my-org\" ) All DataViews must be associated with an organization. This association can be made by passing either an owner_label or an owner_id to the create_dataview method. Note Use the list_organizations method defined on the Project class to get the metadata of the organizations collaborating on the project that you are a member of. Note Unless your dataset is accessible via HTTP or you've provided access to your S3 bucket you'll need to specify your schema .","title":"Add a DataView to your project"},{"location":"libraries/pycape/tutorials/submit_linear_regression_job/#review-your-collaborators-dataview","text":"Before we can submit a job to train our linear regression model, we'll need to review the DataViews added to the project by our collaborators. Use the list_dataviews method defined on the Project class to inspect the name, owner (organization) and location of DataViews added to the project: >>> my_project = c . get_project ( id = \"project_123\" ) >>> dataviews = my_project . list_dataviews () DATAVIEW ID NAME LOCATION OWNER ----------- ------------ --------------- ------------- 01 EY48 orgacle - data s3 : // mydata . csv orgacle ( You ) 01 EY49 atlas - data atlas Note You'll only be able to see the locations or URIs of datasets that belong to your organization. You can also inspect the schema of each Dataview in your project in order to see the data types of the columns, and to assess which data columns should be used to train the linear regression model. >>> dataviews [ 1 ] . schema { 'debt equity ratio' : 'number' , 'operating margin' : 'number' , 'working capital' : 'integer' } You can also review the dataviews added to your project in the UI.","title":"Review Your Collaborator's DataView"},{"location":"libraries/pycape/tutorials/submit_linear_regression_job/#submitting-a-linear-regression-job","text":"Now that we've added our own DataView to the project, and vetted the DataView of our collaborator, we are ready to submit our Cape linear regression job. Pass the DataView that contains training data to x_train_dataview , and the DataView that contains the target values to y_train_dataview . To specify which organization participating in the computation will own the results of the trained model, pass the ID of the intended organization to the model_owner parameter. You can view the IDs of organizations collaborating on the project using the list_organizations method defined on the Project class. You must be a member of the organization to specify them as the model_owner . You'll also need to specify the S3 Bucket location that you would like Cape to save your model results to . >>> dataview_1 = my_project . get_dataview ( id = \"01EY48\" ) >>> dataview_2 = my_project . get_dataview ( id = \"01EY49\" ) >>> vlr = VerticallyPartitionedLinearRegression ( >>> x_train_dataview = dataview_1 , >>> y_train_dataview = dataview_2 , >>> model_location = \"s3://my-bucket\" , >>> model_owner = \"org_123\" , >>> ) >>> my_project . submit_job ( vlr ) You can specify which data columns the model should be trained on or evaluated against by passing the dataview to the VerticallyPartitionedLinearRegression class like so: >>> VerticallyPartitionedLinearRegression ( >>> x_train_dataview = dataview_1 [ \"debt equity ratio\" ], >>> y_train_dataview = dataview_2 [ \"debt equity ratio\" ], >>> model_location = \"s3://my-bucket\" , >>> model_owner = \"org_123\" , >>> ) VerticallyPartitionedLinearRegression ( x_train_dataview = Orgacle Dataview [ 'debt equity ratio' ], y_train_dataview = Atlas Dataview [ 'debt equity ratio' ], model_location = s3 : // my - bucket ) Note In order for your linear regression job to train a model using Cape's encrypted learning protocol, you'll need to run your own Cape workers. Read our documentation to get set up with Cape workers . Note VerticallyPartitionedLinearRegression currently expects a bound on its input data in order to avoid precision loss during model training. See its reference documentation for more details. Note DataView indices must be aligned across parties before being used for a VerticallyPartitionedLinearRegression .","title":"Submitting a Linear Regression Job"},{"location":"libraries/pycape/tutorials/submit_linear_regression_job/#tracking-job-status","text":"After submitting your job, you should be able to see the status and details of your Job in the UI. To check the status of your submitted linear regression job using pycape , use the get_status method: >>> lr_job = my_project . get_job ( id = \"abc_123\" ) >>> lr_job . get_status () Success","title":"Tracking Job Status"},{"location":"libraries/pycape/tutorials/submit_linear_regression_job/#approving-jobs","text":"Before Cape can begin to train a linear regression model using the datasets submitted via submit_job method, both parties need to review and approve the Job. To approve, you'll need to head over to the UI and navigate to your Job's details page. Once you've reviewed the details of your Job are correct, you can click \"Approve Job\" to let Cape know the job looks good on your end. Note Before your job can run, both parties need to approve it.","title":"Approving Jobs"},{"location":"libraries/pycape/tutorials/submit_linear_regression_job/#getting-weights-and-metrics-from-trained-model","text":"Once your job has successfully completed, you can view the results of the trained model. Whether you can view the weights or metrics of the trained model (or both!) depends on the role you and your organization play in the project. To view the weights and metrics of a job, use the get_results method: >>> lr_job = my_project . get_job ( id = \"abc_123\" ) >>> weights , metrics = lr_job . get_results () >>> weights array ([ 12.14955139 , 1.96560669 ]) >>> metrics { 'r_squared_result' : [ 0.8804865768463074 ], 'mse_result' : [ 37.94773864746094 ]} If you are the model owner, the first value in the returned tuple will be populated with a numpy array of weights from your trained model. The first element in the weights array is the intercept of the linear model, and subsequent elements are its feature coefficients. Note To access model weights you'll need to inform pycape about your AWS IAM authentication credentials .","title":"Getting Weights and Metrics from Trained Model"},{"location":"libraries/pycape/usage/dataview/","text":"Managing DataViews # Get list of data views for a project # my_project = c . get_project ( id = \"project_123\" ) my_project . list_dataviews () Default response: DATAVIEW ID NAME LOCATION OWNER -------------------------- ------------- --------------- ----------- 01EY48EFT4H7PWAN45SG2AEZ81 armazorn-data s3://mydata.csv armazorn ( You ) 01EY49J86722ENT9JSMKTE65EX gorgle-data gorgle Get a data view # my_project = c . get_project ( id = \"project_123\" ) # get by id my_project . get_dataview ( id = \"dataview_123\" ) # get by uri my_project . get_dataview ( uri = \"s3://my-data.csv\" ) Default response: DataView ( id = dataview_123, name = my-data, location = s3://my-data.csv ) Add a data view to a project # Initialize a DataView class and pass the instance to the create_dataview method. my_project = c . get_project ( id = \"project_123\" ) my_project . create_dataview ( name = \"my-data\" , uri = \"s3://my-data.csv\" , owner_label = \"my-org\" ) Default response: DataView ( id = dataview_123, name = my-data, location = s3://my-data.csv ) DataViews and Schemas # DataView schemas allow you to clarify the data types of your dataset. They will be visible for other project contributors - even ones from other organizations - to your project to query and inspect. By inspecting the schema property, other project contributors are able to identify which data columns should be used to train the model. If you provide a dataset to Cape that is accessible via HTTP or S3, Cape will download your data's column headers and create a schema. Providing S3 read access to your DataView # In order to make your dataset accessible in S3 you'll need to inform pycape about your S3 bucket's IAM authentication credentials. Cape expects values for the following AWS configuration keys: AWS_ACCESS_KEY_ID , AWS_SECRET_ACCESS_KEY , and AWS_REGION . You can set these keys as environment variables in the interpreter running pycape : export AWS_ACCESS_KEY_ID = <Access-Key> export AWS_SECRET_ACCESS_KEY = <Secret-Key> export AWS_REGION = <Region> Alternatively you can simply add these keys to your AWS Configuration file . Specifying a Schema for your DataView # However, if your dataset is not accessible you'll have to specify your data's schema yourself. You can do so using the schema parameter. DataViews can be instantiated with a pandas Series schema of type dataframe.dtypes : >>> import pandas as pd >>> df = pd . DataFrame ( data = { \"col_1\" : [ 1 , 2 ], \"col_2\" : [ 3 , 4 ]}) >>> dataview = my_project . create_dataview ( name = \"my-data\" , uri = \"s3://my-data.csv\" , owner_label = \"my-org\" , schema = df . dtypes ) >>> dataview . schema { 'col_1' : 'integer' , 'col_2' : 'integer' } DataViews can also be instantiated as a list of data types. Accepted schema data types include: string , integer , number , datetime , and any . >>> schema = [{ \"name\" : \"col_1\" , \"schema_type\" : \"integer\" }, { \"name\" : \"col_2\" , \"schema_type\" : \"integer\" }] >>> data_view = DataView ( name = \"my-data\" , uri = \"s3://my-data.csv\" , schema = schema ) >>> dataview . schema { 'col_1' : 'integer' , 'col_2' : 'integer' } Delete a DataView # my_project = c . get_project ( id = \"project_123\" ) my_project . delete_dataview ( id = \"dataview_123\" ) Default response: DataView ( dataview_123 ) deleted","title":"DataViews"},{"location":"libraries/pycape/usage/dataview/#managing-dataviews","text":"","title":"Managing DataViews"},{"location":"libraries/pycape/usage/dataview/#get-list-of-data-views-for-a-project","text":"my_project = c . get_project ( id = \"project_123\" ) my_project . list_dataviews () Default response: DATAVIEW ID NAME LOCATION OWNER -------------------------- ------------- --------------- ----------- 01EY48EFT4H7PWAN45SG2AEZ81 armazorn-data s3://mydata.csv armazorn ( You ) 01EY49J86722ENT9JSMKTE65EX gorgle-data gorgle","title":"Get list of data views for a project"},{"location":"libraries/pycape/usage/dataview/#get-a-data-view","text":"my_project = c . get_project ( id = \"project_123\" ) # get by id my_project . get_dataview ( id = \"dataview_123\" ) # get by uri my_project . get_dataview ( uri = \"s3://my-data.csv\" ) Default response: DataView ( id = dataview_123, name = my-data, location = s3://my-data.csv )","title":"Get a data view"},{"location":"libraries/pycape/usage/dataview/#add-a-data-view-to-a-project","text":"Initialize a DataView class and pass the instance to the create_dataview method. my_project = c . get_project ( id = \"project_123\" ) my_project . create_dataview ( name = \"my-data\" , uri = \"s3://my-data.csv\" , owner_label = \"my-org\" ) Default response: DataView ( id = dataview_123, name = my-data, location = s3://my-data.csv )","title":"Add a data view to a project"},{"location":"libraries/pycape/usage/dataview/#dataviews-and-schemas","text":"DataView schemas allow you to clarify the data types of your dataset. They will be visible for other project contributors - even ones from other organizations - to your project to query and inspect. By inspecting the schema property, other project contributors are able to identify which data columns should be used to train the model. If you provide a dataset to Cape that is accessible via HTTP or S3, Cape will download your data's column headers and create a schema.","title":"DataViews and Schemas"},{"location":"libraries/pycape/usage/dataview/#providing-s3-read-access-to-your-dataview","text":"In order to make your dataset accessible in S3 you'll need to inform pycape about your S3 bucket's IAM authentication credentials. Cape expects values for the following AWS configuration keys: AWS_ACCESS_KEY_ID , AWS_SECRET_ACCESS_KEY , and AWS_REGION . You can set these keys as environment variables in the interpreter running pycape : export AWS_ACCESS_KEY_ID = <Access-Key> export AWS_SECRET_ACCESS_KEY = <Secret-Key> export AWS_REGION = <Region> Alternatively you can simply add these keys to your AWS Configuration file .","title":"Providing S3 read access to your DataView"},{"location":"libraries/pycape/usage/dataview/#specifying-a-schema-for-your-dataview","text":"However, if your dataset is not accessible you'll have to specify your data's schema yourself. You can do so using the schema parameter. DataViews can be instantiated with a pandas Series schema of type dataframe.dtypes : >>> import pandas as pd >>> df = pd . DataFrame ( data = { \"col_1\" : [ 1 , 2 ], \"col_2\" : [ 3 , 4 ]}) >>> dataview = my_project . create_dataview ( name = \"my-data\" , uri = \"s3://my-data.csv\" , owner_label = \"my-org\" , schema = df . dtypes ) >>> dataview . schema { 'col_1' : 'integer' , 'col_2' : 'integer' } DataViews can also be instantiated as a list of data types. Accepted schema data types include: string , integer , number , datetime , and any . >>> schema = [{ \"name\" : \"col_1\" , \"schema_type\" : \"integer\" }, { \"name\" : \"col_2\" , \"schema_type\" : \"integer\" }] >>> data_view = DataView ( name = \"my-data\" , uri = \"s3://my-data.csv\" , schema = schema ) >>> dataview . schema { 'col_1' : 'integer' , 'col_2' : 'integer' }","title":"Specifying a Schema for your DataView"},{"location":"libraries/pycape/usage/dataview/#delete-a-dataview","text":"my_project = c . get_project ( id = \"project_123\" ) my_project . delete_dataview ( id = \"dataview_123\" ) Default response: DataView ( dataview_123 ) deleted","title":"Delete a DataView"},{"location":"libraries/pycape/usage/job/","text":"Managing Jobs # Submit a job # my_project = c . get_project ( id = \"project_123\" ) dataview_1 = my_project . get_dataview ( uri = \"s3://my-data.csv\" ) dataview_2 = my_project . get_dataview ( uri = \"s3://my-data-2.csv\" ) vlr = VerticallyPartitionedLinearRegression ( x_train_dataview = dataview_1 , y_train_dataview = dataview_2 , model_location = \"s3://my-bucket\" model_owner = \"org_123\" , ) my_project . submit_job ( vlr ) Default response: Job ( id = abc_123, job_type = LINEAR_REGRESSION, status = Created ) Setting the Storage Location as a Model Owner in Cape # The results of the trained model will be saved to an S3 bucket location that you notify Cape about. This can be done in two ways: Pass the S3 Bucket URI via the model_location parameter on the VerticallyPartitionedLinearRegression instance that is used to submit the job. Pass the S3 Bucket URI via an environment variable to your Cape Worker Docker image . This will overwrite the model_location set by pycape . Get a Job's Status # lr_job = my_project . get_job ( id = \"abc_123\" ) lr_job . get_status () Default response: Created Get a Job's Results # lr_job = my_project . get_job ( id = \"abc_123\" ) weights , metrics = lr_job . get_results () Default response: ( array ([ 12 .14955139, 1 .96560669 ]) , { 'r_squared_result' : [ 0 .8804865768463074 ] , 'mse_result' : [ 37 .94773864746094 ]}) Accessing Weights as a Model Owner in Cape # pycape uses boto to access the model weights in your S3 bucket. You'll need to inform pycape about your IAM authentication credentials. Cape expects values for the following AWS configuration keys: AWS_ACCESS_KEY_ID , AWS_SECRET_ACCESS_KEY , and AWS_REGION . You can set these keys as environment variables in the interpreter running pycape : export AWS_ACCESS_KEY_ID = <Access-Key> export AWS_SECRET_ACCESS_KEY = <Secret-Key> export AWS_REGION = <Region> Alternatively you can simply add these keys to your AWS Configuration file .","title":"Jobs"},{"location":"libraries/pycape/usage/job/#managing-jobs","text":"","title":"Managing Jobs"},{"location":"libraries/pycape/usage/job/#submit-a-job","text":"my_project = c . get_project ( id = \"project_123\" ) dataview_1 = my_project . get_dataview ( uri = \"s3://my-data.csv\" ) dataview_2 = my_project . get_dataview ( uri = \"s3://my-data-2.csv\" ) vlr = VerticallyPartitionedLinearRegression ( x_train_dataview = dataview_1 , y_train_dataview = dataview_2 , model_location = \"s3://my-bucket\" model_owner = \"org_123\" , ) my_project . submit_job ( vlr ) Default response: Job ( id = abc_123, job_type = LINEAR_REGRESSION, status = Created )","title":"Submit a job"},{"location":"libraries/pycape/usage/job/#setting-the-storage-location-as-a-model-owner-in-cape","text":"The results of the trained model will be saved to an S3 bucket location that you notify Cape about. This can be done in two ways: Pass the S3 Bucket URI via the model_location parameter on the VerticallyPartitionedLinearRegression instance that is used to submit the job. Pass the S3 Bucket URI via an environment variable to your Cape Worker Docker image . This will overwrite the model_location set by pycape .","title":"Setting the Storage Location as a Model Owner in Cape"},{"location":"libraries/pycape/usage/job/#get-a-jobs-status","text":"lr_job = my_project . get_job ( id = \"abc_123\" ) lr_job . get_status () Default response: Created","title":"Get a Job's Status"},{"location":"libraries/pycape/usage/job/#get-a-jobs-results","text":"lr_job = my_project . get_job ( id = \"abc_123\" ) weights , metrics = lr_job . get_results () Default response: ( array ([ 12 .14955139, 1 .96560669 ]) , { 'r_squared_result' : [ 0 .8804865768463074 ] , 'mse_result' : [ 37 .94773864746094 ]})","title":"Get a Job's Results"},{"location":"libraries/pycape/usage/job/#accessing-weights-as-a-model-owner-in-cape","text":"pycape uses boto to access the model weights in your S3 bucket. You'll need to inform pycape about your IAM authentication credentials. Cape expects values for the following AWS configuration keys: AWS_ACCESS_KEY_ID , AWS_SECRET_ACCESS_KEY , and AWS_REGION . You can set these keys as environment variables in the interpreter running pycape : export AWS_ACCESS_KEY_ID = <Access-Key> export AWS_SECRET_ACCESS_KEY = <Secret-Key> export AWS_REGION = <Region> Alternatively you can simply add these keys to your AWS Configuration file .","title":"Accessing Weights as a Model Owner in Cape"},{"location":"libraries/pycape/usage/login/","text":"Login to PyCape # To use pycape , you'll need to generate a user token . You'll use this token to authenticate requests to Cape Cloud. After setting up an account in Cape , ensure you are working within your user context and navigate to Account Settings to generate a user token. Take note of this value as you cannot recover it after you reload the page. c = Cape () c . login ( token = \"abc,123\" , endpoint = \"http://cape.com\" ) It is also possible to set your Auth Token and Coordinator endpoint via the environment variables CAPE_TOKEN and CAPE_COORDINATOR . # Call the login method after exporting CAPE_TOKEN and CAPE_COORDINATOR. c = Cape () c . login () Default response: Login successful","title":"Login"},{"location":"libraries/pycape/usage/login/#login-to-pycape","text":"To use pycape , you'll need to generate a user token . You'll use this token to authenticate requests to Cape Cloud. After setting up an account in Cape , ensure you are working within your user context and navigate to Account Settings to generate a user token. Take note of this value as you cannot recover it after you reload the page. c = Cape () c . login ( token = \"abc,123\" , endpoint = \"http://cape.com\" ) It is also possible to set your Auth Token and Coordinator endpoint via the environment variables CAPE_TOKEN and CAPE_COORDINATOR . # Call the login method after exporting CAPE_TOKEN and CAPE_COORDINATOR. c = Cape () c . login () Default response: Login successful","title":"Login to PyCape"},{"location":"libraries/pycape/usage/project/","text":"Manage Projects # Using pycape you can create, delete, or query for your Cape projects. List Projects # c . list_projects () Default response: PROJECT ID NAME LABEL ----------- ------------------ ------------------ project_123 Sales Transactions sales-transactions Get Project # # Get project by ID c . get_project ( id = \"project_123\" ) # Get project by label c . get_project ( label = \"my-project\" ) Default response: Project ( id = project_123, name = My Project, label = my-project ) Create a Project # c . create_project ( name = \"My Project\" owner = \"org_123\" description = \"Linear Regression model with amazorn.\" ) Default response: Project ( id = project_123, name = My Project, label = my-project ) Delete a Project # c . delete_project ( id = \"project_123\" ) Default response: Project ( project_123 ) deleted","title":"Projects"},{"location":"libraries/pycape/usage/project/#manage-projects","text":"Using pycape you can create, delete, or query for your Cape projects.","title":"Manage Projects"},{"location":"libraries/pycape/usage/project/#list-projects","text":"c . list_projects () Default response: PROJECT ID NAME LABEL ----------- ------------------ ------------------ project_123 Sales Transactions sales-transactions","title":"List Projects"},{"location":"libraries/pycape/usage/project/#get-project","text":"# Get project by ID c . get_project ( id = \"project_123\" ) # Get project by label c . get_project ( label = \"my-project\" ) Default response: Project ( id = project_123, name = My Project, label = my-project )","title":"Get Project"},{"location":"libraries/pycape/usage/project/#create-a-project","text":"c . create_project ( name = \"My Project\" owner = \"org_123\" description = \"Linear Regression model with amazorn.\" ) Default response: Project ( id = project_123, name = My Project, label = my-project )","title":"Create a Project"},{"location":"libraries/pycape/usage/project/#delete-a-project","text":"c . delete_project ( id = \"project_123\" ) Default response: Project ( project_123 ) deleted","title":"Delete a Project"},{"location":"pythonv1/readme/","text":"Cape Python # A Python library supporting data transformations and collaborative privacy policies, for data science projects in Pandas and Apache Spark See below for instructions on how to get started or visit the documentation . Getting started # Prerequisites # Python 3.6 or above, and pip Pandas 1.0+ PySpark 3.0+ (if using Spark) Make (if installing from source) Install with pip # Cape Python is available through PyPi. pip install cape-privacy Support for Apache Spark is optional. If you plan on using the library together with Apache Spark, we suggest the following instead: pip install cape-privacy [ spark ] We recommend running it in a virtual environment, such as venv . Install from source # It is possible to install the library from source. This installs all dependencies, including Apache Spark: git clone https://github.com/capeprivacy/cape-python.git cd cape-python make bootstrap Usage example # This example is an abridged version of the tutorial found here df = pd . DataFrame ({ \"name\" : [ \"alice\" , \"bob\" ], \"age\" : [ 34 , 55 ], \"birthdate\" : [ pd . Timestamp ( 1985 , 2 , 23 ), pd . Timestamp ( 1963 , 5 , 10 )], }) tokenize = Tokenizer ( max_token_len = 10 , key = b \"my secret\" ) perturb_numeric = NumericPerturbation ( dtype = dtypes . Integer , min =- 10 , max = 10 ) df [ \"name\" ] = tokenize ( df [ \"name\" ]) df [ \"age\" ] = perturb_numeric ( df [ \"age\" ]) print ( df . head ()) # >> # name age birthdate # 0 f42c2f1964 34 1985-02-23 # 1 2e586494b2 63 1963-05-10 These steps can be saved in policy files so you can share them and collaborate with your team: # my-policy.yaml label : my-policy version : 1 rules : - match : name : age actions : - transform : type : numeric-perturbation dtype : Integer min : -10 max : 10 seed : 4984 - match : name : name actions : - transform : type : tokenizer max_token_len : 10 key : my secret You can then load this policy and apply it to your data frame: # df can be a Pandas or Spark data frame policy = cape . parse_policy ( \"my-policy.yaml\" ) df = cape . apply_policy ( policy , df ) print ( df . head ()) # >> # name age birthdate # 0 f42c2f1964 34 1985-02-23 # 1 2e586494b2 63 1963-05-10 You can see more examples and usage here or in our documentation . About Cape Privacy and Cape Python # Cape Privacy helps teams share data and make decisions for safer and more powerful data science. Learn more at capeprivacy.com . Cape Python brings Cape's policy language to Pandas and Apache Spark. The supported techniques include tokenization with linkability as well as perturbation and rounding. You can experiment with these techniques programmatically, in Python or in human-readable policy files. Cape architecture # Cape is comprised of multiples services and libraries. You can use Cape Python as a standalone library, or you can integrate it with the Coordinator in Cape Core , which supports user and policy management. Project status and roadmap # Cape Python 0.1.1 was released 24th June 2020. It is actively maintained and developed, alongside other elements of the Cape ecosystem. Upcoming features: Reversible tokenisation: allow reversing of tokenization to reveal the raw value. Policy audit logging: create logging hooks to allow audit logs for policy downloads and usage in Cape Python. Expand pipeline integrations: add Apache Beam, Apache Flink, Apache Arrow Flight or Dask integration as another pipeline we can support, either as part of Cape Python or in its own separate project. The goal is a complete data management ecosystem. Cape Privacy provides Cape Coordinator , to manage policy and users. This will interact with the Cape Privacy libraries (such as Cape Python ) through a workers interface, and with your own data services through an API. Help and resources # If you need help using Cape Python, you can: View the documentation . Submit an issue. Talk to us on our community Slack . Please file feature requests and bug reports as GitHub issues. Community # Contributing # View our contributing guide for more information. Code of conduct # Our code of conduct is included on the Cape Privacy website. All community members are expected to follow it. Please refer to that page for information on how to report problems. License # Licensed under Apache License, Version 2.0 (see LICENSE or http://www.apache.org/licenses/LICENSE-2.0). Copyright as specified in NOTICE .","title":"README"},{"location":"pythonv1/readme/#cape-python","text":"A Python library supporting data transformations and collaborative privacy policies, for data science projects in Pandas and Apache Spark See below for instructions on how to get started or visit the documentation .","title":"Cape Python"},{"location":"pythonv1/readme/#getting-started","text":"","title":"Getting started"},{"location":"pythonv1/readme/#prerequisites","text":"Python 3.6 or above, and pip Pandas 1.0+ PySpark 3.0+ (if using Spark) Make (if installing from source)","title":"Prerequisites"},{"location":"pythonv1/readme/#install-with-pip","text":"Cape Python is available through PyPi. pip install cape-privacy Support for Apache Spark is optional. If you plan on using the library together with Apache Spark, we suggest the following instead: pip install cape-privacy [ spark ] We recommend running it in a virtual environment, such as venv .","title":"Install with pip"},{"location":"pythonv1/readme/#install-from-source","text":"It is possible to install the library from source. This installs all dependencies, including Apache Spark: git clone https://github.com/capeprivacy/cape-python.git cd cape-python make bootstrap","title":"Install from source"},{"location":"pythonv1/readme/#usage-example","text":"This example is an abridged version of the tutorial found here df = pd . DataFrame ({ \"name\" : [ \"alice\" , \"bob\" ], \"age\" : [ 34 , 55 ], \"birthdate\" : [ pd . Timestamp ( 1985 , 2 , 23 ), pd . Timestamp ( 1963 , 5 , 10 )], }) tokenize = Tokenizer ( max_token_len = 10 , key = b \"my secret\" ) perturb_numeric = NumericPerturbation ( dtype = dtypes . Integer , min =- 10 , max = 10 ) df [ \"name\" ] = tokenize ( df [ \"name\" ]) df [ \"age\" ] = perturb_numeric ( df [ \"age\" ]) print ( df . head ()) # >> # name age birthdate # 0 f42c2f1964 34 1985-02-23 # 1 2e586494b2 63 1963-05-10 These steps can be saved in policy files so you can share them and collaborate with your team: # my-policy.yaml label : my-policy version : 1 rules : - match : name : age actions : - transform : type : numeric-perturbation dtype : Integer min : -10 max : 10 seed : 4984 - match : name : name actions : - transform : type : tokenizer max_token_len : 10 key : my secret You can then load this policy and apply it to your data frame: # df can be a Pandas or Spark data frame policy = cape . parse_policy ( \"my-policy.yaml\" ) df = cape . apply_policy ( policy , df ) print ( df . head ()) # >> # name age birthdate # 0 f42c2f1964 34 1985-02-23 # 1 2e586494b2 63 1963-05-10 You can see more examples and usage here or in our documentation .","title":"Usage example"},{"location":"pythonv1/readme/#about-cape-privacy-and-cape-python","text":"Cape Privacy helps teams share data and make decisions for safer and more powerful data science. Learn more at capeprivacy.com . Cape Python brings Cape's policy language to Pandas and Apache Spark. The supported techniques include tokenization with linkability as well as perturbation and rounding. You can experiment with these techniques programmatically, in Python or in human-readable policy files.","title":"About Cape Privacy and Cape Python"},{"location":"pythonv1/readme/#cape-architecture","text":"Cape is comprised of multiples services and libraries. You can use Cape Python as a standalone library, or you can integrate it with the Coordinator in Cape Core , which supports user and policy management.","title":"Cape architecture"},{"location":"pythonv1/readme/#project-status-and-roadmap","text":"Cape Python 0.1.1 was released 24th June 2020. It is actively maintained and developed, alongside other elements of the Cape ecosystem. Upcoming features: Reversible tokenisation: allow reversing of tokenization to reveal the raw value. Policy audit logging: create logging hooks to allow audit logs for policy downloads and usage in Cape Python. Expand pipeline integrations: add Apache Beam, Apache Flink, Apache Arrow Flight or Dask integration as another pipeline we can support, either as part of Cape Python or in its own separate project. The goal is a complete data management ecosystem. Cape Privacy provides Cape Coordinator , to manage policy and users. This will interact with the Cape Privacy libraries (such as Cape Python ) through a workers interface, and with your own data services through an API.","title":"Project status and roadmap"},{"location":"pythonv1/readme/#help-and-resources","text":"If you need help using Cape Python, you can: View the documentation . Submit an issue. Talk to us on our community Slack . Please file feature requests and bug reports as GitHub issues.","title":"Help and resources"},{"location":"pythonv1/readme/#community","text":"","title":"Community"},{"location":"pythonv1/readme/#contributing","text":"View our contributing guide for more information.","title":"Contributing"},{"location":"pythonv1/readme/#code-of-conduct","text":"Our code of conduct is included on the Cape Privacy website. All community members are expected to follow it. Please refer to that page for information on how to report problems.","title":"Code of conduct"},{"location":"pythonv1/readme/#license","text":"Licensed under Apache License, Version 2.0 (see LICENSE or http://www.apache.org/licenses/LICENSE-2.0). Copyright as specified in NOTICE .","title":"License"},{"location":"pythonv1/cape_privacy.pandas/dtypes/","text":"cape_privacy.pandas.dtypes #","title":"dtypes"},{"location":"pythonv1/cape_privacy.pandas/dtypes/#cape_privacypandasdtypes","text":"","title":"cape_privacy.pandas.dtypes"},{"location":"pythonv1/cape_privacy.pandas/registry/","text":"cape_privacy.pandas.registry # get # get ( transformation : str ) -> TransformationCtor Returns the constructor for the given key. Arguments : transformation - The key of transformation to retrieve. register # register ( label : str , ctor : TransformationCtor ) Registers a new transformation constructor under the label provided. Arguments : label - The label that will be used as the key in the registry ctor - The transformation constructor","title":"registry"},{"location":"pythonv1/cape_privacy.pandas/registry/#cape_privacypandasregistry","text":"","title":"cape_privacy.pandas.registry"},{"location":"pythonv1/cape_privacy.pandas/registry/#get","text":"get ( transformation : str ) -> TransformationCtor Returns the constructor for the given key. Arguments : transformation - The key of transformation to retrieve.","title":"get"},{"location":"pythonv1/cape_privacy.pandas/registry/#register","text":"register ( label : str , ctor : TransformationCtor ) Registers a new transformation constructor under the label provided. Arguments : label - The label that will be used as the key in the registry ctor - The transformation constructor","title":"register"},{"location":"pythonv1/cape_privacy.pandas/transformations/column_redact/","text":"cape_privacy.pandas.transformations.column_redact # ColumnRedact Objects # class ColumnRedact () Redacts columns. Attributes : columns - The columns to redact.","title":"column_redact"},{"location":"pythonv1/cape_privacy.pandas/transformations/column_redact/#cape_privacypandastransformationscolumn_redact","text":"","title":"cape_privacy.pandas.transformations.column_redact"},{"location":"pythonv1/cape_privacy.pandas/transformations/column_redact/#columnredact-objects","text":"class ColumnRedact () Redacts columns. Attributes : columns - The columns to redact.","title":"ColumnRedact Objects"},{"location":"pythonv1/cape_privacy.pandas/transformations/perturbation/","text":"cape_privacy.pandas.transformations.perturbation # NumericPerturbation Objects # class NumericPerturbation ( base . Transformation ) Add uniform random noise to a numeric Pandas series Mask a numeric Pandas series by adding uniform random noise to each value. The amount of noise is drawn from the interval [min, max). Example : s = pd.Series([0, 1, 2, 3, 4]) perturb = NumericPerturbation(dtype=Integer, min=-10, max=10, seed=123) perturb(s) # pd.Series([3, -7, -3, -3]) Attributes : dtype dtypes.Numerics - Pandas Series type min int, float - the values generated will be greater then or equal to min max int, float - the values generated will be less than max seed (int), optional: a seed to initialize the random generator DatePerturbation Objects # class DatePerturbation ( base . Transformation ) Add uniform random noise to a Pandas series of timestamps Mask a Pandas series by adding uniform random noise to the specified frequencies of timestamps. The amount of noise for each frequency is drawn from the internal [min_freq, max_freq). Example : s = pd.Series([datetime.date(year=2020, month=2, day=15)]) perturb = DatePerturbation(frequency=\"MONTH\", min=-10, max=10, seed=1234) perturb(s) # pd.Series([datetime.date(year=2020, month=11, day=11)]) Attributes : frequency str, str list - one or more frequencies to perturbate min int, int list - the frequency value will be greater or equal to min max int, int list - the frequency value will be less than max seed (int), optional: a seed to initialize the random generator","title":"perturbation"},{"location":"pythonv1/cape_privacy.pandas/transformations/perturbation/#cape_privacypandastransformationsperturbation","text":"","title":"cape_privacy.pandas.transformations.perturbation"},{"location":"pythonv1/cape_privacy.pandas/transformations/perturbation/#numericperturbation-objects","text":"class NumericPerturbation ( base . Transformation ) Add uniform random noise to a numeric Pandas series Mask a numeric Pandas series by adding uniform random noise to each value. The amount of noise is drawn from the interval [min, max). Example : s = pd.Series([0, 1, 2, 3, 4]) perturb = NumericPerturbation(dtype=Integer, min=-10, max=10, seed=123) perturb(s) # pd.Series([3, -7, -3, -3]) Attributes : dtype dtypes.Numerics - Pandas Series type min int, float - the values generated will be greater then or equal to min max int, float - the values generated will be less than max seed (int), optional: a seed to initialize the random generator","title":"NumericPerturbation Objects"},{"location":"pythonv1/cape_privacy.pandas/transformations/perturbation/#dateperturbation-objects","text":"class DatePerturbation ( base . Transformation ) Add uniform random noise to a Pandas series of timestamps Mask a Pandas series by adding uniform random noise to the specified frequencies of timestamps. The amount of noise for each frequency is drawn from the internal [min_freq, max_freq). Example : s = pd.Series([datetime.date(year=2020, month=2, day=15)]) perturb = DatePerturbation(frequency=\"MONTH\", min=-10, max=10, seed=1234) perturb(s) # pd.Series([datetime.date(year=2020, month=11, day=11)]) Attributes : frequency str, str list - one or more frequencies to perturbate min int, int list - the frequency value will be greater or equal to min max int, int list - the frequency value will be less than max seed (int), optional: a seed to initialize the random generator","title":"DatePerturbation Objects"},{"location":"pythonv1/cape_privacy.pandas/transformations/rounding/","text":"cape_privacy.pandas.transformations.rounding # NumericRounding Objects # class NumericRounding ( base . Transformation ) Reduce the precision of a numeric Pandas Series Round each value in the Pandas Series to the given number of digits. Example : s = pd.Series([1.384]) round = NumericRounding(precision=1) round(s) # pd.Series([1.4]) Attributes : dtypes dtypes.Numerics - Pandas Series type. precision int - set the number of digits. __call__ # | __call__ ( x : pd . Series ) -> pd . Series Round each value in the Pandas Series Arguments : x A Pandas Series - need to be a list of numeric values. Returns : A Pandas Series with each value rounded DateTruncation Objects # class DateTruncation ( base . Transformation ) Reduce the precision of a date Pandas Series Truncate each date in a Pandas Series to the unit (year or month) specified by frequency. Example : s = pd.Series([pd.Timestamp(\"2018-10-15\")]) trunc = DateTruncation(frequency=\"year\") trunc(s) # pd.Serie([pd.Timestamp(\"2018-01-01\")]) Attributes : frequency string - expect to be 'year' or 'month'","title":"rounding"},{"location":"pythonv1/cape_privacy.pandas/transformations/rounding/#cape_privacypandastransformationsrounding","text":"","title":"cape_privacy.pandas.transformations.rounding"},{"location":"pythonv1/cape_privacy.pandas/transformations/rounding/#numericrounding-objects","text":"class NumericRounding ( base . Transformation ) Reduce the precision of a numeric Pandas Series Round each value in the Pandas Series to the given number of digits. Example : s = pd.Series([1.384]) round = NumericRounding(precision=1) round(s) # pd.Series([1.4]) Attributes : dtypes dtypes.Numerics - Pandas Series type. precision int - set the number of digits.","title":"NumericRounding Objects"},{"location":"pythonv1/cape_privacy.pandas/transformations/rounding/#__call__","text":"| __call__ ( x : pd . Series ) -> pd . Series Round each value in the Pandas Series Arguments : x A Pandas Series - need to be a list of numeric values. Returns : A Pandas Series with each value rounded","title":"__call__"},{"location":"pythonv1/cape_privacy.pandas/transformations/rounding/#datetruncation-objects","text":"class DateTruncation ( base . Transformation ) Reduce the precision of a date Pandas Series Truncate each date in a Pandas Series to the unit (year or month) specified by frequency. Example : s = pd.Series([pd.Timestamp(\"2018-10-15\")]) trunc = DateTruncation(frequency=\"year\") trunc(s) # pd.Serie([pd.Timestamp(\"2018-01-01\")]) Attributes : frequency string - expect to be 'year' or 'month'","title":"DateTruncation Objects"},{"location":"pythonv1/cape_privacy.pandas/transformations/row_redact/","text":"cape_privacy.pandas.transformations.row_redact # RowRedact Objects # class RowRedact () Redacts rows based on the condition. Attributes : condition - The condition to be passed into the query function. __call__ # | __call__ ( df : pd . DataFrame ) -> pd . DataFrame Redacts rows using Dataframe.query. DataFrame.query returns all the fields that it matches so we negate it here to get the opposite.","title":"row_redact"},{"location":"pythonv1/cape_privacy.pandas/transformations/row_redact/#cape_privacypandastransformationsrow_redact","text":"","title":"cape_privacy.pandas.transformations.row_redact"},{"location":"pythonv1/cape_privacy.pandas/transformations/row_redact/#rowredact-objects","text":"class RowRedact () Redacts rows based on the condition. Attributes : condition - The condition to be passed into the query function.","title":"RowRedact Objects"},{"location":"pythonv1/cape_privacy.pandas/transformations/row_redact/#__call__","text":"| __call__ ( df : pd . DataFrame ) -> pd . DataFrame Redacts rows using Dataframe.query. DataFrame.query returns all the fields that it matches so we negate it here to get the opposite.","title":"__call__"},{"location":"pythonv1/cape_privacy.pandas/transformations/tokenizer/","text":"cape_privacy.pandas.transformations.tokenizer # Tokenizer Objects # class Tokenizer ( base . Transformation ) Tokenizer: map a string to a token to obfuscate it. When applying the Tokenizer to a Pandas Series of type string, each value gets mapped to a token (hexadecimal string). If a value is repeated several times across the series, it always get mapped to the same token in order to maintain the count. A value can be mapped to different tokens by setting the key to a different value. Example : s = pd.Series(['A']) tokenize = Tokenizer(max_token_len=5, key='secret') tokenize(s) # pd.Series(['40a1e']) Attributes : max_token_len int or bytes - control the token length (default length is 64) key - expect a string or byte string. If not specified, key will be set to a random byte string. __call__ # | __call__ ( series : pd . Series ) -> pd . Series Map a Pandas Series to tokens. Arguments : series A Pandas Series - need to be a list of strings. Returns : A Pandas Series with a list of tokens represented as hexadecimal strings. ReversibleTokenizer Objects # class ReversibleTokenizer ( base . Transformation ) ReversibleTokenizer: map a string to a token to obfuscate it. When applying the Tokenizer to a Pandas Series of type string, each value gets mapped to a token (hexadecimal string). If a value is repeated several times across the series, it always get mapped to the same token in order to maintain the count. A value can be mapped to different tokens by setting the key to a different value. This tokenizer allows tokens to be reversed to their original data when the secret key is known. Example : s = pd.Series(['A']) tokenize = ReversibleTokenizer(key='secret') tokenize(s) # pd.Series(['40a1e']) Attributes : key - expect a string or byte string of length exactly 32 bytes. encoding - string identifying the Python encoding used for inputs. __call__ # | __call__ ( series : pd . Series ) -> pd . Series Map a Pandas Series to tokens. Arguments : series A Pandas Series - need to be a list of strings. Returns : A Pandas Series with a list of tokens represented as hexadecimal strings. TokenReverser Objects # class TokenReverser ( base . Transformation ) TokenReverser: recover string from token. When applying the TokenReverser to a Pandas Series of tokens, each token is mapped back to the string that was originally used by ReversibleTokenizer to construct the token. The same key must be used. Example : s = pd.Series(['40a1e']) reverser = TokenReverser(key='secret') reverser(s) # pd.Series(['A']) Attributes : key - expect a string or byte string of length exactly 32 bytes. encoding - string identifying the Python encoding used for outputs. __call__ # | __call__ ( series : pd . Series ) -> pd . Series Reverse a Pandas Series of tokens. Arguments : series A Pandas Series - need to be a list of strings. Returns : A Pandas Series with a list of recovered strings.","title":"tokenizer"},{"location":"pythonv1/cape_privacy.pandas/transformations/tokenizer/#cape_privacypandastransformationstokenizer","text":"","title":"cape_privacy.pandas.transformations.tokenizer"},{"location":"pythonv1/cape_privacy.pandas/transformations/tokenizer/#tokenizer-objects","text":"class Tokenizer ( base . Transformation ) Tokenizer: map a string to a token to obfuscate it. When applying the Tokenizer to a Pandas Series of type string, each value gets mapped to a token (hexadecimal string). If a value is repeated several times across the series, it always get mapped to the same token in order to maintain the count. A value can be mapped to different tokens by setting the key to a different value. Example : s = pd.Series(['A']) tokenize = Tokenizer(max_token_len=5, key='secret') tokenize(s) # pd.Series(['40a1e']) Attributes : max_token_len int or bytes - control the token length (default length is 64) key - expect a string or byte string. If not specified, key will be set to a random byte string.","title":"Tokenizer Objects"},{"location":"pythonv1/cape_privacy.pandas/transformations/tokenizer/#__call__","text":"| __call__ ( series : pd . Series ) -> pd . Series Map a Pandas Series to tokens. Arguments : series A Pandas Series - need to be a list of strings. Returns : A Pandas Series with a list of tokens represented as hexadecimal strings.","title":"__call__"},{"location":"pythonv1/cape_privacy.pandas/transformations/tokenizer/#reversibletokenizer-objects","text":"class ReversibleTokenizer ( base . Transformation ) ReversibleTokenizer: map a string to a token to obfuscate it. When applying the Tokenizer to a Pandas Series of type string, each value gets mapped to a token (hexadecimal string). If a value is repeated several times across the series, it always get mapped to the same token in order to maintain the count. A value can be mapped to different tokens by setting the key to a different value. This tokenizer allows tokens to be reversed to their original data when the secret key is known. Example : s = pd.Series(['A']) tokenize = ReversibleTokenizer(key='secret') tokenize(s) # pd.Series(['40a1e']) Attributes : key - expect a string or byte string of length exactly 32 bytes. encoding - string identifying the Python encoding used for inputs.","title":"ReversibleTokenizer Objects"},{"location":"pythonv1/cape_privacy.pandas/transformations/tokenizer/#__call___1","text":"| __call__ ( series : pd . Series ) -> pd . Series Map a Pandas Series to tokens. Arguments : series A Pandas Series - need to be a list of strings. Returns : A Pandas Series with a list of tokens represented as hexadecimal strings.","title":"__call__"},{"location":"pythonv1/cape_privacy.pandas/transformations/tokenizer/#tokenreverser-objects","text":"class TokenReverser ( base . Transformation ) TokenReverser: recover string from token. When applying the TokenReverser to a Pandas Series of tokens, each token is mapped back to the string that was originally used by ReversibleTokenizer to construct the token. The same key must be used. Example : s = pd.Series(['40a1e']) reverser = TokenReverser(key='secret') reverser(s) # pd.Series(['A']) Attributes : key - expect a string or byte string of length exactly 32 bytes. encoding - string identifying the Python encoding used for outputs.","title":"TokenReverser Objects"},{"location":"pythonv1/cape_privacy.pandas/transformations/tokenizer/#__call___2","text":"| __call__ ( series : pd . Series ) -> pd . Series Reverse a Pandas Series of tokens. Arguments : series A Pandas Series - need to be a list of strings. Returns : A Pandas Series with a list of recovered strings.","title":"__call__"},{"location":"pythonv1/cape_privacy.policy/data/","text":"cape_privacy.policy.data # Contains the policy classes that are initialized from a yaml policy file. There are five main classes with Policy being the top level class. Policy contains the PolicySpec and NamedTransformations. PolicySpec contains Rules and Rules contain Transformations. Typical usage example: yaml_str = \"....\" d = yaml.load(yaml_str, Loader=yaml.FullLoad) # **d unpacks the dictionary produced by yaml and # passes them in has keyword arguments. policy = Policy(**d) Transform Objects # class Transform () A actual transform that will be applied. Either named or function must be passed in here. The process to apply this transform will look at both function and named and apply the relevant one. Attributes : field - The field this transform will be applied to. name - The name of the named transform, referenced from the top level policy object. type - The builtin transform that will be initialized. kwargs - The rest of the arguments that will be passed to the transformation. Rule Objects # class Rule () A rule contains actionable information of a policy. Attributes : match - The match used to select a field to be transformed. actions - The actions to take on a matched field. NamedTransform Objects # class NamedTransform () A named transformation that captures the args. Attributes : name - The name of the named transformation. type - The builtin type (i.e. transform) that the named transform initializes to. kwargs - The args that are captured by the named transform. Policy Objects # class Policy () Top level policy object. The top level policy object holds the all of the relevant information for applying policy to data. Attributes : label - The label of the policy. version - The version of the policy. rules - List of rules that will be applied to a data frame. transformations - The named transformations for this policy.","title":"data"},{"location":"pythonv1/cape_privacy.policy/data/#cape_privacypolicydata","text":"Contains the policy classes that are initialized from a yaml policy file. There are five main classes with Policy being the top level class. Policy contains the PolicySpec and NamedTransformations. PolicySpec contains Rules and Rules contain Transformations. Typical usage example: yaml_str = \"....\" d = yaml.load(yaml_str, Loader=yaml.FullLoad) # **d unpacks the dictionary produced by yaml and # passes them in has keyword arguments. policy = Policy(**d)","title":"cape_privacy.policy.data"},{"location":"pythonv1/cape_privacy.policy/data/#transform-objects","text":"class Transform () A actual transform that will be applied. Either named or function must be passed in here. The process to apply this transform will look at both function and named and apply the relevant one. Attributes : field - The field this transform will be applied to. name - The name of the named transform, referenced from the top level policy object. type - The builtin transform that will be initialized. kwargs - The rest of the arguments that will be passed to the transformation.","title":"Transform Objects"},{"location":"pythonv1/cape_privacy.policy/data/#rule-objects","text":"class Rule () A rule contains actionable information of a policy. Attributes : match - The match used to select a field to be transformed. actions - The actions to take on a matched field.","title":"Rule Objects"},{"location":"pythonv1/cape_privacy.policy/data/#namedtransform-objects","text":"class NamedTransform () A named transformation that captures the args. Attributes : name - The name of the named transformation. type - The builtin type (i.e. transform) that the named transform initializes to. kwargs - The args that are captured by the named transform.","title":"NamedTransform Objects"},{"location":"pythonv1/cape_privacy.policy/data/#policy-objects","text":"class Policy () Top level policy object. The top level policy object holds the all of the relevant information for applying policy to data. Attributes : label - The label of the policy. version - The version of the policy. rules - List of rules that will be applied to a data frame. transformations - The named transformations for this policy.","title":"Policy Objects"},{"location":"pythonv1/cape_privacy.policy/policy/","text":"cape_privacy.policy.policy # Utils for parsing policy and applying them. The module reads in policy as yaml and then through apply_policy applies them to dataframes. Example policy yaml: label: test_policy version: 1 rules: - match: name: value actions: # Tells the policy runner to apply the transformation # plusN with the specified arguments. - transform: type: plusN n: 1 # Tells the policy runner to apply another plusN # transformation. - transform: type: plusN n: 2 Applying policy: policy = parse_policy(\"policy.yaml\") df = pd.DataFrame(np.ones(5,), columns=[\"value\"]) df = apply_policy(policy, df) apply_policy # apply_policy ( policy : data . Policy , df , inplace = False ) Applies a Policy to some DataFrame. This function is responsible for inferring the type of the DataFrame, preparing the relevant Spark or Pandas Transformations, and applying them to produce a transformed DataFrame that conforms to the Policy. Arguments : policy - The Policy object that the transformed DataFrame will conform to, e.g. as returned by cape_privacy.parse_policy . df - The DataFrame object to transform according to policies . Must be of type pandas.DataFrame or pyspark.sql.DataFrame. inplace - Whether to mutate the df or produce a new one. This argument is only relevant for Pandas DataFrames, as Spark DataFrames do not support mutation. Raises : ValueError - If df is a Spark DataFrame and inplace=True, or if df is something other than a Pandas or Spark DataFrame. DependencyError - If Spark is not configured correctly in the Python environment. TransformNotFound, NamedTransformNotFound: If the Policy contains a reference to a Transformation or NamedTransformation that is unrecognized in the Transformation registry. parse_policy # parse_policy ( p : Union [ str , Dict [ Any , Any ]]) -> data . Policy Parses a policy YAML file. The passed in string can either be a path to a local file, a URL pointing to a file or a dictionary representing the policy. If it is a URL then requests attempts to download it. Arguments : p - a path string, a URL string or a dictionary representing the policy. Returns : The Policy object initialized by the YAML. reverse # reverse ( policy : data . Policy ) -> data . Policy Turns reversible tokenizations into token reversers If any named transformations contain a reversible tokenization transformation this helper function turns them into token reverser transformations. Arguments : policy - Top level policy object. Returns : The modified policy.","title":"policy"},{"location":"pythonv1/cape_privacy.policy/policy/#cape_privacypolicypolicy","text":"Utils for parsing policy and applying them. The module reads in policy as yaml and then through apply_policy applies them to dataframes. Example policy yaml: label: test_policy version: 1 rules: - match: name: value actions: # Tells the policy runner to apply the transformation # plusN with the specified arguments. - transform: type: plusN n: 1 # Tells the policy runner to apply another plusN # transformation. - transform: type: plusN n: 2 Applying policy: policy = parse_policy(\"policy.yaml\") df = pd.DataFrame(np.ones(5,), columns=[\"value\"]) df = apply_policy(policy, df)","title":"cape_privacy.policy.policy"},{"location":"pythonv1/cape_privacy.policy/policy/#apply_policy","text":"apply_policy ( policy : data . Policy , df , inplace = False ) Applies a Policy to some DataFrame. This function is responsible for inferring the type of the DataFrame, preparing the relevant Spark or Pandas Transformations, and applying them to produce a transformed DataFrame that conforms to the Policy. Arguments : policy - The Policy object that the transformed DataFrame will conform to, e.g. as returned by cape_privacy.parse_policy . df - The DataFrame object to transform according to policies . Must be of type pandas.DataFrame or pyspark.sql.DataFrame. inplace - Whether to mutate the df or produce a new one. This argument is only relevant for Pandas DataFrames, as Spark DataFrames do not support mutation. Raises : ValueError - If df is a Spark DataFrame and inplace=True, or if df is something other than a Pandas or Spark DataFrame. DependencyError - If Spark is not configured correctly in the Python environment. TransformNotFound, NamedTransformNotFound: If the Policy contains a reference to a Transformation or NamedTransformation that is unrecognized in the Transformation registry.","title":"apply_policy"},{"location":"pythonv1/cape_privacy.policy/policy/#parse_policy","text":"parse_policy ( p : Union [ str , Dict [ Any , Any ]]) -> data . Policy Parses a policy YAML file. The passed in string can either be a path to a local file, a URL pointing to a file or a dictionary representing the policy. If it is a URL then requests attempts to download it. Arguments : p - a path string, a URL string or a dictionary representing the policy. Returns : The Policy object initialized by the YAML.","title":"parse_policy"},{"location":"pythonv1/cape_privacy.policy/policy/#reverse","text":"reverse ( policy : data . Policy ) -> data . Policy Turns reversible tokenizations into token reversers If any named transformations contain a reversible tokenization transformation this helper function turns them into token reverser transformations. Arguments : policy - Top level policy object. Returns : The modified policy.","title":"reverse"},{"location":"pythonv1/cape_privacy.spark/dtypes/","text":"cape_privacy.spark.dtypes #","title":"dtypes"},{"location":"pythonv1/cape_privacy.spark/dtypes/#cape_privacysparkdtypes","text":"","title":"cape_privacy.spark.dtypes"},{"location":"pythonv1/cape_privacy.spark/registry/","text":"cape_privacy.spark.registry # get # get ( transformation : str ) -> TransformationCtor Returns the constructor for the given key. Arguments : transformation - The key of transformation to retrieve. register # register ( label : str , ctor : TransformationCtor ) Registers a new transformation constructor under the label provided. Arguments : label - The label that will be used as the key in the registry ctor - The transformation constructor","title":"registry"},{"location":"pythonv1/cape_privacy.spark/registry/#cape_privacysparkregistry","text":"","title":"cape_privacy.spark.registry"},{"location":"pythonv1/cape_privacy.spark/registry/#get","text":"get ( transformation : str ) -> TransformationCtor Returns the constructor for the given key. Arguments : transformation - The key of transformation to retrieve.","title":"get"},{"location":"pythonv1/cape_privacy.spark/registry/#register","text":"register ( label : str , ctor : TransformationCtor ) Registers a new transformation constructor under the label provided. Arguments : label - The label that will be used as the key in the registry ctor - The transformation constructor","title":"register"},{"location":"pythonv1/cape_privacy.spark/transformations/perturbation/","text":"cape_privacy.spark.transformations.perturbation # NumericPerturbation Objects # class NumericPerturbation ( base . Transformation ) Add uniform random noise to a numeric series Mask a numeric series by adding uniform random noise to each value. The amount of noise is drawn from the interval [min, max). Attributes : dtype dtypes.Numerics - series type min int, float - the values generated will be greater or equal to min max int, float - the values generated will be less than max seed (int), optional: a seed to initialize the random generator DatePerturbation Objects # class DatePerturbation ( base . Transformation ) Add uniform random noise to a Pandas series of timestamps Mask a series by adding uniform random noise to the specified frequencies of timestamps. The amount of noise for each frequency is drawn from the internal [min_freq, max_freq). Note that seeds are currently not supported. Attributes : frequency str, str list - one or more frequencies to perturbate min int, int list - the frequency value will be greater or equal to min max int, int list - the frequency value will be less than max","title":"perturbation"},{"location":"pythonv1/cape_privacy.spark/transformations/perturbation/#cape_privacysparktransformationsperturbation","text":"","title":"cape_privacy.spark.transformations.perturbation"},{"location":"pythonv1/cape_privacy.spark/transformations/perturbation/#numericperturbation-objects","text":"class NumericPerturbation ( base . Transformation ) Add uniform random noise to a numeric series Mask a numeric series by adding uniform random noise to each value. The amount of noise is drawn from the interval [min, max). Attributes : dtype dtypes.Numerics - series type min int, float - the values generated will be greater or equal to min max int, float - the values generated will be less than max seed (int), optional: a seed to initialize the random generator","title":"NumericPerturbation Objects"},{"location":"pythonv1/cape_privacy.spark/transformations/perturbation/#dateperturbation-objects","text":"class DatePerturbation ( base . Transformation ) Add uniform random noise to a Pandas series of timestamps Mask a series by adding uniform random noise to the specified frequencies of timestamps. The amount of noise for each frequency is drawn from the internal [min_freq, max_freq). Note that seeds are currently not supported. Attributes : frequency str, str list - one or more frequencies to perturbate min int, int list - the frequency value will be greater or equal to min max int, int list - the frequency value will be less than max","title":"DatePerturbation Objects"},{"location":"pythonv1/cape_privacy.spark/transformations/redaction/","text":"cape_privacy.spark.transformations.redaction # ColumnRedact Objects # class ColumnRedact () Redacts columns from a Spark dataframe. Attributes : columns - Which columns are redacted. RowRedact Objects # class RowRedact () Redacts rows satisfying some condition from a Spark DataFrame. Attributes : condition - When this condition evaluates to True for a row, that row will be dropped.","title":"redaction"},{"location":"pythonv1/cape_privacy.spark/transformations/redaction/#cape_privacysparktransformationsredaction","text":"","title":"cape_privacy.spark.transformations.redaction"},{"location":"pythonv1/cape_privacy.spark/transformations/redaction/#columnredact-objects","text":"class ColumnRedact () Redacts columns from a Spark dataframe. Attributes : columns - Which columns are redacted.","title":"ColumnRedact Objects"},{"location":"pythonv1/cape_privacy.spark/transformations/redaction/#rowredact-objects","text":"class RowRedact () Redacts rows satisfying some condition from a Spark DataFrame. Attributes : condition - When this condition evaluates to True for a row, that row will be dropped.","title":"RowRedact Objects"},{"location":"pythonv1/cape_privacy.spark/transformations/rounding/","text":"cape_privacy.spark.transformations.rounding # NumericRounding Objects # class NumericRounding ( base . Transformation ) Reduce the precision of a numeric series Round each value in the series to the given number of digits. Attributes : dtypes dtypes.Numerics - series type. precision int - set the number of digits. DateTruncation Objects # class DateTruncation ( base . Transformation ) Reduce the precision of a date series Truncate each date in a series to the unit (year or month) specified by frequency. Attributes : frequency string - expect to be 'year' or 'month'","title":"rounding"},{"location":"pythonv1/cape_privacy.spark/transformations/rounding/#cape_privacysparktransformationsrounding","text":"","title":"cape_privacy.spark.transformations.rounding"},{"location":"pythonv1/cape_privacy.spark/transformations/rounding/#numericrounding-objects","text":"class NumericRounding ( base . Transformation ) Reduce the precision of a numeric series Round each value in the series to the given number of digits. Attributes : dtypes dtypes.Numerics - series type. precision int - set the number of digits.","title":"NumericRounding Objects"},{"location":"pythonv1/cape_privacy.spark/transformations/rounding/#datetruncation-objects","text":"class DateTruncation ( base . Transformation ) Reduce the precision of a date series Truncate each date in a series to the unit (year or month) specified by frequency. Attributes : frequency string - expect to be 'year' or 'month'","title":"DateTruncation Objects"},{"location":"pythonv1/cape_privacy.spark/transformations/tokenizer/","text":"cape_privacy.spark.transformations.tokenizer # Tokenizer Objects # class Tokenizer ( base . Transformation ) Tokenizer: map a string to a token to obfuscate it. When applying the tokenizer to a Spark series of type string, each value gets mapped to a token (hexadecimal string). If a value is repeated several times across the series, it always get mapped to the same token in order to maintain the count. A value can be mapped to different tokens by setting the key to a different value. Attributes : max_token_len int or bytes - control the token length (default length is 64) key - expect a string or byte string. if not specified, key will be set to a random byte string. ReversibleTokenizer Objects # class ReversibleTokenizer ( base . Transformation ) ReversibleTokenizer: map a string to a token to obfuscate it. When applying the Tokenizer to a Spark series of type string, each value gets mapped to a token (hexadecimal string). If a value is repeated several times across the series, it always get mapped to the same token in order to maintain the count. A value can be mapped to different tokens by setting the key to a different value. This tokenizer allows tokens to be reversed to their original data when the secret key is known. Attributes : key - expect a string or byte string of length exactly 32 bytes. encoding - string identifying the Python encoding used for inputs. TokenReverser Objects # class TokenReverser ( base . Transformation ) TokenReverser: recover string from token. When applying the TokenReverser to a Spark series of tokens, each token is mapped back to the string that was originally used by ReversibleTokenizer to construct the token. The same key must be used. Attributes : key - expect a string or byte string of length exactly 32 bytes. encoding - string identifying the Python encoding used for outputs.","title":"tokenizer"},{"location":"pythonv1/cape_privacy.spark/transformations/tokenizer/#cape_privacysparktransformationstokenizer","text":"","title":"cape_privacy.spark.transformations.tokenizer"},{"location":"pythonv1/cape_privacy.spark/transformations/tokenizer/#tokenizer-objects","text":"class Tokenizer ( base . Transformation ) Tokenizer: map a string to a token to obfuscate it. When applying the tokenizer to a Spark series of type string, each value gets mapped to a token (hexadecimal string). If a value is repeated several times across the series, it always get mapped to the same token in order to maintain the count. A value can be mapped to different tokens by setting the key to a different value. Attributes : max_token_len int or bytes - control the token length (default length is 64) key - expect a string or byte string. if not specified, key will be set to a random byte string.","title":"Tokenizer Objects"},{"location":"pythonv1/cape_privacy.spark/transformations/tokenizer/#reversibletokenizer-objects","text":"class ReversibleTokenizer ( base . Transformation ) ReversibleTokenizer: map a string to a token to obfuscate it. When applying the Tokenizer to a Spark series of type string, each value gets mapped to a token (hexadecimal string). If a value is repeated several times across the series, it always get mapped to the same token in order to maintain the count. A value can be mapped to different tokens by setting the key to a different value. This tokenizer allows tokens to be reversed to their original data when the secret key is known. Attributes : key - expect a string or byte string of length exactly 32 bytes. encoding - string identifying the Python encoding used for inputs.","title":"ReversibleTokenizer Objects"},{"location":"pythonv1/cape_privacy.spark/transformations/tokenizer/#tokenreverser-objects","text":"class TokenReverser ( base . Transformation ) TokenReverser: recover string from token. When applying the TokenReverser to a Spark series of tokens, each token is mapped back to the string that was originally used by ReversibleTokenizer to construct the token. The same key must be used. Attributes : key - expect a string or byte string of length exactly 32 bytes. encoding - string identifying the Python encoding used for outputs.","title":"TokenReverser Objects"},{"location":"release-notes/","text":"Release notes # This section contains a complete history of Cape Privacy's release notes.","title":"Introduction"},{"location":"release-notes/#release-notes","text":"This section contains a complete history of Cape Privacy's release notes.","title":"Release notes"},{"location":"release-notes/04032021/","text":"4 March 2021 - pycape 0.1.0 - \"pycape pre-release\" # This release offers a preview of Cape Privacy's Encrypted Learning platform, via pycape. It gives data scientists a first look at some key elements of Cape Privacy's encrypted learning capabilities, including encrypted linear regression and running of encrypted learning jobs on Cape workers. Refer to the Quickstart for information on installing and using pycape. You can view the source code in the pycape GitHub repository . Import pycape; Run encrypted learning # pycape allows you to run encrypted learning jobs in a few lines of Python. To review how the process works for an encrypted linear regression job, review Cape's tutorial on running an encrypted Linear Regression job . Refer to the full pycape documentation for more information.","title":"March 2021"},{"location":"release-notes/04032021/#4-march-2021-pycape-010-pycape-pre-release","text":"This release offers a preview of Cape Privacy's Encrypted Learning platform, via pycape. It gives data scientists a first look at some key elements of Cape Privacy's encrypted learning capabilities, including encrypted linear regression and running of encrypted learning jobs on Cape workers. Refer to the Quickstart for information on installing and using pycape. You can view the source code in the pycape GitHub repository .","title":"4 March 2021 - pycape 0.1.0 - \"pycape pre-release\""},{"location":"release-notes/04032021/#import-pycape-run-encrypted-learning","text":"pycape allows you to run encrypted learning jobs in a few lines of Python. To review how the process works for an encrypted linear regression job, review Cape's tutorial on running an encrypted Linear Regression job . Refer to the full pycape documentation for more information.","title":"Import pycape; Run encrypted learning"},{"location":"release-notes/26062020/","text":"26 June 2020 - Cape Python 0.1.1 - \"Snack\" # This release offers a preview of Cape Privacy's functionality, using Cape Python. It gives data scientists a first look at some key elements of Cape Privacy's tooling, including writing policies and using Cape Privacy's built-in transformations. Refer to the Quickstart for information on installing and using Cape Python. You can view the source code in the Cape Python GitHub repository . Writing data policies # Cape Python supports data policies written in YAML. Refer to Policies for more information. Five built-in transformations # Cape Python includes five built-in transformations: date-pertubation number-pertubation date-truncation numeric-rounding tokenizer Refer to Transformations for more information.","title":"June 2020"},{"location":"release-notes/26062020/#26-june-2020-cape-python-011-snack","text":"This release offers a preview of Cape Privacy's functionality, using Cape Python. It gives data scientists a first look at some key elements of Cape Privacy's tooling, including writing policies and using Cape Privacy's built-in transformations. Refer to the Quickstart for information on installing and using Cape Python. You can view the source code in the Cape Python GitHub repository .","title":"26 June 2020 - Cape Python 0.1.1 - \"Snack\""},{"location":"release-notes/26062020/#writing-data-policies","text":"Cape Python supports data policies written in YAML. Refer to Policies for more information.","title":"Writing data policies"},{"location":"release-notes/26062020/#five-built-in-transformations","text":"Cape Python includes five built-in transformations: date-pertubation number-pertubation date-truncation numeric-rounding tokenizer Refer to Transformations for more information.","title":"Five built-in transformations"},{"location":"release-notes/30072020/","text":"30 July 2020 - Cape Core 0.0.1 and Cape Python 0.2.0 # This release offers to elements of Cape Core: the Cape Coordinator service, and the command line interface. Cape Core 0.0.1 (Archived and Removed) # Manage data policies with Cape Coordinator # Cape Coordinator is a server application that manages access to policies and projects based on user authentication and user roles. This release makes it available to deploy to Kubernetes, or install locally using a CLI to test it out. A CLI interface # The CLI allows users to interact with Cape Coordinator in order to manage users, projects, and policies. Cape Python 0.2.0 # Cape Core Integration # This release offers integrations to elements of Cape Core: the Cape Coordinator service. Refer to Cape Core for installation and usage guides.","title":"July 2020"},{"location":"release-notes/30072020/#30-july-2020-cape-core-001-and-cape-python-020","text":"This release offers to elements of Cape Core: the Cape Coordinator service, and the command line interface.","title":"30 July 2020 - Cape Core 0.0.1 and Cape Python 0.2.0"},{"location":"release-notes/30072020/#cape-core-001-archived-and-removed","text":"","title":"Cape Core 0.0.1 (Archived and Removed)"},{"location":"release-notes/30072020/#manage-data-policies-with-cape-coordinator","text":"Cape Coordinator is a server application that manages access to policies and projects based on user authentication and user roles. This release makes it available to deploy to Kubernetes, or install locally using a CLI to test it out.","title":"Manage data policies with Cape Coordinator"},{"location":"release-notes/30072020/#a-cli-interface","text":"The CLI allows users to interact with Cape Coordinator in order to manage users, projects, and policies.","title":"A CLI interface"},{"location":"release-notes/30072020/#cape-python-020","text":"","title":"Cape Python 0.2.0"},{"location":"release-notes/30072020/#cape-core-integration","text":"This release offers integrations to elements of Cape Core: the Cape Coordinator service. Refer to Cape Core for installation and usage guides.","title":"Cape Core Integration"},{"location":"understand/","text":"Understand Cape Privacy # This section provides high-level information to help you understand what Cape Privacy offers, its underlying principles, and how the different libraries and services work together.","title":"Introduction"},{"location":"understand/#understand-cape-privacy","text":"This section provides high-level information to help you understand what Cape Privacy offers, its underlying principles, and how the different libraries and services work together.","title":"Understand Cape Privacy"},{"location":"understand/roadmap/","text":"Roadmap # Cape Privacy's software is currently in alpha state. This document describes Cape Privacy's goals, and some upcoming features. The goal # Cape Privacy's goal is to make encrypted learning available for everyone. We believe the future of machine learning is encrypted -- to offer more privacy, more security and provide the right data for solving real world problems. Through our open source approach, we ensure our own code and tools are transparent and auditable. Data flow # Data will flow between the following elements of Cape Privacy's architecture: Cape workers access local data, encrypt it using multi-party computation and connect with the broker and coordinator to execute encrypted learning jobs. The broker (maintained by Cape) allows two or more organizations to work together on encrypted computations by acting as a secure bridge when jobs are running. The pycape library is built for data scientists: allowing data scientists to view projects they are a part of, add or remove data views, create and check status of jobs and pull model metrics and results (if permitted). If approved, these jobs are then sent to the workers to be run. Upcoming features # pycape # Debug encrypted learning jobs more easily with shape and schema checking Define a model_owner for a job explicitly in the Job declaration Enhanced roles and permissions for data scientists and operators Cape workers # Performance enhancements Networking improvements Enhanced roles and permissions for operators","title":"Roadmap"},{"location":"understand/roadmap/#roadmap","text":"Cape Privacy's software is currently in alpha state. This document describes Cape Privacy's goals, and some upcoming features.","title":"Roadmap"},{"location":"understand/roadmap/#the-goal","text":"Cape Privacy's goal is to make encrypted learning available for everyone. We believe the future of machine learning is encrypted -- to offer more privacy, more security and provide the right data for solving real world problems. Through our open source approach, we ensure our own code and tools are transparent and auditable.","title":"The goal"},{"location":"understand/roadmap/#data-flow","text":"Data will flow between the following elements of Cape Privacy's architecture: Cape workers access local data, encrypt it using multi-party computation and connect with the broker and coordinator to execute encrypted learning jobs. The broker (maintained by Cape) allows two or more organizations to work together on encrypted computations by acting as a secure bridge when jobs are running. The pycape library is built for data scientists: allowing data scientists to view projects they are a part of, add or remove data views, create and check status of jobs and pull model metrics and results (if permitted). If approved, these jobs are then sent to the workers to be run.","title":"Data flow"},{"location":"understand/roadmap/#upcoming-features","text":"","title":"Upcoming features"},{"location":"understand/roadmap/#pycape","text":"Debug encrypted learning jobs more easily with shape and schema checking Define a model_owner for a job explicitly in the Job declaration Enhanced roles and permissions for data scientists and operators","title":"pycape"},{"location":"understand/roadmap/#cape-workers","text":"Performance enhancements Networking improvements Enhanced roles and permissions for operators","title":"Cape workers"},{"location":"understand/architecture/","text":"Cape Privacy architecture # Cape Privacy's architecture is a collection of services and libraries that work with each other and your own data management tooling. Components # Cape Privacy currently offers three components: The Cape Cloud Service (referred to as \"Cape\"), where you can set up data-science projects to collaborate on with other organizations. This service also includes the Cape broker, allowing you to easily and securely work across different clouds. Cape Workers , which are run by your organization to orchestrate and run encrypted learning jobs. The Python libary pycape . With pycape you can interact with your Cape projects, view and update details of your project, and manipulate and upload your datasets to Cape. Roadmap # More services and libraries are coming soon. Refer to the roadmap for more information. Archived Projects # Cape Python . The first Cape Privacy library to be released, providing Python-based data transformations for Pandas and Apache Spark (PySpark). This project is now archived and may later be removed.","title":"Overview"},{"location":"understand/architecture/#cape-privacy-architecture","text":"Cape Privacy's architecture is a collection of services and libraries that work with each other and your own data management tooling.","title":"Cape Privacy architecture"},{"location":"understand/architecture/#components","text":"Cape Privacy currently offers three components: The Cape Cloud Service (referred to as \"Cape\"), where you can set up data-science projects to collaborate on with other organizations. This service also includes the Cape broker, allowing you to easily and securely work across different clouds. Cape Workers , which are run by your organization to orchestrate and run encrypted learning jobs. The Python libary pycape . With pycape you can interact with your Cape projects, view and update details of your project, and manipulate and upload your datasets to Cape.","title":"Components"},{"location":"understand/architecture/#roadmap","text":"More services and libraries are coming soon. Refer to the roadmap for more information.","title":"Roadmap"},{"location":"understand/architecture/#archived-projects","text":"Cape Python . The first Cape Privacy library to be released, providing Python-based data transformations for Pandas and Apache Spark (PySpark). This project is now archived and may later be removed.","title":"Archived Projects"},{"location":"understand/architecture/cape-workers/","text":"Cape Workers # Cape workers are Docker images that can be built and run in the environment of your choice. Each party collaborating on training a model using Cape will deploy a separate cape worker as a Docker container. Cape workers are designed to facilitate the sharing of datasets between each other using public-key encryption, while keeping all other data isolated to the context of each customized, containerized service. Prerequisites # Docker A Cape Cloud Account A Cape Cloud organization token An S3 bucket for retrieving data Note In order to create the cape-worker docker container, you will need to pull the image from Docker Hub . You will need to be granted access to Cape Privacy's private cape-worker Docker Hub repo, and your development environment will need HTTP access to hub.docker.com . Supported SHA Commits # sha-d68d933 , latest Starting your cape-worker instance # Starting your Cape worker is as simple as running a Docker image using docker run : export CAPE_TOKEN = my-cape-org-token docker run -d --rm \\ --name cape-worker \\ -e CAPE_TOKEN \\ -e CAPE_BUCKET = my-s3-bucket-location \\ -e CAPE_COORDINATOR = https://app.capeprivacy.com \\ -e CAPE_BROKER = https://app.capeprivacy.com/broker \\ -e AWS_ACCESS_KEY_ID \\ -e AWS_SECRET_ACCESS_KEY \\ -e AWS_REGION \\ capeprivacy/cape-worker:sha-1234567 In this command: cape-worker is the name you want to assign your container. my-cape-org-token is your Cape organization token . s3-bucket-location is the URI of the S3 bucket that you would like Cape to write the results of the computation to. sha-1234567 is the commit SHA specifying the commit you would like to use to run your worker. Note In order for your worker to communicate with Cape's API and other workers, your cape-worker docker container will need HTTP access to the endpoints specified in the CAPE_COORDINATOR and CAPE_BROKER environment variables. Environment Variables # When you run the cape-worker image, you can specify the following environment variables, either by passing on the docker run command, or by setting them in a separate .env file. Refer to the Docker documentation for options for configuring Docker environment variables . CAPE_TOKEN (required) # This is your Cape organization token. You can generate Cape organization tokens from your \"Organization Settings\" in the Cape UI. Refer to our tokens usage documentation for more information on how tokens are used in Cape. AWS_ACCESS_KEY_ID # Set this to the AWS access key associated with an IAM user that has s3 write permissions to the location specified by CAPE_BUCKET . This variable is only required if your container doesn't already have S3 permissions, for example if you run your container on an EC2 instance using an IAM role that has the correct permissions. AWS_SECRET_ACCESS_KEY # Set this to the secret key associated with the access key. This is essentially the \"password\" for the access key. This variable is only required if your container doesn't already have S3 permissions, for example if you run your container on an EC2 instance using an IAM role that has the correct permissions. AWS_REGION # Set this to the AWS Region you would like to send the request to. Note These AWS configuration variables line up with the AWS CLI environment variables. For up-to-date info on these variables, refer to the AWS documentation . (Optional) Configure your container to restart automatically # Add the flag --restart unless-stopped to your docker command-line when launching your container to have it restart automatically. Alternatively, use a process manager like supervisor or systemd if you need more control. For more information, refer to the Docker documentation . (Optional) Send your logs to an aggregation service like AWS CloudWatch # It is highly recommended that you send container logs to a log aggregation service. For example, if the worker is running in an AWS environment, the Docker daemon or the individual container can send logs directly to AWS CloudWatch. See here for detailed documentation, along with other drivers provided by docker.","title":"Cape Workers"},{"location":"understand/architecture/cape-workers/#cape-workers","text":"Cape workers are Docker images that can be built and run in the environment of your choice. Each party collaborating on training a model using Cape will deploy a separate cape worker as a Docker container. Cape workers are designed to facilitate the sharing of datasets between each other using public-key encryption, while keeping all other data isolated to the context of each customized, containerized service.","title":"Cape Workers"},{"location":"understand/architecture/cape-workers/#prerequisites","text":"Docker A Cape Cloud Account A Cape Cloud organization token An S3 bucket for retrieving data Note In order to create the cape-worker docker container, you will need to pull the image from Docker Hub . You will need to be granted access to Cape Privacy's private cape-worker Docker Hub repo, and your development environment will need HTTP access to hub.docker.com .","title":"Prerequisites"},{"location":"understand/architecture/cape-workers/#supported-sha-commits","text":"sha-d68d933 , latest","title":"Supported SHA Commits"},{"location":"understand/architecture/cape-workers/#starting-your-cape-worker-instance","text":"Starting your Cape worker is as simple as running a Docker image using docker run : export CAPE_TOKEN = my-cape-org-token docker run -d --rm \\ --name cape-worker \\ -e CAPE_TOKEN \\ -e CAPE_BUCKET = my-s3-bucket-location \\ -e CAPE_COORDINATOR = https://app.capeprivacy.com \\ -e CAPE_BROKER = https://app.capeprivacy.com/broker \\ -e AWS_ACCESS_KEY_ID \\ -e AWS_SECRET_ACCESS_KEY \\ -e AWS_REGION \\ capeprivacy/cape-worker:sha-1234567 In this command: cape-worker is the name you want to assign your container. my-cape-org-token is your Cape organization token . s3-bucket-location is the URI of the S3 bucket that you would like Cape to write the results of the computation to. sha-1234567 is the commit SHA specifying the commit you would like to use to run your worker. Note In order for your worker to communicate with Cape's API and other workers, your cape-worker docker container will need HTTP access to the endpoints specified in the CAPE_COORDINATOR and CAPE_BROKER environment variables.","title":"Starting your cape-worker instance"},{"location":"understand/architecture/cape-workers/#environment-variables","text":"When you run the cape-worker image, you can specify the following environment variables, either by passing on the docker run command, or by setting them in a separate .env file. Refer to the Docker documentation for options for configuring Docker environment variables .","title":"Environment Variables"},{"location":"understand/architecture/cape-workers/#cape_token-required","text":"This is your Cape organization token. You can generate Cape organization tokens from your \"Organization Settings\" in the Cape UI. Refer to our tokens usage documentation for more information on how tokens are used in Cape.","title":"CAPE_TOKEN (required)"},{"location":"understand/architecture/cape-workers/#aws_access_key_id","text":"Set this to the AWS access key associated with an IAM user that has s3 write permissions to the location specified by CAPE_BUCKET . This variable is only required if your container doesn't already have S3 permissions, for example if you run your container on an EC2 instance using an IAM role that has the correct permissions.","title":"AWS_ACCESS_KEY_ID"},{"location":"understand/architecture/cape-workers/#aws_secret_access_key","text":"Set this to the secret key associated with the access key. This is essentially the \"password\" for the access key. This variable is only required if your container doesn't already have S3 permissions, for example if you run your container on an EC2 instance using an IAM role that has the correct permissions.","title":"AWS_SECRET_ACCESS_KEY"},{"location":"understand/architecture/cape-workers/#aws_region","text":"Set this to the AWS Region you would like to send the request to. Note These AWS configuration variables line up with the AWS CLI environment variables. For up-to-date info on these variables, refer to the AWS documentation .","title":"AWS_REGION"},{"location":"understand/architecture/cape-workers/#optional-configure-your-container-to-restart-automatically","text":"Add the flag --restart unless-stopped to your docker command-line when launching your container to have it restart automatically. Alternatively, use a process manager like supervisor or systemd if you need more control. For more information, refer to the Docker documentation .","title":"(Optional) Configure your container to restart automatically"},{"location":"understand/architecture/cape-workers/#optional-send-your-logs-to-an-aggregation-service-like-aws-cloudwatch","text":"It is highly recommended that you send container logs to a log aggregation service. For example, if the worker is running in an AWS environment, the Docker daemon or the individual container can send logs directly to AWS CloudWatch. See here for detailed documentation, along with other drivers provided by docker.","title":"(Optional) Send your logs to an aggregation service like AWS CloudWatch"},{"location":"understand/best-practices/","text":"Best practice guides # This section of the documentation provides guidance and advice on how to use Cape Privacy's tools in your own workflow. These are not prescriptive user manuals. Instead, they are articles drawing on the expertise available at Cape Privacy, designed to help you implement Cape Privacy tools in a way that works for you.","title":"Overview"},{"location":"understand/best-practices/#best-practice-guides","text":"This section of the documentation provides guidance and advice on how to use Cape Privacy's tools in your own workflow. These are not prescriptive user manuals. Instead, they are articles drawing on the expertise available at Cape Privacy, designed to help you implement Cape Privacy tools in a way that works for you.","title":"Best practice guides"},{"location":"understand/best-practices/linear-regression/","text":"Linear Regression Best Practices # When training using VerticallyPartitionedLinearRegression , the algorithm expects a bound on its input data in order to avoid precision loss during model training. This means you will need to preprocess your data with attention to numeric precision, standardizing or normalizing it around the origin. See its reference documentation for more details. DataView indices must be aligned across parties before being used for a VerticallyPartitionedLinearRegression . We recommend coordinating this offline with the other parties you will be training with. Before running a large amount of data, try a smaller dummy data view, to quickly learn if there are any issues with alignment, precision or worker connectivity.","title":"Linear Regression"},{"location":"understand/best-practices/linear-regression/#linear-regression-best-practices","text":"When training using VerticallyPartitionedLinearRegression , the algorithm expects a bound on its input data in order to avoid precision loss during model training. This means you will need to preprocess your data with attention to numeric precision, standardizing or normalizing it around the origin. See its reference documentation for more details. DataView indices must be aligned across parties before being used for a VerticallyPartitionedLinearRegression . We recommend coordinating this offline with the other parties you will be training with. Before running a large amount of data, try a smaller dummy data view, to quickly learn if there are any issues with alignment, precision or worker connectivity.","title":"Linear Regression Best Practices"},{"location":"understand/features/","text":"Cape Features & Concepts # This section outlines the various features and concepts found within Cape.","title":"Overview"},{"location":"understand/features/#cape-features-concepts","text":"This section outlines the various features and concepts found within Cape.","title":"Cape Features &amp; Concepts"},{"location":"understand/features/projects/","text":"Projects # Projects are the building block for encrypted learning within Cape. In the real world, most machine learning experiments are run for a specific use case with a set amount of data. On Cape, organizations can work together on a project to build a model using encrypted data that is spread within an organization or across organizations and clouds. Cape Project's allow your organization to register data for usage, create, approve and run encrypted learning jobs and manage the variety of data science tasks necessary for keeping your experiment successful.","title":"Projects"},{"location":"understand/features/projects/#projects","text":"Projects are the building block for encrypted learning within Cape. In the real world, most machine learning experiments are run for a specific use case with a set amount of data. On Cape, organizations can work together on a project to build a model using encrypted data that is spread within an organization or across organizations and clouds. Cape Project's allow your organization to register data for usage, create, approve and run encrypted learning jobs and manage the variety of data science tasks necessary for keeping your experiment successful.","title":"Projects"},{"location":"understand/features/roles/","text":"Roles # In Cape, there are five roles which have different permissions and responsibilities for using the Cape Cloud service. Each Cape user can be assigned a role when they join an organization. Currently, Cape only supports one role per user. When you join a project, you will also be assigned a role by the person who invited you to the project. Note Cape refers to all users associated with a project as \"contributors\" to that project, but their roles for that project have specific permissions. For example, if a user has a data scientist role on that project, they now can perform actions associated with that role on that project. Organizational-Level Roles # Administrator (Organizational Level) # An organizational administrator has full permissions for all features on Cape Privacy. It is a role held by super administrators who need to on- and offboard other users and have the highest level of permissions on Cape. Organizational Administrator Actions: Full permissions for organization, including: Can add or remove persons from the organization Can change roles for any members of the organization Can delete the organization Full permissions for all projects that organization is a part of, including: Can join or leave projects on behalf of an organization Can add or edit contributors on all projects Can add, remove data views for all projects Can approve and run encrypted learning jobs for all projects Operator # An operator controls the organization's tokens and is in charge of deploying, running and monitoring Cape workers. They are able to revoke tokens and view all projects and project activity for an organization. Operator Actions: Can view all organization projects and job / project activity Can issue / revoke organizational tokens Ability to install and download Cape Workers User # A Cape user can look at projects they are a member of and join and leave projects that they are invited to. They can only see and join projects they have been invited to by a Cape organization or project administrator. User Actions: Join and leave projects I am invited to by an organizational or project administrator Can view projects pages and logs, but cannot change data views or jobs or approve or reject jobs Project-Level Roles # Administrator (Project-Level) # A project administrator has full permissions for their organization's projects on Cape Privacy, add and edit contributors on projects they are a part of and perform necessary project actions like adding and removing data views or approving and running encrypted learning jobs. Project Administrator Actions: Full permissions for all projects that organization is a part of, including: Can add or edit contributors on all projects Can add, remove data views for all projects Can approve and run encrypted learning jobs for all projects Data Scientist # A Data Scientist can add, remove and edit data views and create, run, reject and approve encrypted learning jobs for projects. They can only see and join projects they have been invited to by a Cape organization or project administrator Data Scientist Actions: Join and leave projects I am invited to by an organizational or project administrator Can add, remove data views for projects I am a part of Can create, approve, run encrypted learning jobs for projects I am a part of Can access metrics and model weights for jobs if my organization is the model owner User # A user for a project has the same permissions as a user for an organization (see above).","title":"Roles"},{"location":"understand/features/roles/#roles","text":"In Cape, there are five roles which have different permissions and responsibilities for using the Cape Cloud service. Each Cape user can be assigned a role when they join an organization. Currently, Cape only supports one role per user. When you join a project, you will also be assigned a role by the person who invited you to the project. Note Cape refers to all users associated with a project as \"contributors\" to that project, but their roles for that project have specific permissions. For example, if a user has a data scientist role on that project, they now can perform actions associated with that role on that project.","title":"Roles"},{"location":"understand/features/roles/#organizational-level-roles","text":"","title":"Organizational-Level Roles"},{"location":"understand/features/roles/#administrator-organizational-level","text":"An organizational administrator has full permissions for all features on Cape Privacy. It is a role held by super administrators who need to on- and offboard other users and have the highest level of permissions on Cape. Organizational Administrator Actions: Full permissions for organization, including: Can add or remove persons from the organization Can change roles for any members of the organization Can delete the organization Full permissions for all projects that organization is a part of, including: Can join or leave projects on behalf of an organization Can add or edit contributors on all projects Can add, remove data views for all projects Can approve and run encrypted learning jobs for all projects","title":"Administrator (Organizational Level)"},{"location":"understand/features/roles/#operator","text":"An operator controls the organization's tokens and is in charge of deploying, running and monitoring Cape workers. They are able to revoke tokens and view all projects and project activity for an organization. Operator Actions: Can view all organization projects and job / project activity Can issue / revoke organizational tokens Ability to install and download Cape Workers","title":"Operator"},{"location":"understand/features/roles/#user","text":"A Cape user can look at projects they are a member of and join and leave projects that they are invited to. They can only see and join projects they have been invited to by a Cape organization or project administrator. User Actions: Join and leave projects I am invited to by an organizational or project administrator Can view projects pages and logs, but cannot change data views or jobs or approve or reject jobs","title":"User"},{"location":"understand/features/roles/#project-level-roles","text":"","title":"Project-Level Roles"},{"location":"understand/features/roles/#administrator-project-level","text":"A project administrator has full permissions for their organization's projects on Cape Privacy, add and edit contributors on projects they are a part of and perform necessary project actions like adding and removing data views or approving and running encrypted learning jobs. Project Administrator Actions: Full permissions for all projects that organization is a part of, including: Can add or edit contributors on all projects Can add, remove data views for all projects Can approve and run encrypted learning jobs for all projects","title":"Administrator (Project-Level)"},{"location":"understand/features/roles/#data-scientist","text":"A Data Scientist can add, remove and edit data views and create, run, reject and approve encrypted learning jobs for projects. They can only see and join projects they have been invited to by a Cape organization or project administrator Data Scientist Actions: Join and leave projects I am invited to by an organizational or project administrator Can add, remove data views for projects I am a part of Can create, approve, run encrypted learning jobs for projects I am a part of Can access metrics and model weights for jobs if my organization is the model owner","title":"Data Scientist"},{"location":"understand/features/roles/#user_1","text":"A user for a project has the same permissions as a user for an organization (see above).","title":"User"},{"location":"understand/features/tokens/","text":"Tokens # In Cape there are two types of tokens that can be created to authenticate with different parts of Cape's architecture- user tokens and organization tokens. User Tokens # Individual user tokens are used to identify you to the Cape Coordinator, so you can register DataViews , create and run Cape Jobs and perform other necessary functions. See the pycape Usage docs for instructions on how to use your user token to log into the pycape Python library . User tokens can be generated from your User Settings . Organization Tokens # Organizational tokens are used to run Cape Workers . These tokens facilitate the secure sharing of data across all workers that are collaborating on a project using Cape privacy. and are stored as environment variables. Organizational tokens can be generated from your Organization Settings .","title":"Tokens"},{"location":"understand/features/tokens/#tokens","text":"In Cape there are two types of tokens that can be created to authenticate with different parts of Cape's architecture- user tokens and organization tokens.","title":"Tokens"},{"location":"understand/features/tokens/#user-tokens","text":"Individual user tokens are used to identify you to the Cape Coordinator, so you can register DataViews , create and run Cape Jobs and perform other necessary functions. See the pycape Usage docs for instructions on how to use your user token to log into the pycape Python library . User tokens can be generated from your User Settings .","title":"User Tokens"},{"location":"understand/features/tokens/#organization-tokens","text":"Organizational tokens are used to run Cape Workers . These tokens facilitate the secure sharing of data across all workers that are collaborating on a project using Cape privacy. and are stored as environment variables. Organizational tokens can be generated from your Organization Settings .","title":"Organization Tokens"},{"location":"understand/features/users/","text":"Users # Users in Cape are much like users in other applications. All actions performed within Cape must be performed by an authenticated user. Users are added to organizations via an organizational administrator and have the ability to create tokens identifying themselves and their role to the Cape Cloud service.","title":"Users"},{"location":"understand/features/users/#users","text":"Users in Cape are much like users in other applications. All actions performed within Cape must be performed by an authenticated user. Users are added to organizations via an organizational administrator and have the ability to create tokens identifying themselves and their role to the Cape Cloud service.","title":"Users"}]}